{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM\n",
    "# input_dim, factors_num\n",
    "# field_dims, embed_dim=16\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "embedding_size = 768\n",
    "field_num = len(columns_to_train)\n",
    "\n",
    "# 先跑最好的（最新的）\n",
    "# dl_learning_rate_list = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "dl_learning_rate_list = [0.0001, 0.001, 0.01] # (4) 1e-3(0.001) 先跑看看，接下來測試\n",
    "print(\"dl_learning_rate_list:\", len(dl_learning_rate_list))\n",
    "epochs_num_list = [10, 20, 30] # (2) [50, 100]，先跑 100，邊跑邊畫圖，還沒收斂繼續加 跑一個畫圖 接下來測試 還有 optimizer\n",
    "print(\"epochs_num_list:\", len(epochs_num_list))\n",
    "batch_size_list = [64, 128, 256, 512] # [256, 512] # (1) 512，一定要 shuffle，一開始要決定好\n",
    "print(\"batch_size_list:\", len(batch_size_list))\n",
    "mlp_dims_list = [tuple([size] * num_layers) for num_layers in range(1, 4) for size in [i*100 for i in range(1, 5)]]\n",
    "print(\"mlp_dims_list:\", len(mlp_dims_list)) # 先跑 3 層 256\n",
    "mlp_dims_list_2times = [tuple([size] * num_layers) for num_layers in range(1, 4) for size in [2**i for i in range(5, 10)]]\n",
    "print(\"mlp_dims_list_2times:\", len(mlp_dims_list_2times)) # \n",
    "dropout_list_1 = [size * 0.1 for size in range(1, 6)]\n",
    "print(\"dropout_list_1:\", len(dropout_list_1)) # 0, 0.5 # 一開始要決定好 (3) 先 0\n",
    "dropout_list_2 = [tuple([size*0.1] * 2) for size in range(1, 6)]\n",
    "print(\"dropout_list_2:\", len(dropout_list_2))\n",
    "dropout_list_3 = [tuple([size*0.1] * 3) for size in range(1, 6)]\n",
    "print(\"dropout_list_3:\", len(dropout_list_3))\n",
    "\n",
    "FM_params = {\n",
    "    'hyperparameters': {\n",
    "        'input_dim': [input_dim], \n",
    "#         'factors_num': [5, 10, 15],\n",
    "#         'factors_num': [i+1 for i in range(20)]\n",
    "#         'factors_num': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "        'factors_num': [2**i for i in range(3, 7)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# MLP\n",
    "# input_dim, embed_dims, dropout\n",
    "\n",
    "MLP_params = {\n",
    "    'hyperparameters': {\n",
    "        'input_dim': [input_dim], \n",
    "#         'factors_num': [5, 10, 15],\n",
    "        'embed_dims': [tuple([size] * num_layers) for num_layers in range(1, 4) for size in [i*100 for i in range(1, 5)]],\n",
    "        'dropout': dropout_list_1\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# DeepFM\n",
    "# feature_sizes, embedding_size, hidden_dims, num_classes=1, dropout=[0.5, 0.5]\n",
    "\n",
    "# Paper:\n",
    "# test activation function: relu, tanh (relu is better)\n",
    "# embedding size seems to be 5\n",
    "# dropout: test 1, 0.9, 0.8, 0.7, 0.6, 0.5 (when 0.6 ~ 0.9 is better)\n",
    "# hidden layer num: test 1, 3, 5, 7, better at begining, but more getting worse\n",
    "# shape: given layer num and total neuron num, test 4 shape, such as 3, 600 =>\n",
    "# constant (200-200-200), increasing (100-200-300), decreasing (300-200-100), and diamond (150-300-150)\n",
    "# constant is better, which is consistent with previous studies\n",
    "\n",
    "DeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'feature_sizes': [[embedding_size for i in range(field_num)]], \n",
    "#         'embedding_size': [4, 8], \n",
    "        'embedding_size': [5, 10],\n",
    "#         'hidden_dims': [[64, 32], [128, 64], [256, 128]], \n",
    "#         'hidden_dims': [[100, 100, 100], [200, 200, 200], [400, 400, 400]],\n",
    "        'hidden_dims': mlp_dims_list,\n",
    "        'num_classes': [1], \n",
    "#         'dropout': [(0.5, 0.5)]\n",
    "        'dropout': dropout_list_2\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFM\n",
    "# field_dims, embed_dim, attn_size, dropouts \n",
    "# embed_dim=16, attn_size=16, dropouts=(0.2, 0.2)\n",
    "\n",
    "# Paper\n",
    "# optimizer: mini-batch Adagrad. The batch size for Frappe and MovieLens is set to 128 and 4096 \n",
    "# The embedding size is set to 256 for all methods\n",
    "# without special mention, the attention factor is also 256 (test on 1, 4, 8, 16, 32, 64, 128, 256)\n",
    "# Validation error: performance is stable across different size of attention factors\n",
    "# also test dropout for fm, afm, fifm, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8\n",
    "# Specifically, for AFM, the optimal dropout ratio on Frappe and MovieLens is 0.2 and 0.5\n",
    "# also test l2 regulation(attention network) from 0, 0.5, 1, 2, 4, 8, 16\n",
    "# prove using l2 is better, and when l2 is larger, rmse is decreasing gently \n",
    "\n",
    "AFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "#         'attn_size': [8, 16], \n",
    "        'attn_size': [1, 4, 8, 16, 32, 64, 128, 256],\n",
    "#         'dropouts': [(0.5, 0.5)]\n",
    "#         'dropouts': [(0.2, 0.2), (0.5, 0.5)]\n",
    "        'dropouts': dropout_list_2\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# DCN\n",
    "# field_dims, embed_dim, num_layers, mlp_dims, dropout \n",
    "# embed_dim=16, num_layers=3, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "# Paper:\n",
    "# cross layer(num_layers): 1~6, hidden layer: 2 ~ 5, with size 32 ~ 1024, \n",
    "# Based on test loss, 2 deep layers of size 1024 and 6 cross layers for the DCN model\n",
    "# 5 deep layers of size 1024 for the DNN\n",
    "# Based on validation loss, more cross layer num DCN loss decrease\n",
    " \n",
    "# initial learning rate was tuned from 0.0001 to 0.001 with increments of 0.0001\n",
    "# early stopping: at training step 150,000\n",
    "# optimization: mini-batch stochastic optimization with Adam optimizer. \n",
    "# batch size: 512. Batch normalization was applied to the deep network\n",
    "# gradient clip: 100\n",
    "# Real-valued features are normalized by applying a log transform. \n",
    "# For categorical features, we embed the features in dense vectors of dimension 6×(category cardinality)1/4.\n",
    "# Concatenating all embeddings results in a vector of dimension 1026\n",
    "\n",
    "# Not CTR task, grid search:\n",
    "# deep layers num ranged from 1 to 10 with layer size from 50 to 300. \n",
    "# The number of cross layers ranged from 4 to 10. \n",
    "# The number of residual units ranged from 1 to 5 with their input dim and cross dim from 50 to 300.\n",
    "# For DCN, the input vector was fed to the cross network directly\n",
    "\n",
    "DCN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "#         'num_layers': [3],\n",
    "        'num_layers': [i+1 for i in range(6)],\n",
    "#         'mlp_dims': [(16, 16)],\n",
    "#         'mlp_dims': mlp_dims_list_2times,\n",
    "        'mlp_dims': mlp_dims_list,\n",
    "#         'dropout': [0.2, 0.5]\n",
    "        'dropout': dropout_list_1\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# xDeepFM\n",
    "# embed_dim, mlp_dims, dropout, cross_layer_sizes, split_half=True \n",
    "# embed_dim=16, cross_layer_sizes=(16, 16), split_half=False, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "# Paper\n",
    "# Learning rate : 0.001. optimization method: Adam, mini-batch size: 4096. \n",
    "# L2 regularization with λ = 0.0001 for DNN, DCN, Wide&Deep, DeepFM and xDeepFM, dropout 0.5 for PNN\n",
    "# embed_dim = 10, \n",
    "# Neuron num per layer: (1) 400 for DNN layers; \n",
    "# (2) 200 for CIN layers on Criteo dataset, and 100 for CIN layers on Dianping and Bing News datasets\n",
    "# test cin num_layer 1, 2, 3, 4 (3 best, degrad after 3)\n",
    "# test cin num neurons 20, 40, 100, 200, Bing news is increasing(200 best), Dianping is 20 ~ 100 better, 200 degrades \n",
    "# test cin sigmoid, tanh, relu, identity 4 activation, identity is best, then relu\n",
    "# best cross depth and dnn depth is (3, 2)\n",
    "\n",
    "xDeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "#         'cross_layer_sizes': [(16, 16)],\n",
    "        'cross_layer_sizes': [tuple([neuron] * layer) for layer in range(1, 4) for neuron in [20, 40, 60]],\n",
    "        \"split_half\": [False],\n",
    "#         'mlp_dims': [(16, 16)],\n",
    "        'mlp_dims': mlp_dims_list,\n",
    "#         'dropout': [0.2, 0.5]\n",
    "        'dropout': dropout_list_1\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# AutoInt\n",
    "# field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True\n",
    "# embed_dim=16, atten_embed_dim=64, num_heads=2, num_layers=3, mlp_dims=(400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "# Paper:\n",
    "# embed_dim is set to 16, batch size set to 1024\n",
    "# num_head is 2, num_layer(interaction layer) is 3, num of hidden units (interaction layer) is 32\n",
    "# test dropout from 0.1 ~ 0.9\n",
    "# optimizer is Adam\n",
    "# test num_layer(interaction layer) from 0 ~ 4, 1 increase dramaticaly, 1 ~ 4 become stable\n",
    "# test atten_embed_dim 8, 16, 24, 32, movie len is getting better, for KDD12, 24 is best, then decrease\n",
    "# hidden units shape is set to (1, 200) or (4, 100)\n",
    "# residaul is crucial\n",
    "\n",
    "AutoInt_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "#         'atten_embed_dim': [(64, 32)],\n",
    "#         'atten_embed_dim': [2**i for i in range(3, 6)],\n",
    "        'atten_embed_dim': [8, 16, 24],\n",
    "        \"num_heads\": [2],\n",
    "#         \"num_layers\": [3],\n",
    "        \"num_layers\": [i+1 for i in range(3)],\n",
    "#         'mlp_dims': [(16, 16), (400, 400)],\n",
    "        'mlp_dims': mlp_dims_list,\n",
    "#         'dropouts': [(0.5, 0.5, 0.5)],\n",
    "        'dropouts': dropout_list_3,\n",
    "        \"has_residual\": [True]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFN\n",
    "# field_dims, embed_dim, LNN_dim, mlp_dims, dropouts\n",
    "# embed_dim=16, LNN_dim=1500, mlp_dims=(400, 400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "# Paper:\n",
    "# LNN_dim: 1500, 1200, 800, 600, embed_dim: 10\n",
    "# batch size: 4096\n",
    "# optimizer: Adam # earning rate :0.001\n",
    "# mlp_dims: (400, 400, 400)\n",
    "# All the other hyperparameters are tuned on the validation set\n",
    "\n",
    "AFN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "#         'LNN_dim': [1500, 1000],\n",
    "        'LNN_dim': [1500, 1200, 1000, 800, 600, 400, 200],\n",
    "#         'mlp_dims': [(400, 400, 400)],\n",
    "        'mlp_dims': mlp_dims_list,\n",
    "#         'dropouts': [(0.5, 0.5, 0.5)]\n",
    "        'dropouts': dropout_list_3\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "#         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': dl_learning_rate_list,\n",
    "        'epochs_num': epochs_num_list, \n",
    "#         'batch_size': [100, 200, 500]\n",
    "        'batch_size': batch_size_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Linear parameters\n",
    "Linear_params = {\n",
    "    'hyperparameters': {\n",
    "        \n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # K-Nearest Neighbors parameters\n",
    "# n_neighbors: int, default=5  # Number of neighbors\n",
    "# weights: {'uniform', 'distance'}, default='uniform'  # Weight function used in prediction\n",
    "# metric: {'euclidean', 'manhattan', 'chebyshev', 'minkowski', \n",
    "#          'wminkowski', 'seuclidean', 'mahalanobis'}, default='minkowski'  # Distance metric\n",
    "\n",
    "KNN_params = {\n",
    "    'hyperparameters': {\n",
    "#         'n_neighbors': [5],\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "#         'weights': ['uniform'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Support Vector Machine parameters\n",
    "# C: float, default=1.0  # Penalty parameter C of the error term\n",
    "# kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'  # Kernel function\n",
    "# gamma: {'scale', 'auto'} or float, default='scale'  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "\n",
    "SVM_params = {\n",
    "    'hyperparameters': {\n",
    "#         'C': [1.0],\n",
    "        'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0], \n",
    "#         'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "#         'gamma': ['scale']\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Decision Tree parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "\n",
    "DecisionTree_params = {\n",
    "    'hyperparameters': {\n",
    "#         'max_depth': [None],\n",
    "#         'min_samples_split': [2],\n",
    "#         'min_samples_leaf': [1]\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Random Forest parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "# n_estimators: int, default=100  # Number of trees in the forest\n",
    "\n",
    "RandomForest_params = {\n",
    "    'hyperparameters': {\n",
    "#         'max_depth': [None],\n",
    "#         'min_samples_split': [2],\n",
    "#         'min_samples_leaf': [1],\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "#         'n_estimators': [50, 100]\n",
    "        'n_estimators': [10, 20, 30 ,40 ,50, 60, 70, 80, 90, 100]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # AdaBoost parameters\n",
    "# n_estimators: int, default=50  # Maximum number of estimators at which boosting is terminated\n",
    "# learning_rate: float, default=1.0  # Learning rate shrinks the contribution of each classifier\n",
    "\n",
    "AdaBoost_params = {\n",
    "    'hyperparameters': {\n",
    "#         'n_estimators': [50, 100],\n",
    "        'n_estimators': [10, 20, 30 ,40 ,50, 60, 70, 80, 90, 100],\n",
    "#         'learning_rate': [0.01, 0.02, 0.001]\n",
    "        'learning_rate': [i/100 for i in range(1, 21)],\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # XGBoost parameters\n",
    "# learning_rate: float, default=0.1  # Boosting learning rate (xgb's \"eta\")\n",
    "# n_estimators: int, default=100  # Number of boosted trees to fit\n",
    "# subsample: float, default=1  # Subsample ratio of the training instances (xgb's \"subsample\")\n",
    "# colsample_bytree: float, default=1  # Subsample ratio of columns when constructing each tree (xgb's \"colsample_bytree\")\n",
    "\n",
    "XGBoost_params = {\n",
    "    'hyperparameters': {\n",
    "#         'learning_rate': [0.01, 0.02, 0.001],\n",
    "        'learning_rate': [i/100 for i in range(1, 11)],\n",
    "#         'n_estimators': [50, 100],\n",
    "        'n_estimators': [20, 40, 60, 80, 100],\n",
    "#         'subsample': [1],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#         'colsample_bytree': [1]\n",
    "        'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training hyperparameter combinations setting\n",
    "\n",
    "func_params_list = {\n",
    "    \n",
    "    \"FM\": FM_params,\n",
    "    \"MLP\": MLP_params,\n",
    "    \"DeepFM\": DeepFM_params,\n",
    "    \"AFM\": AFM_params,\n",
    "    \"DCN\": DCN_params,\n",
    "    \"xDeepFM\": xDeepFM_params,\n",
    "    \"AutoInt\": AutoInt_params,\n",
    "    \"AFN\": AFN_params,\n",
    "    \n",
    "    \"Linear\": Linear_params,\n",
    "    \"KNN\": KNN_params,\n",
    "    \"SVM\": SVM_params,\n",
    "    \"DecisionTree\": DecisionTree_params,\n",
    "    \"RandomForest\": RandomForest_params,\n",
    "    \"AdaBoost\": AdaBoost_params,\n",
    "    \"XGBoost\": XGBoost_params \n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
