{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2670aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Layers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        return self.fc(x) + self.bias\n",
    "\n",
    "\n",
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(sum(field_dims), embed_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class FieldAwareFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        xs = [embedding(x) for embedding in self.embeddings]\n",
    "        ix = []\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "\n",
    "\n",
    "class FactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class InnerProductNetwork(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        return torch.sum(x[:, row] * x[:, col], dim=2)\n",
    "\n",
    "\n",
    "class OuterProductNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_fields, embed_dim, kernel_type='mat'):\n",
    "        super().__init__()\n",
    "        num_ix = num_fields * (num_fields - 1) // 2\n",
    "        if kernel_type == 'mat':\n",
    "            kernel_shape = embed_dim, num_ix, embed_dim\n",
    "        elif kernel_type == 'vec':\n",
    "            kernel_shape = num_ix, embed_dim\n",
    "        elif kernel_type == 'num':\n",
    "            kernel_shape = num_ix, 1\n",
    "        else:\n",
    "            raise ValueError('unknown kernel type: ' + kernel_type)\n",
    "        self.kernel_type = kernel_type\n",
    "        self.kernel = torch.nn.Parameter(torch.zeros(kernel_shape))\n",
    "        torch.nn.init.xavier_uniform_(self.kernel.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        p, q = x[:, row], x[:, col]\n",
    "        if self.kernel_type == 'mat':\n",
    "            kp = torch.sum(p.unsqueeze(1) * self.kernel, dim=-1).permute(0, 2, 1)\n",
    "            return torch.sum(kp * q, -1)\n",
    "        else:\n",
    "            return torch.sum(p * q * self.kernel.unsqueeze(0), -1)\n",
    "\n",
    "\n",
    "class CrossNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.w = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, 1, bias=False) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.b = torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.zeros((input_dim,))) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            xw = self.w[i](x)\n",
    "            x = x0 * xw + self.b[i] + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionalFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, attn_size, dropouts):\n",
    "        super().__init__()\n",
    "        self.attention = torch.nn.Linear(embed_dim, attn_size)\n",
    "        self.projection = torch.nn.Linear(attn_size, 1)\n",
    "        self.fc = torch.nn.Linear(embed_dim, 1)\n",
    "        self.dropouts = dropouts\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        p, q = x[:, row], x[:, col]\n",
    "        inner_product = p * q\n",
    "        attn_scores = F.relu(self.attention(inner_product))\n",
    "        attn_scores = F.softmax(self.projection(attn_scores), dim=1)\n",
    "        attn_scores = F.dropout(attn_scores, p=self.dropouts[0], training=self.training)\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)\n",
    "        attn_output = F.dropout(attn_output, p=self.dropouts[1], training=self.training)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CompressedInteractionNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, cross_layer_sizes, split_half=True):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(cross_layer_sizes)\n",
    "        self.split_half = split_half\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        prev_dim, fc_input_dim = input_dim, 0\n",
    "        for i in range(self.num_layers):\n",
    "            cross_layer_size = cross_layer_sizes[i]\n",
    "            self.conv_layers.append(torch.nn.Conv1d(input_dim * prev_dim, cross_layer_size, 1,\n",
    "                                                    stride=1, dilation=1, bias=True))\n",
    "            if self.split_half and i != self.num_layers - 1:\n",
    "                cross_layer_size //= 2\n",
    "            prev_dim = cross_layer_size\n",
    "            fc_input_dim += prev_dim\n",
    "        self.fc = torch.nn.Linear(fc_input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        xs = []\n",
    "        x0, h = x.unsqueeze(2), x\n",
    "        for i in range(self.num_layers):\n",
    "            x = x0 * h.unsqueeze(1)\n",
    "            batch_size, f0_dim, fin_dim, embed_dim = x.shape\n",
    "            x = x.view(batch_size, f0_dim * fin_dim, embed_dim)\n",
    "            x = F.relu(self.conv_layers[i](x))\n",
    "            if self.split_half and i != self.num_layers - 1:\n",
    "                x, h = torch.split(x, x.shape[1] // 2, dim=1)\n",
    "            else:\n",
    "                h = x\n",
    "            xs.append(x)\n",
    "        return self.fc(torch.sum(torch.cat(xs, dim=1), 2))\n",
    "\n",
    "\n",
    "class AnovaKernel(torch.nn.Module):\n",
    "    def __init__(self, order, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        batch_size, num_fields, embed_dim = x.shape\n",
    "        a_prev = torch.ones((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "        for t in range(self.order):\n",
    "            a = torch.zeros((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "            a[:, t+1:, :] += x[:, t:, :] * a_prev[:, t:-1, :]\n",
    "            a = torch.cumsum(a, dim=1)\n",
    "            a_prev = a\n",
    "        if self.reduce_sum:\n",
    "            return torch.sum(a[:, -1, :], dim=-1, keepdim=True)\n",
    "        else:\n",
    "            return a[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6638c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# FM Model\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, input_dim, factors_num):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.interaction = nn.Parameter(torch.rand(input_dim, factors_num), requires_grad=True)\n",
    "        nn.init.uniform_(self.interaction, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear_term = self.linear(x)\n",
    "        interaction_1 = torch.matmul(x, self.interaction).pow(2)\n",
    "        interaction_2 = torch.matmul(x. pow(2), self.interaction.pow(2))\n",
    "        interaction_term = 0.5 * torch.sum(interaction_2 - interaction_1, 1, keepdim=True)\n",
    "        output = linear_term + interaction_term\n",
    "        return output\n",
    "\n",
    "# DeepFM Model\n",
    "class DeepFM(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_sizes, embedding_size, hidden_dims, num_classes=1, dropout=[0.5, 0.5]):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.field_size = len(feature_sizes)\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_classes = num_classes\n",
    "        # self.dtype = torch.long\n",
    "        # self.bias = torch.nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        \n",
    "        # FM part\n",
    "        self.fm_first_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, 1) for feature_size in self.feature_sizes])\n",
    "        self.fm_second_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, self.embedding_size) for feature_size in self.feature_sizes])\n",
    "        \n",
    "        # Deep part\n",
    "        all_dims = [self.field_size * self.embedding_size] + self.hidden_dims\n",
    "        for i in range(1, len(hidden_dims) + 1):\n",
    "            setattr(self, 'linear_'+str(i), nn.Linear(all_dims[i-1], all_dims[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(all_dims[i]))\n",
    "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
    "            setattr(self, 'dropout_'+str(i), nn.Dropout(dropout[i-1]))\n",
    "        \n",
    "    def forward(self, Xi, Xv):\n",
    "        \n",
    "        # FM part\n",
    "        fm_first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_first_order_embeddings)]\n",
    "        fm_first_order = torch.cat(fm_first_order_emb_arr, 1)\n",
    "        fm_second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_second_order_embeddings)]\n",
    "        fm_sum_second_order_emb = sum(fm_second_order_emb_arr)\n",
    "        fm_sum_second_order_emb_square = fm_sum_second_order_emb * fm_sum_second_order_emb  # (x+y)^2\n",
    "        fm_second_order_emb_square = [\n",
    "            item*item for item in fm_second_order_emb_arr]\n",
    "        fm_second_order_emb_square_sum = sum(\n",
    "            fm_second_order_emb_square)  # x^2+y^2\n",
    "        fm_second_order = (fm_sum_second_order_emb_square - fm_second_order_emb_square_sum) * 0.5\n",
    "        \n",
    "        # Deep part\n",
    "        deep_emb = torch.cat(fm_second_order_emb_arr, 1)\n",
    "        deep_out = deep_emb\n",
    "        for i in range(1, len(self.hidden_dims) + 1):\n",
    "            deep_out = getattr(self, 'linear_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'batchNorm_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'activation_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'dropout_' + str(i))(deep_out)\n",
    "        \n",
    "        # Sum part\n",
    "        total_sum = torch.sum(fm_first_order, 1) + torch.sum(fm_second_order, 1) + torch.sum(deep_out, 1)\n",
    "        \n",
    "        return total_sum\n",
    "\n",
    "# AFM Model\n",
    "class AttentionalFactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Attentional Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        J Xiao, et al. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, attn_size, dropouts):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.afm = AttentionalFactorizationMachine(embed_dim, attn_size, dropouts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "        x = self.linear(x) + self.afm(embed_x)\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added\n",
    "\n",
    "# DCN Model\n",
    "class DeepCrossNetworkModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Deep & Cross Network.\n",
    "\n",
    "    Reference:\n",
    "        R Wang, et al. Deep & Cross Network for Ad Click Predictions, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, num_layers, mlp_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.num_fields = len(field_dims) # manual added\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.cn = CrossNetwork(self.embed_output_dim, num_layers)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout, output_layer=False)\n",
    "        self.linear = torch.nn.Linear(mlp_dims[-1] + self.embed_output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x).view(-1, self.embed_output_dim)\n",
    "        embed_x = embed_x.view(-1, self.embed_output_dim) # manual added\n",
    "        x_l1 = self.cn(embed_x)\n",
    "        h_l2 = self.mlp(embed_x)\n",
    "        x_stack = torch.cat([x_l1, h_l2], dim=1)\n",
    "        p = self.linear(x_stack)\n",
    "#         return torch.sigmoid(p.squeeze(1))\n",
    "        return p.squeeze(1) # manual added\n",
    "\n",
    "# xDeepFM Model\n",
    "class ExtremeDeepFactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of xDeepFM.\n",
    "\n",
    "    Reference:\n",
    "        J Lian, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, mlp_dims, dropout, cross_layer_sizes, split_half=True):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims) # manual added\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.cin = CompressedInteractionNetwork(len(field_dims), cross_layer_sizes, split_half)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x)\n",
    "        x = self.linear(x) + self.cin(embed_x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added\n",
    "\n",
    "# AutoInt Model\n",
    "class AutomaticFeatureInteractionModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AutoInt.\n",
    "\n",
    "    Reference:\n",
    "        W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.atten_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.atten_output_dim = len(field_dims) * atten_embed_dim\n",
    "        self.has_residual = has_residual\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropouts[1])\n",
    "        self.self_attns = torch.nn.ModuleList([\n",
    "            torch.nn.MultiheadAttention(atten_embed_dim, num_heads, dropout=dropouts[0]) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.attn_fc = torch.nn.Linear(self.atten_output_dim, 1)\n",
    "        if self.has_residual:\n",
    "            self.V_res_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"x:\", x.size())\n",
    "#         embed_x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim)\n",
    "        embed_x = x.view(embed_shape)\n",
    "#         print(\"embed_x:\", embed_x.size())\n",
    "        atten_x = self.atten_embedding(embed_x)\n",
    "#         print(\"atten_x:\", atten_x.size())\n",
    "        cross_term = atten_x.transpose(0, 1)\n",
    "#         cross_term = atten_x\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        for self_attn in self.self_attns:\n",
    "            cross_term, _ = self_attn(cross_term, cross_term, cross_term)\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        cross_term = cross_term.transpose(0, 1)\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        if self.has_residual:\n",
    "            V_res = self.V_res_embedding(embed_x)\n",
    "#             print(\"V_res\", V_res.size())\n",
    "            cross_term += V_res\n",
    "        cross_term = F.relu(cross_term).contiguous().view(-1, self.atten_output_dim)\n",
    "        x = self.linear(x) + self.attn_fc(cross_term) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "        return x.squeeze(1)\n",
    "\n",
    "# AFN Model\n",
    "class LNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of LNN layer\n",
    "    Input shape\n",
    "        - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "    Output shape\n",
    "        - 2D tensor with shape:``(batch_size,LNN_dim*embedding_size)``.\n",
    "    Arguments\n",
    "        - **in_features** : Embedding of feature.\n",
    "        - **num_fields**: int.The field size of feature.\n",
    "        - **LNN_dim**: int.The number of Logarithmic neuron.\n",
    "        - **bias**: bool.Whether or not use bias in LNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_fields, embed_dim, LNN_dim, bias=False):\n",
    "        super(LNN, self).__init__()\n",
    "        self.num_fields = num_fields\n",
    "        self.embed_dim = embed_dim\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.lnn_output_dim = LNN_dim * embed_dim\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(LNN_dim, num_fields))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(LNN_dim, embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields, embedding_size)``\n",
    "        \"\"\"\n",
    "        embed_x_abs = torch.abs(x) # Computes the element-wise absolute value of the given input tensor.\n",
    "        embed_x_afn = torch.add(embed_x_abs, 1e-7)\n",
    "        # Logarithmic Transformation\n",
    "        embed_x_log = torch.log1p(embed_x_afn) # torch.log1p and torch.expm1\n",
    "        lnn_out = torch.matmul(self.weight, embed_x_log)\n",
    "        if self.bias is not None:\n",
    "            lnn_out += self.bias\n",
    "        lnn_exp = torch.expm1(lnn_out)\n",
    "        output = F.relu(lnn_exp).contiguous().view(-1, self.lnn_output_dim)\n",
    "        return output\n",
    "\n",
    "class AdaptiveFactorizationNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AFN.\n",
    "\n",
    "    Reference:\n",
    "        Cheng W, et al. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, 2019.\n",
    "    \"\"\"\n",
    "    def __init__(self, field_dims, embed_dim, LNN_dim, mlp_dims, dropouts):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.linear = FeaturesLinear(field_dims)    # Linear\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)   # Embedding\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.LNN_output_dim = self.LNN_dim * embed_dim\n",
    "        self.LNN = LNN(self.num_fields, embed_dim, LNN_dim)\n",
    "        self.mlp = MultiLayerPerceptron(self.LNN_output_dim, mlp_dims, dropouts[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x)\n",
    "        lnn_out = self.LNN(embed_x)\n",
    "        x = self.linear(x) + self.mlp(lnn_out)\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
