{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b570aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import nbformat\n",
    "\n",
    "# def load_notebook(notebook_path):\n",
    "#     with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#         nb = nbformat.read(f, as_version=4)\n",
    "#     code_cells = [cell.source for cell in nb.cells if cell.cell_type == 'code']\n",
    "#     exec('\\n'.join(code_cells), globals())\n",
    "\n",
    "# # import written function and variable\n",
    "\n",
    "# parent_directory = Path('./')\n",
    "# data_preprocessing_utils_path = parent_directory / 'data_preprocessing_utils.ipynb'\n",
    "# load_notebook(data_preprocessing_utils_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9c33e",
   "metadata": {},
   "source": [
    "# Obtain and log best training and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def obtain_best_train_params(model_name, record_data_df):\n",
    "    \n",
    "    train_best_params = {\n",
    "        'MSE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "        'MAE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "        'RMSE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "    }\n",
    "    \n",
    "    if model_name == \"AutoInt\":\n",
    "        record_data_df = record_data_df[(record_data_df[f\"{model_name}_atten_embed_dim\"]==32) &\n",
    "                                    (record_data_df[f\"{model_name}_num_layers\"]==3) &\n",
    "                                    (record_data_df[f\"{model_name}_mlp_dims\"]=='(256, 256, 256)') &\n",
    "                                    (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "    elif model_name == \"MLP\":\n",
    "        record_data_df = record_data_df[(record_data_df[f\"{model_name}_embed_dims\"]=='(256, 256, 256)') &\n",
    "                                    (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "    batch_size = 'Batch_size'\n",
    "    dl_learning_rate = 'DL_learning_rate'\n",
    "    dropouts = f'{model_name}_dropouts'\n",
    "    dropout = f'{model_name}_dropout'\n",
    "\n",
    "    with open(f\"{model_name}_train_best_params.txt\", \"w\") as f:\n",
    "        for metric in ['MSE', 'MAE', 'RMSE']:\n",
    "            print(f'Train {metric}:\\n')\n",
    "            min_metric_indices = record_data_df[metric].nsmallest(5).index\n",
    "            n = 0\n",
    "            for index in min_metric_indices:\n",
    "                n += 1\n",
    "                train_best_params[metric][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                train_best_params[metric][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                if model_name == \"MLP\":\n",
    "                    train_best_params[metric][f'{n}']['dropout'] = float(record_data_df.loc[index, dropout])\n",
    "                else: # AutoInt\n",
    "                    train_best_params[metric][f'{n}']['dropout'] = ast.literal_eval(record_data_df.loc[index, dropouts])\n",
    "                print(f'{n}. ',\n",
    "                    \"batch_size:\", train_best_params[metric][f'{n}']['batch_size'], \n",
    "                    \"dl_learning_rate:\", train_best_params[metric][f'{n}']['dl_learning_rate'], \n",
    "                    \"dropout:\", train_best_params[metric][f'{n}']['dropout'],\n",
    "                    #               \"\\n\",\n",
    "                    \"MSE\", record_data_df.loc[index, 'MSE'], \n",
    "                    \"MAE\", record_data_df.loc[index, 'MAE'], \n",
    "                    \"RMSE\", record_data_df.loc[index, 'RMSE'], \n",
    "                    \"Index:\", index\n",
    "                )\n",
    "                f.write(f'Train {metric}:\\n')\n",
    "                f.write(f'{n}. ' + \n",
    "                      \" batch_size: \" + str(train_best_params[metric][f'{n}']['batch_size']) + \n",
    "                      \" dl_learning_rate: \" + str(train_best_params[metric][f'{n}']['dl_learning_rate']) + \n",
    "                      \" dropout: \" + str(train_best_params[metric][f'{n}']['dropout']) +\n",
    "        #               \"\\n\",\n",
    "                      \" MSE: \" + str(record_data_df.loc[index, 'MSE']) +\n",
    "                      \" MAE: \" + str(record_data_df.loc[index, 'MAE']) +\n",
    "                      \" RMSE: \" + str(record_data_df.loc[index, 'RMSE']) + \n",
    "                      \" Index: \" + str(index)\n",
    "                )\n",
    "            print(\"\\n\")\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "    return train_best_params\n",
    "    \n",
    "def obtain_best_all_params(model_name, record_data, train_best_params=None):\n",
    "    \n",
    "    if model_name in [\"AutoInt\", \"MLP\"]:\n",
    "        \n",
    "        if train_best_params == None:\n",
    "            print(\"AutoInt and MLP need training params first !\")\n",
    "            return 0\n",
    "        \n",
    "        best_all_params = {\n",
    "            \"Train_MSE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },\n",
    "            \"Train_MAE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },\n",
    "            \"Train_RMSE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },    \n",
    "        }\n",
    "        \n",
    "        batch_size = 'Batch_size'\n",
    "        dl_learning_rate = 'DL_learning_rate'\n",
    "        dropouts = f'{model_name}_dropouts'\n",
    "        dropout = f'{model_name}_dropout'\n",
    "        \n",
    "        atten_embed_dim = f\"{model_name}_atten_embed_dim\" # AutoInt\n",
    "        num_layers = f\"{model_name}_num_layers\" # AutoInt\n",
    "        mlp_dims = f\"{model_name}_mlp_dims\" # AutoInt\n",
    "        embed_dims = f\"{model_name}_embed_dims\" # MLP\n",
    "        \n",
    "        for train_metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            \n",
    "            record_data_df = record_data\n",
    "            \n",
    "            metric_batch_size = train_best_params[train_metric]['1']['batch_size']\n",
    "            metric_dl_learning_rate = train_best_params[train_metric]['1']['dl_learning_rate']\n",
    "            metric_dropout = train_best_params[train_metric]['1']['dropout']\n",
    "#             print(\"metric_batch_size:\", metric_batch_size, \"metric_batch_size type:\", type(metric_batch_size))\n",
    "#             print(\"metric_dl_learning_rate:\", metric_dl_learning_rate, \"metric_dl_learning_rate type:\", type(metric_dl_learning_rate))\n",
    "#             print(\"metric_dropout:\", metric_dropout, \"metric_dropout type:\", type(metric_dropout))\n",
    "            record_data_df = record_data_df[(record_data_df[f\"{batch_size}\"]==metric_batch_size) &\n",
    "                                        (record_data_df[f\"{dl_learning_rate}\"]==metric_dl_learning_rate) &\n",
    "                                        (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "#             print(len(record_data_df))\n",
    "            if model_name == \"AutoInt\":\n",
    "                record_data_df = record_data_df[record_data_df[f\"{dropouts}\"]==str(metric_dropout)]\n",
    "#                 print(len(record_data_df))\n",
    "            elif model_name == \"MLP\":\n",
    "                record_data_df = record_data_df[record_data_df[f\"{dropout}\"]==metric_dropout]\n",
    "#                 print(len(record_data_df))\n",
    "            else:\n",
    "                print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "                return 0\n",
    "            \n",
    "            for model_metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            \n",
    "                min_metric_indices = record_data_df[model_metric].nsmallest(5).index\n",
    "                n = 0\n",
    "                for index in min_metric_indices:\n",
    "                    n += 1\n",
    "                    best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                    best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                    \n",
    "                    if model_name == \"AutoInt\":\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dropout'] = ast.literal_eval(record_data_df.loc[index, dropouts])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['atten_embed_dim'] = int(record_data_df.loc[index, atten_embed_dim])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['num_layers'] = int(record_data_df.loc[index, num_layers])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['mlp_dims'] = ast.literal_eval(record_data_df.loc[index, mlp_dims])\n",
    "                        \n",
    "                    elif model_name == \"MLP\":\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dropout'] = float(record_data_df.loc[index, dropout])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['embed_dims'] = ast.literal_eval(record_data_df.loc[index, embed_dims])\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "                        return 0\n",
    "        \n",
    "        print_nested_dict(best_all_params)\n",
    "        with open(f\"{model_name}_all_best_params.txt\", \"w\") as f:\n",
    "            write_nested_dict_to_file(best_all_params, f)\n",
    "        \n",
    "        return best_all_params\n",
    "    \n",
    "    elif model_name in [\"FM\", \"XGBoost\", \"RandomForest\"]:\n",
    "        \n",
    "        best_all_params = {\n",
    "            \"MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "            \"MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "            \"RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "        }\n",
    "        \n",
    "        # FM\n",
    "        batch_size = 'Batch_size'\n",
    "        dl_learning_rate = 'DL_learning_rate'\n",
    "        factors_num = f\"{model_name}_factors_num\"\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_colsample_bytree = f\"{model_name}_colsample_bytree\"\n",
    "        xgb_lambda = f\"{model_name}_lambda\"\n",
    "        xgb_learning_rate = f\"{model_name}_learning_rate\"\n",
    "        xgb_max_depth = f\"{model_name}_max_depth\"\n",
    "        xgb_min_child_weight = f\"{model_name}_min_child_weight\"\n",
    "        xgb_n_estimators = f\"{model_name}_n_estimators\"\n",
    "        xgb_subsample = f\"{model_name}_subsample\"\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_max_depth = f\"{model_name}_max_depth\"\n",
    "        rf_max_features = f\"{model_name}_max_features\"\n",
    "        rf_min_samples_leaf = f\"{model_name}_min_samples_leaf\"\n",
    "        rf_min_samples_split = f\"{model_name}_min_samples_split\"\n",
    "        rf_n_estimators = f\"{model_name}_n_estimators\"\n",
    "        \n",
    "        record_data_df = record_data\n",
    "        \n",
    "        if model_name == \"FM\":\n",
    "            record_data_df = record_data_df[record_data_df[\"Epoch/Epochs_num\"]=='100th/100']\n",
    "        \n",
    "        for metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            min_metric_indices = record_data_df[metric].nsmallest(5).index\n",
    "            n = 0\n",
    "            for index in min_metric_indices:\n",
    "                n += 1\n",
    "                if model_name == \"FM\":\n",
    "                    best_all_params[metric][f'{n}']['factors_num'] = int(record_data_df.loc[index, factors_num])\n",
    "                    best_all_params[metric][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                    best_all_params[metric][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                \n",
    "                elif model_name == \"XGBoost\":\n",
    "                    best_all_params[metric][f'{n}']['colsample_bytree'] = float(record_data_df.loc[index, xgb_colsample_bytree])\n",
    "                    best_all_params[metric][f'{n}']['lambda'] = float(record_data_df.loc[index, xgb_lambda])\n",
    "                    best_all_params[metric][f'{n}']['learning_rate'] = float(record_data_df.loc[index, xgb_learning_rate])\n",
    "                    best_all_params[metric][f'{n}']['max_depth'] = int(record_data_df.loc[index, xgb_max_depth])\n",
    "                    best_all_params[metric][f'{n}']['min_child_weight'] = int(record_data_df.loc[index, xgb_min_child_weight])\n",
    "                    best_all_params[metric][f'{n}']['n_estimators'] = int(record_data_df.loc[index, xgb_n_estimators])\n",
    "                    best_all_params[metric][f'{n}']['subsample'] = float(record_data_df.loc[index, xgb_subsample])\n",
    "    \n",
    "                elif model_name == \"RandomForest\":\n",
    "                    max_feature_num = record_data_df.loc[index, rf_max_features]\n",
    "                    max_feature_num = int(max_feature_num) if max_feature_num.isdigit() else max_feature_num\n",
    "                    max_depth = record_data_df.loc[index, rf_max_depth]\n",
    "                    max_depth = None if math.isnan(max_depth) else int(max_depth)\n",
    "                    best_all_params[metric][f'{n}']['max_depth'] = max_depth\n",
    "                    best_all_params[metric][f'{n}']['max_features'] = max_feature_num\n",
    "                    best_all_params[metric][f'{n}']['min_samples_leaf'] = int(record_data_df.loc[index, rf_min_samples_leaf])\n",
    "                    best_all_params[metric][f'{n}']['min_samples_split'] = int(record_data_df.loc[index, rf_min_samples_split])\n",
    "                    best_all_params[metric][f'{n}']['n_estimators'] = int(record_data_df.loc[index, rf_n_estimators])\n",
    "        \n",
    "        print_nested_dict(best_all_params)\n",
    "        with open(f\"{model_name}_all_best_params.txt\", \"w\") as f:\n",
    "            write_nested_dict_to_file(best_all_params, f)\n",
    "        \n",
    "        return best_all_params\n",
    "        \n",
    "    else:\n",
    "        print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36ed8e",
   "metadata": {},
   "source": [
    "# Mediators (pass best params to training validation testing function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f470b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def generate_grid_search_combinations(model_name, is_model_params=False):\n",
    "    \n",
    "    current_models_two_phase = [\"AutoInt\", \"MLP\"]\n",
    "    current_models_one_phase = [\"FM\", \"XGBoost\", \"RandomForest\"]\n",
    "    \n",
    "    params_combinations = 0\n",
    "    \n",
    "    if model_name == \"FM\":\n",
    "        # optimizers = [\"Adam\", \"SGD\"]\n",
    "        factors_num_list = [4, 8, 16, 32, 64]\n",
    "        optimizers = [\"Adam\"]\n",
    "        batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "        learning_rates = [0.0001, 0.0005, 0.001, 0.005]\n",
    "        # dropouts = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        params_combinations = list(product(optimizers, batch_sizes, learning_rates, factors_num_list))\n",
    "    \n",
    "    elif model_name == \"AutoInt\":\n",
    "        if is_model_params:\n",
    "            atten_embed_dim_list = [32, 64]\n",
    "            num_layers = [2, 3, 4, 5]\n",
    "            mlp_dims = [tuple([2**size]*3) for size in range(5, 10)]\n",
    "            params_combinations = list(product(atten_embed_dim_list, num_layers, mlp_dims))\n",
    "        else:\n",
    "            # optimizers = [\"Adam\", \"SGD\"]\n",
    "            optimizers = [\"Adam\"]\n",
    "            batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "            learning_rates = [0.0001, 0.0005, 0.001, 0.005]\n",
    "            dropouts = [tuple([size * 0.1] * 3) for size in range(1, 10, 2)]\n",
    "            params_combinations = list(product(optimizers, batch_sizes, learning_rates, dropouts))\n",
    "    \n",
    "    elif model_name == \"MLP\":\n",
    "        if is_model_params:\n",
    "            embed_dims = [tuple([2**size]*3) for size in range(5, 10)]\n",
    "            params_combinations = list(product(embed_dims))\n",
    "        else:\n",
    "            # optimizers = [\"Adam\", \"SGD\"]\n",
    "            optimizers = [\"Adam\"]\n",
    "            batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "            learning_rates = [0.0001, 0.0005, 0.001, 0.005]\n",
    "            dropouts = [size * 0.1 for size in range(1, 10, 2)]\n",
    "            params_combinations = list(product(optimizers, batch_sizes, learning_rates, dropouts))\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        learning_rates = [0.001, 0.01, 0.1]\n",
    "        n_estimators_list = [50, 100, 200]\n",
    "        subsample_list = [0.8, 1]\n",
    "        colsample_bytree_list = [0.8, 1]\n",
    "        min_child_weight_list = [1, 3, 5]\n",
    "        max_depth_list = [4, 5, 6]\n",
    "        lambda_list = [0, 0.1, 0.5]\n",
    "        params_combinations = list(product(learning_rates, n_estimators_list, subsample_list, colsample_bytree_list,\n",
    "                                              min_child_weight_list, max_depth_list, lambda_list))\n",
    "    \n",
    "    elif model_name == \"RandomForest\":\n",
    "        n_estimators_list = [50, 100, 200] # to 200\n",
    "        max_features_list = [3, 5, 'sqrt', 'log2'] # None, most takes too long, so remove none\n",
    "        max_depth_list = [5, 10, None]\n",
    "        min_samples_split_list = [2, 4, 6] # to 4\n",
    "        min_samples_leaf_list = [1, 3, 5] # 1, 3, 5\n",
    "        params_combinations = list(product(n_estimators_list, max_features_list, max_depth_list, \n",
    "                                               min_samples_split_list, min_samples_leaf_list))\n",
    "    \n",
    "    else:\n",
    "        print(f\"Please make sure model in {current_models_two_phase} and {current_models_one_phase} !\")\n",
    "        return 0\n",
    "    \n",
    "    return params_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de60afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_mediator(model_name, input_dim, field_num, embedding_size=768, params_input=None, \n",
    "                    validating=False, training_combo=None, model_combo=None, train_best_params=None, \n",
    "                    testing=True, best_all_params=None):\n",
    "    \n",
    "#     input_dim = len(X_train[0])\n",
    "#     embedding_size = 768\n",
    "#     field_num = len(columns_to_train)\n",
    "   \n",
    "    FM_params = {\n",
    "        'hyperparameters': {\n",
    "            'input_dim': input_dim, \n",
    "    #         'factors_num': [5, 10, 15],\n",
    "    #         'factors_num': [i+1 for i in range(20)]\n",
    "    #         'factors_num': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    #         'factors_num': [2**i for i in range(3, 7)],\n",
    "            'factors_num': 8\n",
    "        },\n",
    "\n",
    "        'task_type': 'Regression', 'loss_type': 'MSE', 'optimizer_type': 'Adam', \n",
    "    #   'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': 0.001,\n",
    "        'epochs_num': 100, \n",
    "    #   'batch_size': [100, 500, 1000]\n",
    "        'batch_size': 512\n",
    "\n",
    "    }\n",
    "    \n",
    "    # AutoInt\n",
    "    # field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True\n",
    "    # embed_dim=16, atten_embed_dim=64, num_heads=2, num_layers=3, mlp_dims=(400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "    # Paper:\n",
    "    # embed_dim is set to 16, batch size set to 1024\n",
    "    # num_head is 2, num_layer(interaction layer) is 3, num of hidden units (interaction layer) is 32\n",
    "    # test dropout from 0.1 ~ 0.9\n",
    "    # optimizer is Adam\n",
    "    # test num_layer(interaction layer) from 0 ~ 4, 1 increase dramaticaly, 1 ~ 4 become stable\n",
    "    # test atten_embed_dim 8, 16, 24, 32, movie len is getting better, for KDD12, 24 is best, then decrease\n",
    "    # hidden units shape is set to (1, 200) or (4, 100)\n",
    "    # residaul is crucial\n",
    "\n",
    "    AutoInt_params = {\n",
    "        'hyperparameters': {\n",
    "            'field_dims': [embedding_size for i in range(field_num)], \n",
    "    #         'embed_dim': [768], \n",
    "            'embed_dim': 768, \n",
    "    #         'atten_embed_dim': [(64, 32)],\n",
    "    #         'atten_embed_dim': [2**i for i in range(2, 7)],\n",
    "            'atten_embed_dim': 32,\n",
    "    #         \"num_heads\": [2],\n",
    "            \"num_heads\": 2,\n",
    "    #         \"num_layers\": [3],\n",
    "    #         \"num_layers\": [i+1 for i in range(5)],\n",
    "            \"num_layers\": 3,\n",
    "    #         'mlp_dims': [(16, 16), (400, 400)],\n",
    "    #         'mlp_dims': [tuple([size] * num_layers) for num_layers in range(1, 6) for size in [i*100 for i in range(1, 6)]],\n",
    "            'mlp_dims': (256, 256, 256),\n",
    "    #         'dropouts': [(0.5, 0.5, 0.5)],\n",
    "    #         'dropouts': [tuple([size * 0.1] * 3) for size in range(1, 10)],\n",
    "    #         'dropouts': (0.0, 0.0, 0.0),\n",
    "            'dropouts': (0.5, 0.5, 0.5),\n",
    "            \"has_residual\": True\n",
    "        },\n",
    "\n",
    "        'task_type': 'Regression', 'loss_type': 'MSE', 'optimizer_type': 'Adam', \n",
    "    #         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "    #     'dl_learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "        'dl_learning_rate': 0.001,\n",
    "    #     'epochs_num': [10, 20, 30], \n",
    "        'epochs_num': 100,\n",
    "    #         'batch_size': [100, 500, 1000]\n",
    "    #     'batch_size': [128, 256, 512, 1024]\n",
    "        'batch_size': 512\n",
    "\n",
    "    }\n",
    "    \n",
    "    # MLP\n",
    "    # input_dim, embed_dims, dropout\n",
    "\n",
    "    MLP_params = {\n",
    "        'hyperparameters': {\n",
    "            'input_dim': input_dim, \n",
    "    #         'factors_num': [5, 10, 15],\n",
    "            'embed_dims': (256, 256, 256),\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "\n",
    "        'task_type': 'Regression', 'loss_type': 'MSE', 'optimizer_type': 'Adam', \n",
    "    #         'dl_learning_rate': [0.01, 0.02, 0.001], \n",
    "        'dl_learning_rate': 0.001,\n",
    "        'epochs_num': 100, \n",
    "    #         'batch_size': [100, 500, 1000]\n",
    "        'batch_size': 512\n",
    "\n",
    "    }\n",
    "    \n",
    "    XGBoost_params = {\n",
    "        'hyperparameters': {\n",
    "    #         'learning_rate': [0.01, 0.02, 0.001],\n",
    "    #         'learning_rate': [i/100 for i in range(1, 11)],\n",
    "            'learning_rate': 0.01,\n",
    "    #         'n_estimators': [50, 100],\n",
    "    #         'n_estimators': [20, 40, 60, 80, 100],\n",
    "            'n_estimators': 50,\n",
    "    #         'subsample': [1],\n",
    "    #         'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'subsample': 0.8,\n",
    "    #         'colsample_bytree': [1]\n",
    "    #         'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "            'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 1,\n",
    "            'max_depth': 5,\n",
    "    #         'gamma': 0,\n",
    "#             'alpha':0,\n",
    "            'lambda':0\n",
    "        },\n",
    "        'task_type': 'Regression', 'loss_type': None, 'optimizer_type': None, \n",
    "        'dl_learning_rate': None, 'epochs_num': None, 'batch_size': None\n",
    "    }\n",
    "\n",
    "    RandomForest_params = {\n",
    "        'hyperparameters': {\n",
    "            'max_depth': 5,\n",
    "            'max_features': 3,\n",
    "            'min_samples_leaf': 5,\n",
    "            'min_samples_split': 2,\n",
    "            'n_estimators': 200\n",
    "    #         'max_leaf_nodes':,\n",
    "    #         'bootstrap':,\n",
    "    #         'min_impurity_decrease':,\n",
    "    #         'min_weight_fraction_leaf':\n",
    "        },\n",
    "        'task_type': 'Regression', 'loss_type': None, 'optimizer_type': None, \n",
    "        'dl_learning_rate': None, 'epochs_num': None, 'batch_size': None\n",
    "    }\n",
    "\n",
    "    current_models_two_phase = [\"AutoInt\", \"MLP\"]\n",
    "    current_models_one_phase = [\"FM\", \"XGBoost\", \"RandomForest\"]\n",
    "    top_n = '1'\n",
    "    \n",
    "    if validating == True and testing == False:\n",
    "        \n",
    "        params = 0\n",
    "        \n",
    "        if model_name == \"FM\":\n",
    "            if training_combo != None:\n",
    "                params = FM_params\n",
    "                optimizer, batch_size, learning_rate, factor_num = combo\n",
    "                params['optimizer_type'] = optimizer\n",
    "                params['batch_size'] = batch_size\n",
    "                params['dl_learning_rate'] = learning_rate\n",
    "                params['hyperparameters']['factors_num'] = factor_num\n",
    "            else:\n",
    "                print(f\"If you want to validate {model_name}, make sure training_combo not None !\")\n",
    "                return 0\n",
    "            \n",
    "        elif model_name == \"AutoInt\":\n",
    "            if training_combo != None and model_combo == None:\n",
    "                optimizer, batch_size, learning_rate, dropout = training_combo\n",
    "                params = params_input if params_input != None else AutoInt_params\n",
    "                params['optimizer_type'] = optimizer\n",
    "                params['batch_size'] = batch_size\n",
    "                params['dl_learning_rate'] = learning_rate\n",
    "                params['hyperparameters']['dropouts'] = dropout\n",
    "            elif training_combo == None and model_combo != None and train_best_params != None:\n",
    "                atten_dim, num_layer, mlp_dim = model_combo\n",
    "                params = params_input if params_input != None else AutoInt_params\n",
    "                params['batch_size'] = train_best_params[top_n]['batch_size']\n",
    "                params['dl_learning_rate'] = train_best_params[top_n]['dl_learning_rate']\n",
    "                params['hyperparameters']['dropouts'] = train_best_params[top_n]['dropout']\n",
    "                params['hyperparameters']['atten_embed_dim'] = atten_dim\n",
    "                params['hyperparameters']['num_layers'] = num_layer\n",
    "                params['hyperparameters']['mlp_dims'] = mlp_dim\n",
    "            else:\n",
    "                print(f\"If you want to validate {model_name}, make sure training_combo not None or model_combo and train_best_params not None !\")\n",
    "                return 0\n",
    "        \n",
    "        elif model_name == \"MLP\":\n",
    "            if training_combo != None and model_combo == None:\n",
    "                optimizer, batch_size, learning_rate, dropout = training_combo\n",
    "                params = params_input if params_input != None else MLP_params\n",
    "                params['optimizer_type'] = optimizer\n",
    "                params['batch_size'] = batch_size\n",
    "                params['dl_learning_rate'] = learning_rate\n",
    "                params['hyperparameters']['dropout'] = round(dropout, 1)\n",
    "            elif training_combo == None and model_combo != None and train_best_params != None:\n",
    "                embed_dim = model_combo[0]\n",
    "                params = MLP_params\n",
    "                params['batch_size'] = train_best_params[top_n]['batch_size']\n",
    "                params['dl_learning_rate'] = train_best_params[top_n]['dl_learning_rate']\n",
    "                params['hyperparameters']['dropout'] = round(train_best_params[top_n]['dropout'], 1)\n",
    "                params['hyperparameters']['embed_dims'] = embed_dim\n",
    "            else:\n",
    "                print(f\"If you want to validate {model_name}, make sure training_combo not None or model_combo and train_best_params not None !\")\n",
    "                return 0\n",
    "        \n",
    "        elif model_name == \"XGBoost\":\n",
    "            if training_combo != None:\n",
    "                learning_rate, n_estimator, subsample, colsample_bytree, min_child_weight, max_depth, lambda_value = training_combo\n",
    "                params = params_input if params_input != None else XGBoost_params \n",
    "                params['hyperparameters']['learning_rate'] = float(learning_rate) # validation reocord dtype is float\n",
    "                params['hyperparameters']['n_estimators'] = n_estimator\n",
    "                params['hyperparameters']['subsample'] = float(subsample)\n",
    "                params['hyperparameters']['colsample_bytree'] = float(colsample_bytree)\n",
    "                params['hyperparameters']['min_child_weight'] = min_child_weight\n",
    "                params['hyperparameters']['max_depth'] = max_depth\n",
    "                params['hyperparameters']['lambda'] = float(lambda_value)\n",
    "            else:\n",
    "                print(f\"If you want to validate {model_name}, make sure training_combo not None !\")\n",
    "                return 0\n",
    "        \n",
    "        elif model_name == \"RandomForest\":\n",
    "            if training_combo != None:\n",
    "                n_estimator, max_feature, max_depth, min_samples_split, min_samples_leaf = training_combo\n",
    "                params = params_input if params_input != None else RandomForest_params\n",
    "                params['hyperparameters']['n_estimators'] = n_estimator\n",
    "                params['hyperparameters']['max_features'] = max_feature\n",
    "#                 params['hyperparameters']['max_depth'] = float(max_depth) if isinstance(max_depth, int) else np.nan\n",
    "                params['hyperparameters']['max_depth'] = max_depth\n",
    "                params['hyperparameters']['min_samples_split'] = min_samples_split\n",
    "                params['hyperparameters']['min_samples_leaf'] = min_samples_leaf\n",
    "            else:\n",
    "                print(f\"If you want to validate {model_name}, make sure training_combo not None !\")\n",
    "                return 0\n",
    "        \n",
    "        else:\n",
    "            print(f\"Please make sure model in {current_models_two_phase} and {current_models_one_phase} !\")\n",
    "            return 0\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    elif testing == True and validating == False:\n",
    "        \n",
    "        if best_all_params == None:\n",
    "            print(f\"Please make sure best_all_params not None if you want to use this function to test !\")\n",
    "            return 0 \n",
    "        \n",
    "        best_all_params = best_all_params[top_n]\n",
    "        test_params = 0\n",
    "        \n",
    "        if model_name == \"FM\":\n",
    "            test_params = params_input if params_input != None else FM_params\n",
    "            test_params[\"batch_size\"] = best_all_params['batch_size']\n",
    "            test_params[\"dl_learning_rate\"] = best_all_params['dl_learning_rate']\n",
    "            test_params['hyperparameters'][\"factors_num\"] = best_all_params['factors_num']\n",
    "        \n",
    "        elif model_name == \"AutoInt\":\n",
    "            test_params = params_input if params_input != None else AutoInt_params\n",
    "            test_params['batch_size'] = best_all_params['batch_size']\n",
    "            test_params['dl_learning_rate'] = best_all_params['dl_learning_rate']\n",
    "            test_params['hyperparameters']['dropouts'] = best_all_params['dropout']\n",
    "            test_params['hyperparameters']['atten_embed_dim'] = best_all_params['atten_embed_dim']\n",
    "            test_params['hyperparameters']['num_layers'] = best_all_params['num_layers']\n",
    "            test_params['hyperparameters']['mlp_dims'] = best_all_params['mlp_dims']\n",
    "        \n",
    "        elif model_name == \"MLP\":\n",
    "            test_params = params_input if params_input != None else MLP_params\n",
    "            test_params['batch_size'] = best_all_params['batch_size']\n",
    "            test_params['dl_learning_rate'] = best_all_params['dl_learning_rate']\n",
    "            test_params['hyperparameters']['dropout'] = best_all_params['dropout']\n",
    "            test_params['hyperparameters']['embed_dims'] = best_all_params['embed_dims']\n",
    "        \n",
    "        elif model_name == \"XGBoost\":\n",
    "            test_params = params_input if params_input != None else XGBoost_params\n",
    "            test_params['hyperparameters']['colsample_bytree'] = best_all_params['colsample_bytree']\n",
    "            test_params['hyperparameters']['lambda'] = best_all_params['lambda']\n",
    "            test_params['hyperparameters']['learning_rate'] = best_all_params['learning_rate']\n",
    "            test_params['hyperparameters']['max_depth'] = best_all_params['max_depth']\n",
    "            test_params['hyperparameters']['min_child_weight'] = best_all_params['min_child_weight']\n",
    "            test_params['hyperparameters']['n_estimators'] = best_all_params['n_estimators']\n",
    "            test_params['hyperparameters']['subsample'] = best_all_params['subsample']\n",
    "\n",
    "        elif model_name == \"RandomForest\":\n",
    "            test_params = params_input if params_input != None else RandomForest_params\n",
    "            test_params['hyperparameters']['max_depth'] = best_all_params['max_depth']\n",
    "            test_params['hyperparameters']['max_features'] = best_all_params['max_features']\n",
    "            test_params['hyperparameters']['min_samples_leaf'] = best_all_params['min_samples_leaf']\n",
    "            test_params['hyperparameters']['min_samples_split'] = best_all_params['min_samples_split']\n",
    "            test_params['hyperparameters']['n_estimators'] = best_all_params['n_estimators']\n",
    "            \n",
    "        else:\n",
    "            print(f\"Please make sure model in {current_models_two_phase} and {current_models_one_phase} !\")\n",
    "            return 0\n",
    "        \n",
    "        return test_params\n",
    "    \n",
    "    else:\n",
    "        print(f\"Please make sure validating True or testing True, can not be both True or both False !\")\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8aaa2",
   "metadata": {},
   "source": [
    "# Testing with mlflow experiment name (run id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def test_with_mlflow_experiment_name(X_train, y_train, X_valid, y_valid, model_name, \n",
    "                                experiment_name, metric=\"MSE\", train_metric=None, model_metric=None, \n",
    "                                     best_all_params=None, representation=None, sorting='ASC', top_n=1, \n",
    "                                     save_records=True, not_duplicate=False, device_name=\"cpu\"):\n",
    "    \n",
    "    device = torch.device(device_name)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    \n",
    "    if \"tv\" not in experiment_name:\n",
    "        print(\"Experiment must come from validation experiment !\")\n",
    "        return 0\n",
    "    \n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    experiment_id = 0\n",
    "    if experiment is not None:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    else:\n",
    "        print(\"Experiment is empty !\")\n",
    "        return 0\n",
    "    \n",
    "    current_models_two_phase = [\"AutoInt\", \"MLP\"]\n",
    "    current_models_one_phase = [\"FM\", \"XGBoost\", \"RandomForest\"]\n",
    "    \n",
    "    if model_name in current_models_two_phase and best_all_params!=None:\n",
    "        if train_metric == None or model_metric == None:\n",
    "            print(f\"Model in {current_models_two_phase}. If you want to use best_all_params, please make sure train_metric and model_metric not None !\")\n",
    "            return 0\n",
    "            \n",
    "    filter_str = None\n",
    "    if best_all_params != None:\n",
    "        best_all_params = best_all_params['1']\n",
    "        if model_name == \"FM\":\n",
    "            batch_size = best_all_params['batch_size']\n",
    "            dl_learning_rate = best_all_params['dl_learning_rate']\n",
    "            factors_num = best_all_params['factors_num']\n",
    "            filter_str = f\"params.batch_size = '{batch_size}' and params.dl_learning_rate = '{dl_learning_rate}' and params.factors_num = '{factors_num}'\"\n",
    "        elif model_name == \"AutoInt\":\n",
    "            batch_size = best_all_params['batch_size']\n",
    "            dl_learning_rate = best_all_params['dl_learning_rate']\n",
    "            dropouts = best_all_params['dropout']\n",
    "            atten_embed_dim = best_all_params['atten_embed_dim']\n",
    "            num_layers = best_all_params['num_layers']\n",
    "            mlp_dims = best_all_params['mlp_dims']\n",
    "            filter_str = f\"params.batch_size = '{batch_size}' and params.dl_learning_rate = '{dl_learning_rate}' and params.dropouts = '{dropouts}' and params.atten_embed_dim = '{atten_embed_dim}' and params.num_layers = '{num_layers}' and params.mlp_dims = '{mlp_dims}'\"\n",
    "        elif model_name == \"MLP\":\n",
    "            batch_size = best_all_params['batch_size']\n",
    "            dl_learning_rate = best_all_params['dl_learning_rate']\n",
    "            dropout = best_all_params['dropout']\n",
    "            embed_dims = best_all_params['embed_dims']\n",
    "            filter_str = f\"params.batch_size = '{batch_size}' and params.dl_learning_rate = '{dl_learning_rate}' and params.dropout = '{dropout}' and params.embed_dims = '{embed_dims}'\"\n",
    "        elif model_name == \"XGBoost\":\n",
    "            colsample_bytree = best_all_params['colsample_bytree']\n",
    "            lambda_str = best_all_params['lambda']\n",
    "            learning_rate = best_all_params['learning_rate']\n",
    "            max_depth = best_all_params['max_depth']\n",
    "            min_child_weight = best_all_params['min_child_weight']\n",
    "            n_estimators = best_all_params['n_estimators']\n",
    "            subsample = best_all_params['subsample']\n",
    "            filter_str = f\"params.colsample_bytree = '{colsample_bytree}' and params.lambda = '{lambda_str}' and params.learning_rate = '{learning_rate}' and params.max_depth = '{max_depth}' and params.min_child_weight = '{min_child_weight}' and params.n_estimators = '{n_estimators}' and params.subsample = '{subsample}'\"\n",
    "        elif model_name == \"RandomForest\":\n",
    "            max_depth = best_all_params['max_depth']\n",
    "            max_features = best_all_params['max_features']\n",
    "            min_samples_leaf = best_all_params['min_samples_leaf']\n",
    "            min_samples_split = best_all_params['min_samples_split']\n",
    "            n_estimators = best_all_params['n_estimators']\n",
    "            filter_str = f\"params.max_depth = '{max_depth}' and params.max_features = '{max_features}' and params.min_samples_leaf = '{min_samples_leaf}' and params.min_samples_split = '{min_samples_split}' and params.n_estimators = '{n_estimators}'\"\n",
    "        else:\n",
    "            print(f\"Model params must in {current_models_two_phase} and {current_models_one_phase} !\")\n",
    "            return 0\n",
    "    \n",
    "    search_params = {\n",
    "        \"order_by\": [f\"metrics.eval_{metric.lower()} {sorting}\"],\n",
    "        \"max_results\": top_n\n",
    "    }\n",
    "    \n",
    "    results = mlflow.search_runs(experiment_ids=experiment_id, \n",
    "                                 filter_string=filter_str,\n",
    "                                 order_by=search_params[\"order_by\"], \n",
    "                                 max_results=search_params[\"max_results\"])\n",
    "    run_id = 0\n",
    "    if not results.empty:\n",
    "        run_id = results.iloc[0][\"run_id\"]\n",
    "        run_id = str(run_id)\n",
    "        print(f\"Run ID from best {metric} params in {experiment_name}:\", run_id)\n",
    "    else:\n",
    "        print(\"Run ID is empty !\")\n",
    "        return 0\n",
    "\n",
    "    run_info = mlflow.get_run(run_id)\n",
    "    params = run_info.data.params\n",
    "    \n",
    "    print_nested_dict(params)\n",
    "    \n",
    "    phase = \"test\"\n",
    "    \n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\"]\n",
    "    regression_loss_list = [\"MSE, RMSE\", \"MAE\"]\n",
    "    classification_loss_list = [\"CrossEntropy\", \"BinaryCrossEntropy\"]\n",
    "    \n",
    "    if save_records:\n",
    "            \n",
    "        validation_column_list = [\n",
    "            \"Timestamp\", \"Model_name\", *params.keys(), \"RMSE\", \"MSE\", \"MAE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "        ]\n",
    "\n",
    "        # Create folder if not exists\n",
    "        folder_name = f\"{model_name}_result_records\"\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records_with_runID.csv')\n",
    "\n",
    "        if not os.path.exists(validation_file_path):\n",
    "            empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "            empty_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "\n",
    "        else:\n",
    "            if not_duplicate:\n",
    "                record_df = pd.read_csv(validation_file_path)\n",
    "                columns_to_compare = [\"Model_name\", *params.keys()]\n",
    "                record_df = record_df[columns_to_compare]\n",
    "                record_df = record_df.values.tolist()\n",
    "                record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "                temp_df_values = [[model_name, *params.values()]]\n",
    "                temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                temp_df = temp_df.values.tolist()[0]\n",
    "                temp_df = [str(value) for value in temp_df]\n",
    "\n",
    "                if temp_df in record_df:\n",
    "                    print(\"Parameters already exists in test results !\")\n",
    "                    return 0\n",
    "\n",
    "    else:\n",
    "        print(\"Test results are not saved to csv !\")\n",
    "    \n",
    "    run_name=0\n",
    "    representation_name = f\"{representation} \" if representation != None else \"\"\n",
    "    if train_metric == None and model_metric == None and best_all_params == None:\n",
    "        run_name = f\"Trained {representation_name}{metric.upper()} params\"\n",
    "    elif train_metric != None and model_metric == None:\n",
    "        run_name = f\"Trained {representation_name}{train_metric.upper()} params\"\n",
    "    elif train_metric != None and model_metric != None:\n",
    "        run_name = f\"Trained {representation_name}{train_metric.upper()} and {model_metric.upper()} params\"\n",
    "    elif train_metric == None and model_metric == None:\n",
    "        run_name = None\n",
    "    else:\n",
    "        print(\"If you want to set run name, please make sure train_metric is not None !\")\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    if model_name in deep_learning_model_names:\n",
    " \n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            \n",
    "            mlflow.log_params(params) # Log training parameters\n",
    "            model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "            model = mlflow.pytorch.load_model(model_dir) # Load the PyTorch model from the specified directory\n",
    "            \n",
    "            # Log model summary.\n",
    "            with open(f\"{model_name}_model_summary.txt\", \"w\") as f:\n",
    "                f.write(str(summary(model)))\n",
    "            mlflow.log_artifact(f\"{model_name}_model_summary.txt\")\n",
    "\n",
    "            # Convert to float tensor\n",
    "#             X_train_tensor = torch.from_numpy(X_train).float()\n",
    "#             Xi_train_tensor = torch.arange(X_train_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_train_tensor.size(0), 1, 1).int()\n",
    "#             y_train_tensor = torch.from_numpy(y_train).float()\n",
    "            X_valid_tensor = torch.from_numpy(X_valid).float().to(device)\n",
    "            Xi_valid_tensor = torch.arange(X_valid_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_valid_tensor.size(0), 1, 1).int()\n",
    "            y_valid_tensor = torch.from_numpy(y_valid).float().to(device)\n",
    "\n",
    "            # Convert data to DataLoader\n",
    "#             train_dataset = TensorDataset(Xi_train_tensor, X_train_tensor, y_train_tensor)\n",
    "#             train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataset = TensorDataset(Xi_valid_tensor, X_valid_tensor, y_valid_tensor)\n",
    "            validation_loader = DataLoader(validation_dataset, batch_size=len(X_valid), shuffle=True)\n",
    "            \n",
    "            y_pred = 0\n",
    "            validation_results_df = []\n",
    "            criterion = 0\n",
    "            \n",
    "            print(f\"Start testing with best {metric.upper()} params ...\")\n",
    "            \n",
    "            for t, (xi, x, y_true) in enumerate(validation_loader):\n",
    "\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                elif model_name == \"MLP\":\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = y_pred.view(-1)\n",
    "                else:\n",
    "                    y_pred = model(x)\n",
    "            \n",
    "            # Calculate metric values\n",
    "            loss = 0.0\n",
    "\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if params['task_type'] == \"Regression\":\n",
    "                criterion = nn.MSELoss()\n",
    "                if params['loss_type'] == \"RMSE\":\n",
    "                    eps = 1e-6\n",
    "                    if model_name == \"FM\":\n",
    "                        loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                    else:\n",
    "                        loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                elif params['loss_type'] == \"MSE\":\n",
    "                    if model_name == \"FM\":\n",
    "                        loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                    else:        \n",
    "                        loss = criterion(y_pred, y_true.view(-1))\n",
    "                else:\n",
    "                    print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                    return 0\n",
    "\n",
    "                y_true = y_true.to(cpu_device).detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = y_pred.to(cpu_device).detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                predictions = y_pred\n",
    "                \n",
    "                rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                mse = mean_squared_error(y_true, y_pred)\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "            elif params['task_type'] == \"Classification\":\n",
    "                if params['loss_type'] == \"BCE\":\n",
    "                        criterion = nn.BCELoss()\n",
    "                        y_pred = torch.sigmoid(y_pred)\n",
    "                elif params['loss_type'] == \"CE\":\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                else:\n",
    "                    print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                    return 0\n",
    "\n",
    "                loss = criterion(y_pred, y_true)\n",
    "\n",
    "                y_true = y_true.to(cpu_device).detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = y_pred.to(cpu_device).detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                predictions = y_pred\n",
    "                \n",
    "                accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                precision = precision_score(y_true, y_pred) # Precision\n",
    "                recall = recall_score(y_true, y_pred) # Recall\n",
    "                specificity = recall_score(y_true, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "#             mlflow.log_metric(f\"{phase}_loss\", f\"{loss:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mse\", f\"{mse:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mae\", f\"{mae:6f}\")\n",
    "\n",
    "            mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_f1\", f\"{f1:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_precision\", f\"{precision:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_recall\", f\"{recall:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:6f}\")\n",
    "\n",
    "            # Save the trained model to MLflow.\n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            mlflow.pytorch.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *params.values(), \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            validation_results_df.append(value_list)\n",
    "            \n",
    "            # Save training and validation results to file\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "    \n",
    "    elif model_name in machine_learning_model_names:\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            \n",
    "            mlflow.log_params(params) # Log training parameters\n",
    "            model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "            model = 0\n",
    "            \n",
    "            if model_name in [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\"]:\n",
    "                model = mlflow.sklearn.load_model(model_dir)\n",
    "            elif model_name == \"XGBoost\":\n",
    "                model = mlflow.xgboost.load_model(model_dir)\n",
    "            elif model_name == \"CatBoost\":\n",
    "                model = mlflow.catboost.load_model(model_dir)\n",
    "            else:\n",
    "                print(f\"Model not in {machine_learning_model_names}, couldn't load the model !\")\n",
    "                        \n",
    "            validation_results_df = []\n",
    "\n",
    "            print(f\"Start testing with best {metric.upper()} params ...\")\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "            predictions = y_pred\n",
    "            \n",
    "            # Calculate metric values\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if params['task_type'] == \"Regression\":\n",
    "                rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "                mse = mean_squared_error(y_valid, y_pred)\n",
    "                mae = mean_absolute_error(y_valid, y_pred)\n",
    "            elif params['task_type'] == \"Classification\":\n",
    "                accuracy = accuracy_score(y_valid, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_valid, y_pred) # AUC\n",
    "                f1 = f1_score(y_valid, y_pred) # F1-score\n",
    "                precision = precision_score(y_valid, y_pred) # Precision\n",
    "                recall = recall_score(y_valid, y_pred) # Recall\n",
    "                specificity = recall_score(y_valid, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "            mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mse\", f\"{mse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mae\", f\"{mae:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_f1\", f\"{f1:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_precision\", f\"{precision:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_recall\", f\"{recall:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:4f}\")\n",
    "\n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            # Save the trained model to MLflow.\n",
    "            if model_name in [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\"]:\n",
    "                mlflow.sklearn.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"XGBoost\":\n",
    "                mlflow.xgboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.xgboost.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"CatBoost\":\n",
    "                mlflow.catboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.catboost.log_model(model, f\"{model_name}_model\")\n",
    "            else:\n",
    "                print(f\"Model not in {machine_learning_model_names}, couldn't save the model !\")\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *params.values(), \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            validation_results_df.append(value_list)       \n",
    "    #         print(f\"RMSE on the validation set: {rmse}\")\n",
    "\n",
    "            # Save validation results to file\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "    \n",
    "    else:\n",
    "        return f\"Please select a model in {deep_learning_model_names} and {machine_learning_model_names} !\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58c6ce",
   "metadata": {},
   "source": [
    "# Review num distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def get_word_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def calcuate_reviews_num(reviews_df): \n",
    "\n",
    "    # Count reviews per user and calculate total word count and token count per user\n",
    "    user_stats = defaultdict(dict)\n",
    "    \n",
    "    for index, review in reviews_df.iterrows():\n",
    "        user_id = review['user_id']\n",
    "        if user_id not in user_stats:\n",
    "            user_stats[user_id]['reviews_num'] = 0\n",
    "            user_stats[user_id]['words_num'] = 0\n",
    "            user_stats[user_id]['tokens_num'] = 0\n",
    "\n",
    "        user_stats[user_id]['reviews_num'] += 1\n",
    "        user_stats[user_id]['words_num'] += len(review['text'].split())\n",
    "        user_stats[user_id]['tokens_num'] += len(get_word_tokens(review['text']))\n",
    "\n",
    "    # Count reviews per business and calculate total word count and token count per business\n",
    "    business_stats = defaultdict(dict)\n",
    "\n",
    "    for index, review in reviews_df.iterrows():\n",
    "        business_id = review['business_id']\n",
    "        if business_id not in business_stats:\n",
    "            business_stats[business_id]['reviews_num'] = 0\n",
    "            business_stats[business_id]['words_num'] = 0\n",
    "            business_stats[business_id]['tokens_num'] = 0\n",
    "\n",
    "        business_stats[business_id]['reviews_num'] += 1\n",
    "        business_stats[business_id]['words_num'] += len(review['text'].split())\n",
    "        business_stats[business_id]['tokens_num'] += len(get_word_tokens(review['text']))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    user_df = pd.DataFrame.from_dict(user_stats, orient='index').reset_index()\n",
    "    user_df.columns = ['user_id', 'reviews_num', 'words_num', 'tokens_num']\n",
    "\n",
    "    business_df = pd.DataFrame.from_dict(business_stats, orient='index').reset_index()\n",
    "    business_df.columns = ['business_id', 'reviews_num', 'words_num', 'tokens_num']\n",
    "    \n",
    "    return user_df, business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05256d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    lower_bound = mean - 2 * std\n",
    "    upper_bound = mean + 2 * std\n",
    "    return data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "def calculate_distribution(data):\n",
    "    \n",
    "    data = remove_outliers(data)\n",
    "    \n",
    "    distribution = {\n",
    "        'min': np.min(data),\n",
    "        'max': np.max(data),\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'mode': float(np.argmax(np.bincount(data))),\n",
    "        'percentiles': {\n",
    "            '5th': np.percentile(data, 5),\n",
    "            '10th': np.percentile(data, 10),\n",
    "            '15th': np.percentile(data, 15),\n",
    "            '20th': np.percentile(data, 20),\n",
    "            '25th': np.percentile(data, 25),\n",
    "            '30th': np.percentile(data, 30),\n",
    "            '35th': np.percentile(data, 35),\n",
    "            '40th': np.percentile(data, 40),\n",
    "            '45th': np.percentile(data, 45),\n",
    "            '50th': np.percentile(data, 50),\n",
    "            '55th': np.percentile(data, 55),\n",
    "            '60th': np.percentile(data, 60),\n",
    "            '65th': np.percentile(data, 65),\n",
    "            '70th': np.percentile(data, 70),\n",
    "            '75th': np.percentile(data, 75),\n",
    "            '80th': np.percentile(data, 80),\n",
    "            '85th': np.percentile(data, 85),\n",
    "            '90th': np.percentile(data, 90),\n",
    "            '95th': np.percentile(data, 95),\n",
    "            '100th': np.percentile(data, 100),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "#     print(\"Minimum:\", distribution['min'])\n",
    "#     print(\"Maximum:\", distribution['max'])\n",
    "#     print(\"Mean:\", distribution['mean'])\n",
    "#     print(\"Standard Deviation:\", distribution['std'])\n",
    "#     print(\"Mode:\", distribution['mode'])\n",
    "\n",
    "#     percentiles = distribution['percentiles']\n",
    "#     for percentile, value in percentiles.items():\n",
    "#         print(f\"{percentile.capitalize()} Percentile:\", value)\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842d8ad",
   "metadata": {},
   "source": [
    "# Few reviews performance inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31939082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def few_reviews_performance_inspection(train_df, test_df, run_id_experiment_task_model, \n",
    "                                       user_reviews_num=3,user_comparison=\"equal\", \n",
    "                                       user_reviews_num_range=None, business_reviews_num=100000,\n",
    "                                       business_comparison=\"max\", business_reviews_num_range=None):\n",
    "    \n",
    "    # filter reviews\n",
    "    filtered_reviews, calculation_results = sample_reviews_and_calculate_price_then_return_data(train_df, 100, \n",
    "                                                        user_reviews_num=user_reviews_num,\n",
    "                                                        user_comparison=user_comparison,\n",
    "                                                        user_reviews_num_range=user_reviews_num_range,\n",
    "                                                        business_reviews_num=business_reviews_num,\n",
    "                                                        business_comparison=business_comparison,\n",
    "                                                        business_reviews_num_range=business_reviews_num_range,\n",
    "                                                        genai=\"GPT-3.5 Turbo\", \n",
    "                                                        sampling_method='random', column='None')\n",
    "    \n",
    "    filtered_reviews_num = calculation_results[\"filtered_reviews_num\"]\n",
    "    filtered_users_count = calculation_results[\"filtered_users_count\"]\n",
    "    filtered_businesses_count = calculation_results[\"filtered_businesses_count\"]\n",
    "    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "        return 0\n",
    "    print(\"filtered_reviews_num:\", filtered_reviews_num)\n",
    "    print(\"filtered_users_count:\", filtered_users_count)\n",
    "    print(\"filtered_businesses_count:\", filtered_businesses_count)\n",
    "    \n",
    "    unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "    unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "    test_data = test_df[\n",
    "        (test_df['user_id'].isin(unique_user_ids)) & \n",
    "        (test_df['business_id'].isin(unique_business_ids))\n",
    "    ]\n",
    "    test_data.reset_index(inplace=True)\n",
    "\n",
    "    # get X_test and y_test\n",
    "    cols = list(test_data.columns)\n",
    "    columns_to_train = [col for col in cols if \"vector\" in col.lower()]\n",
    "\n",
    "    concatenated_vectors = []\n",
    "    for i in range(len(test_data)):\n",
    "        col_vectors = []\n",
    "        for col in columns_to_train:\n",
    "            col_vectors.append(np.array(eval(test_data.loc[i, col])))\n",
    "        concatenated_vector = np.concatenate(col_vectors)\n",
    "        concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "    X_test = np.array(concatenated_vectors)\n",
    "    y_test = np.array(test_data['stars'])\n",
    "\n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\", \"CatBoost\"]\n",
    "    \n",
    "    num_experiments = len(run_id_experiment_task_model[\"run_id\"])\n",
    "    performance_results = {}\n",
    "    for i in range(num_experiments):\n",
    "        run_id = run_id_experiment_task_model[\"run_id\"][i]\n",
    "        experiment_name = run_id_experiment_task_model[\"experiment_name\"][i]\n",
    "    \n",
    "        task_type = run_id_experiment_task_model[\"task_type\"]\n",
    "        model_name = run_id_experiment_task_model[\"model_name\"]\n",
    "\n",
    "        print(\"run_id:\", run_id)\n",
    "        print(\"experiment_name:\", experiment_name)\n",
    "        print(\"task_type:\", task_type)\n",
    "        print(\"model_name\", model_name)\n",
    "\n",
    "        model = 0\n",
    "        y_pred = 0\n",
    "\n",
    "        # Specify the directory containing the MLmodel file\n",
    "        mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "        model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "\n",
    "        print(f'Start predicting with {model_name} model ...')\n",
    "\n",
    "        if model_name in deep_learning_model_names:\n",
    "\n",
    "            # Load the model from the specified directory\n",
    "            model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "            # Convert to float tensor\n",
    "            X_test_tensor = torch.from_numpy(X_test).float()\n",
    "            Xi_test_tensor = torch.arange(X_test_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_test_tensor.size(0), 1, 1).int()\n",
    "            y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "            # Convert data to DataLoader\n",
    "            test_dataset = TensorDataset(Xi_test_tensor, X_test_tensor, y_test_tensor)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            for t, (xi, x, _) in enumerate(test_loader):\n",
    "\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                elif model_name == \"MLP\":\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = y_pred.view(-1)\n",
    "                else:    \n",
    "                    y_pred = model(x)\n",
    "\n",
    "                y_pred = y_pred.detach().numpy()\n",
    "\n",
    "        elif model_name in machine_learning_model_names:\n",
    "            if model_name == \"XGBoost\":\n",
    "                model = mlflow.xgboost.load_model(model_dir)\n",
    "            elif model_name == \"CatBoost\":\n",
    "                model = mlflow.catboost.load_model(model_dir)\n",
    "            else:\n",
    "                model = mlflow.sklearn.load_model(model_dir)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Please select model in {deep_learning_model_names} or {machine_learning_model_names} !\")\n",
    "            return 0\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        mae = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "\n",
    "        if task_type==\"regression\":\n",
    "            rmse = root_mean_squared_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_test, y_pred) # AUC\n",
    "            f1 = f1_score(y_test, y_pred) # F1-score\n",
    "            precision = precision_score(y_test, y_pred) # Precision\n",
    "            recall = recall_score(y_test, y_pred) # Recall\n",
    "            specificity = recall_score(y_test, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "\n",
    "        results[\"rmse\"] = rmse\n",
    "        results[\"mse\"] = mse\n",
    "        results[\"mae\"] = mae\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"auc_score\"] = auc_score\n",
    "        results[\"f1\"] = f1\n",
    "        results[\"precision\"] = precision\n",
    "        results[\"recall\"] = recall\n",
    "        results[\"specificity\"] = specificity\n",
    "        \n",
    "        print(f\"{experiment_name}:\\n\", results)\n",
    "        \n",
    "        performance_results[experiment_name] = results\n",
    "\n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ee0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def inspect_few_reviews_performance(experiment_model_best_run_ids, task_type=\"regression\"):\n",
    "    \n",
    "    performance_results = {}\n",
    "#     performance_results_df = []\n",
    "    \n",
    "    for large_experiment, small_experiments in experiment_model_best_run_ids.items():\n",
    "        \n",
    "        if experiment_model_best_run_ids[large_experiment][\"bbs\"][\"FM\"][\"MSE\"] != 0:\n",
    "            \n",
    "            for representation, models in small_experiments.items():\n",
    "\n",
    "                representation_name = \"BERT_base_sentence_embedding\" if representation == \"bbs\" else \"RoBERTa_base_sentence_embedding\"\n",
    "                format_name = \"text_format4\" if \"ori\" in large_experiment and \"gpt\" not in large_experiment else \"text_format_v4\"\n",
    "                path = f\"./{large_experiment}/{format_name}/{representation_name}/\"\n",
    "\n",
    "                training_data = 0\n",
    "                validation_data = 0\n",
    "                test_data = 0\n",
    "                temp_training_data_list = []\n",
    "                temp_validation_data_list = []\n",
    "                temp_test_data_list = []\n",
    "                basic_experiments = [\"original\", \"gpt_prompt_v1\", \"gpt_prompt_v3\"]\n",
    "\n",
    "                # loading data\n",
    "                if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "\n",
    "                    for basic_experiment in basic_experiments:\n",
    "                        if basic_experiment in large_experiment:\n",
    "                            path = f\"./{basic_experiment}/{format_name}/{representation_name}/\"\n",
    "                            files = os.listdir(path)\n",
    "                            train_data_name = [file for file in files if 'train' in file and 'set' in file][0]\n",
    "                            valid_data_name = [file for file in files if 'valid' in file and 'set' in file][0]\n",
    "                            test_data_name = [file for file in files if 'test' in file and 'set' in file][0]\n",
    "                            temp_training_data = pd.read_csv(path+train_data_name)\n",
    "        #                     print(training_data.shape)\n",
    "                            temp_training_data_list.append(temp_training_data)\n",
    "                            temp_validation_data = pd.read_csv(path+valid_data_name)\n",
    "        #                     print(validation_data.shape)\n",
    "                            temp_validation_data_list.append(temp_validation_data)\n",
    "                            temp_test_data = pd.read_csv(path+test_data_name)\n",
    "        #                     print(test_data.shape)\n",
    "                            temp_test_data_list.append(temp_test_data)\n",
    "\n",
    "        #             training_data = temp_training_data_list[0]\n",
    "        #             validation_data = temp_validation_data_list[0]\n",
    "        #             test_data = temp_test_data_list[0]\n",
    "\n",
    "                else:\n",
    "                    files = os.listdir(path)\n",
    "                    train_data_name = [file for file in files if 'train' in file and 'set' in file][0]\n",
    "                    valid_data_name = [file for file in files if 'valid' in file and 'set' in file][0]\n",
    "                    test_data_name = [file for file in files if 'test' in file and 'set' in file][0]\n",
    "                    training_data = pd.read_csv(path+train_data_name)\n",
    "        #             print(training_data.shape)\n",
    "                    validation_data = pd.read_csv(path+valid_data_name)\n",
    "        #             print(validation_data.shape)\n",
    "                    test_data = pd.read_csv(path+test_data_name)\n",
    "        #             print(test_data.shape)\n",
    "\n",
    "                train_df = 0\n",
    "                if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "                    train_df = temp_training_data_list[0]\n",
    "                else:\n",
    "                    train_df = training_data\n",
    "\n",
    "                user_df, business_df = calcuate_reviews_num(train_df)\n",
    "                user_distribution = calculate_distribution(user_df[\"reviews_num\"])\n",
    "                business_distribution = calculate_distribution(business_df[\"reviews_num\"])\n",
    "\n",
    "                # specify filter condition\n",
    "                user_max_reviews_num = user_distribution['max']\n",
    "                business_max_reviews_num = business_distribution['max']\n",
    "                user_comparison_list = [\"equal\", \"max\"]\n",
    "                user_reviews_num_range = None\n",
    "                business_comparison_list = [\"max\", \"equal\"]\n",
    "                business_reviews_num_range = None\n",
    "        #         user_reviews_num_list = \n",
    "\n",
    "                for user_comparison in user_comparison_list:\n",
    "                    for business_comparison in business_comparison_list:\n",
    "                        \n",
    "                        if user_comparison == \"equal\" and business_comparison == \"equal\":\n",
    "                            print(\"Skip user_comparison equal and business_comparison equal !\")\n",
    "                        elif user_comparison == \"max\" and business_comparison == \"max\":\n",
    "                            print(\"Skip user_comparison max and business_comparison max !\")\n",
    "                        else:\n",
    "                        \n",
    "                            for temp_user_reviews_num in range(1, user_max_reviews_num+1):\n",
    "                                for temp_business_reviews_num in range(1, business_max_reviews_num+1):\n",
    "                                \n",
    "                                    user_reviews_num = 0\n",
    "                                    business_reviews_num = 0\n",
    "                                    \n",
    "                                    if user_comparison == \"equal\" and business_comparison == \"max\":\n",
    "                                        user_reviews_num = temp_user_reviews_num\n",
    "                                        business_reviews_num = 10000000\n",
    "                                    else:\n",
    "#                                         user_comparison == \"max\" and business_comparison == \"equal\":\n",
    "                                        user_reviews_num = 10000000\n",
    "                                        business_reviews_num = temp_business_reviews_num\n",
    "                                \n",
    "                                    # filter reviews\n",
    "                                    filtered_reviews, calculation_results = sample_reviews_and_calculate_price_then_return_data(train_df, 100, \n",
    "                                                                                        user_reviews_num=user_reviews_num,\n",
    "                                                                                        user_comparison=user_comparison,\n",
    "                                                                                        user_reviews_num_range=user_reviews_num_range,\n",
    "                                                                                        business_reviews_num=business_reviews_num,\n",
    "                                                                                        business_comparison=business_comparison,\n",
    "                                                                                        business_reviews_num_range=business_reviews_num_range,\n",
    "                                                                                        genai=\"GPT-3.5 Turbo\", \n",
    "                                                                                        sampling_method='random', column='None')\n",
    "\n",
    "                                    filtered_reviews_num = calculation_results[\"filtered_reviews_num\"]\n",
    "                                    filtered_users_count = calculation_results[\"filtered_users_count\"]\n",
    "                                    filtered_businesses_count = calculation_results[\"filtered_businesses_count\"]\n",
    "                                    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "                                        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "                                        print(\"\\n\\n\\n\")\n",
    "                                        print(\"-----------------------------------------------------------\")\n",
    "                                        print(\"\\n\\n\\n\")\n",
    "    #                                     return 0\n",
    "                                    else:\n",
    "                                        print(\"filtered_reviews_num:\", filtered_reviews_num)\n",
    "                                        print(\"filtered_users_count:\", filtered_users_count)\n",
    "                                        print(\"filtered_businesses_count:\", filtered_businesses_count)\n",
    "\n",
    "                                        unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "                                        unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "                                        # obtaining X_test and y_test\n",
    "                                        X_test = 0\n",
    "                                        y_test = 0\n",
    "                                        if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "\n",
    "                                            test_data_list = []\n",
    "                                            is_test_data_empty = False\n",
    "                                            for data_df in temp_test_data_list:\n",
    "                                                data_df = data_df[\n",
    "                                                    (data_df['user_id'].isin(unique_user_ids)) & \n",
    "                                                    (data_df['business_id'].isin(unique_business_ids))\n",
    "                                                ]\n",
    "                                                if len(data_df) < 1:\n",
    "                                                    is_test_data_empty = True\n",
    "                                                else:\n",
    "                                                    data_df.reset_index(inplace=True)\n",
    "                                                    test_data_list.append(data_df)\n",
    "\n",
    "                                            if is_test_data_empty:\n",
    "                                                X_test = 0\n",
    "                                                y_test = 0\n",
    "                                            else:\n",
    "                                                concatenated_vectors = []\n",
    "                                                for i in range(len(test_data_list[0])):\n",
    "                                                    col_vectors = []\n",
    "                                                    for j in range(2): # 2 vectors\n",
    "                                                        for data in test_data_list:\n",
    "                                                            data_cols = list(data.columns)\n",
    "                                                            columns_to_train = [col for col in data_cols if \"vector\" in col.lower()]\n",
    "                                                            col = columns_to_train[j]\n",
    "                                                            col_vectors.append(np.array(eval(data[col][i])))\n",
    "                                                    concatenated_vector = np.concatenate(col_vectors)\n",
    "                                                    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "                                                X_test = np.array(concatenated_vectors)\n",
    "                                                y_test = np.array(test_data_list[0]['stars'])\n",
    "\n",
    "                                        else:\n",
    "    #                                         print(\"1\")\n",
    "                                            test_df = test_data[\n",
    "                                                (test_data['user_id'].isin(unique_user_ids)) & \n",
    "                                                (test_data['business_id'].isin(unique_business_ids))\n",
    "                                            ]\n",
    "    #                                         print(\"2\")\n",
    "                                            if len(test_df) < 1:\n",
    "                                                X_test = 0\n",
    "                                                y_test = 0\n",
    "                                            else:\n",
    "                                                test_df.reset_index(inplace=True)        \n",
    "                                                cols = list(test_df.columns)\n",
    "                                                columns_to_train = [col for col in cols if \"vector\" in col.lower()]\n",
    "\n",
    "                                                concatenated_vectors = []\n",
    "                                                for i in range(len(test_df)):\n",
    "                                                    col_vectors = []\n",
    "                                                    for col in columns_to_train:\n",
    "                                                        col_vectors.append(np.array(eval(test_df.loc[i, col])))\n",
    "                                                    concatenated_vector = np.concatenate(col_vectors)\n",
    "                                                    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "                                                X_test = np.array(concatenated_vectors)\n",
    "                                                y_test = np.array(test_df['stars'])\n",
    "    #                                             print(\"3\")\n",
    "\n",
    "                                        if not isinstance(X_test, np.ndarray) and X_test == 0:\n",
    "                                            print('Test data is empty ! \\n\\n')\n",
    "                                            print(\"-----------------------------------------------------------\")\n",
    "\n",
    "                                        else:\n",
    "    #                                         print(\"3.1\")\n",
    "                                            # start testing\n",
    "                                            deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "                                            machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                                                            \"AdaBoost\", \"XGBoost\", \"CatBoost\"]\n",
    "\n",
    "                                            for model_name, metrics in models.items():\n",
    "\n",
    "                                                for metric, run_id in metrics.items():\n",
    "\n",
    "#                                                     run_name = f\"{large_experiment}/{format_name}/{representation_name}/{model_name}\"\n",
    "#                                                     run_name = f\"{large_experiment}/{format_name}/{representation_name}\"\n",
    "    \n",
    "                                                    results = {}\n",
    "\n",
    "                                                    rmse = 0.0\n",
    "                                                    mse = 0.0\n",
    "                                                    mae = 0.0\n",
    "                                                    accuracy = 0.0\n",
    "                                                    auc_score = 0.0\n",
    "                                                    f1 = 0.0\n",
    "                                                    precision = 0.0\n",
    "                                                    recall = 0.0\n",
    "                                                    specificity = 0.0\n",
    "\n",
    "                                                    results[\"rmse\"] = rmse\n",
    "                                                    results[\"mse\"] = mse\n",
    "                                                    results[\"mae\"] = mae\n",
    "                                                    results[\"accuracy\"] = accuracy\n",
    "                                                    results[\"auc_score\"] = auc_score\n",
    "                                                    results[\"f1\"] = f1\n",
    "                                                    results[\"precision\"] = precision\n",
    "                                                    results[\"recall\"] = recall\n",
    "                                                    results[\"specificity\"] = specificity\n",
    "    #                                                 print(\"3.5\")\n",
    "                                                    record_file_name = \"inspect_few_reviews_performance_results.csv\"\n",
    "                                                    if not os.path.exists(record_file_name):\n",
    "                                                        column_list = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"user_comparison\", \"user_reviews_num\", \n",
    "                                                                       \"business_comparison\", \"business_reviews_num\", \"metric\",\n",
    "                                                                       \"filtered_reviews_num\", \"filtered_users_count\", \n",
    "                                                                       \"filtered_businesses_count\", \"num_of_test_data\", *results.keys()]\n",
    "                                                        empty_df = pd.DataFrame(columns=column_list)\n",
    "                                                        empty_df.to_csv(record_file_name, index=False, float_format='%.6f')\n",
    "                                                    else:\n",
    "                                                        record_df = pd.read_csv(record_file_name)\n",
    "                                                        columns_to_compare = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"user_comparison\", \"user_reviews_num\", \n",
    "                                                                              \"business_comparison\", \"business_reviews_num\", \"metric\"]\n",
    "                                                        record_df = record_df[columns_to_compare]\n",
    "                                                        record_df = record_df.values.tolist()\n",
    "                                                        record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "\n",
    "                                                        temp_df_values = [[large_experiment, format_name, representation_name, model_name, user_comparison, user_reviews_num, \n",
    "                                                                  business_comparison, business_reviews_num, metric]]\n",
    "                                                        \n",
    "                                                        temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                                                        temp_df = temp_df.values.tolist()[0]\n",
    "                                                        temp_df = [str(value) for value in temp_df]\n",
    "    #                                                     print(\"4\")\n",
    "                                                        if temp_df in record_df:\n",
    "                                                            print(\"This combination already exists in record results !\")\n",
    "        #                                                     return 0\n",
    "                                                        else:\n",
    "\n",
    "                                                            if run_id == 0 or len(X_test) < 1:\n",
    "\n",
    "                                                                results[\"rmse\"] = rmse\n",
    "                                                                results[\"mse\"] = mse\n",
    "                                                                results[\"mae\"] = mae\n",
    "                                                                results[\"accuracy\"] = accuracy\n",
    "                                                                results[\"auc_score\"] = auc_score\n",
    "                                                                results[\"f1\"] = f1\n",
    "                                                                results[\"precision\"] = precision\n",
    "                                                                results[\"recall\"] = recall\n",
    "                                                                results[\"specificity\"] = specificity\n",
    "\n",
    "                                                            else:\n",
    "    #                                                             print(\"5\")\n",
    "                                                                # Specify the directory containing the MLmodel file\n",
    "                                                                mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "                                                                model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "                                                                # Load the model from the specified directory\n",
    "    #                                                             model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "                                                                print(f'Start predicting with {large_experiment}/{format_name}/{representation_name}/{model_name}/{metric} ...')\n",
    "\n",
    "                                                                if model_name in deep_learning_model_names:\n",
    "\n",
    "                                                                    model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "                                                                    # Convert to float tensor\n",
    "                                                                    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "                                                                    Xi_test_tensor = torch.arange(X_test_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_test_tensor.size(0), 1, 1).int()\n",
    "                                                                    y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "                                                                    # Convert data to DataLoader\n",
    "                                                                    test_dataset = TensorDataset(Xi_test_tensor, X_test_tensor, y_test_tensor)\n",
    "                                                                    test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "                                                                    for t, (xi, x, _) in enumerate(test_loader):\n",
    "\n",
    "                                                                        if model_name == \"DeepFM\":\n",
    "                                                                            y_pred = model(xi, x)\n",
    "                                                                        elif model_name == \"MLP\":\n",
    "                                                                            y_pred = model(x)\n",
    "                                                                            y_pred = y_pred.view(-1)\n",
    "                                                                        else:    \n",
    "                                                                            y_pred = model(x)\n",
    "\n",
    "                                                                        y_pred = y_pred.detach().numpy()\n",
    "\n",
    "                                                                elif model_name in machine_learning_model_names:\n",
    "                                                                    if model_name == \"XGBoost\":\n",
    "                                                                        model = mlflow.xgboost.load_model(model_dir)\n",
    "                                                                    elif model_name == \"CatBoost\":\n",
    "                                                                        model = mlflow.catboost.load_model(model_dir)\n",
    "                                                                    else:\n",
    "                                                                        model = mlflow.sklearn.load_model(model_dir)\n",
    "\n",
    "                                                                    y_pred = model.predict(X_test)\n",
    "\n",
    "                                                                else:\n",
    "                                                                    print(f\"Please select model in {deep_learning_model_names} or {machine_learning_model_names} !\")\n",
    "                                                                    return 0\n",
    "\n",
    "                                                                if task_type==\"regression\":\n",
    "                                                                    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "                                                                    mse = mean_squared_error(y_test, y_pred)\n",
    "                                                                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                                                                else:\n",
    "                                                                    accuracy = accuracy_score(y_test, y_pred) # Accuracy\n",
    "                                                                    auc_score = roc_auc_score(y_test, y_pred) # AUC\n",
    "                                                                    f1 = f1_score(y_test, y_pred) # F1-score\n",
    "                                                                    precision = precision_score(y_test, y_pred) # Precision\n",
    "                                                                    recall = recall_score(y_test, y_pred) # Recall\n",
    "                                                                    specificity = recall_score(y_test, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "\n",
    "                                                                results[\"rmse\"] = rmse\n",
    "                                                                results[\"mse\"] = mse\n",
    "                                                                results[\"mae\"] = mae\n",
    "                                                                results[\"accuracy\"] = accuracy\n",
    "                                                                results[\"auc_score\"] = auc_score\n",
    "                                                                results[\"f1\"] = f1\n",
    "                                                                results[\"precision\"] = precision\n",
    "                                                                results[\"recall\"] = recall\n",
    "                                                                results[\"specificity\"] = specificity\n",
    "\n",
    "                                                            print(f\"{large_experiment}/{format_name}/{representation_name}/{model_name}/{metric}, uc:{user_comparison}, un:{user_reviews_num}, bc:{business_comparison}, bn:{business_reviews_num} :\\n\", results)\n",
    "\n",
    "                                                            performance_results[large_experiment] = {}\n",
    "                                                            performance_results[large_experiment][format_name] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name][metric] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name][metric][user_comparison] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name][metric][user_comparison][user_reviews_num] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name][metric][user_comparison][user_reviews_num][business_comparison] = {}\n",
    "                                                            performance_results[large_experiment][format_name][representation_name][model_name][metric][user_comparison][user_reviews_num][business_comparison][business_reviews_num] = results\n",
    "\n",
    "                                                            value_list = [large_experiment, format_name, representation_name, model_name, user_comparison, user_reviews_num, \n",
    "                                                                          business_comparison, business_reviews_num, metric, filtered_reviews_num, \n",
    "                                                                          filtered_users_count, filtered_businesses_count, len(X_test), *results.values()]\n",
    "\n",
    "                                                            performance_results_df = []\n",
    "                                                            performance_results_df.append(value_list)\n",
    "                                                            column_list = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"user_comparison\", \"user_reviews_num\", \"business_comparison\", \n",
    "                                                                           \"business_reviews_num\", \"metric\", \"filtered_reviews_num\", \"filtered_users_count\",\n",
    "                                                                           \"filtered_businesses_count\", \"num_of_test_data\", *results.keys()]\n",
    "\n",
    "                                                            # Save results to file\n",
    "                                                            if run_id != 0:\n",
    "                                                                records_df = pd.DataFrame(performance_results_df, columns=column_list)\n",
    "                                                                records_df.to_csv(record_file_name, mode='a', header=False, index=False, float_format='%.6f')\n",
    "\n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7a2f4",
   "metadata": {},
   "source": [
    "# Inspect performance of different stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def inspect_stars_performance(experiment_model_best_run_ids, task_type=\"regression\"):\n",
    "    \n",
    "    performance_results = {}\n",
    "#     performance_results_df = []\n",
    "    \n",
    "    for large_experiment, small_experiments in experiment_model_best_run_ids.items():\n",
    "        \n",
    "        if experiment_model_best_run_ids[large_experiment][\"bbs\"][\"FM\"][\"MSE\"] != 0:\n",
    "            \n",
    "            for representation, models in small_experiments.items():\n",
    "\n",
    "                representation_name = \"BERT_base_sentence_embedding\" if representation == \"bbs\" else \"RoBERTa_base_sentence_embedding\"\n",
    "                format_name = \"text_format4\" if \"ori\" in large_experiment and \"gpt\" not in large_experiment else \"text_format_v4\"\n",
    "                path = f\"./{large_experiment}/{format_name}/{representation_name}/\"\n",
    "\n",
    "                training_data = 0\n",
    "                validation_data = 0\n",
    "                test_data = 0\n",
    "                temp_training_data_list = []\n",
    "                temp_validation_data_list = []\n",
    "                temp_test_data_list = []\n",
    "                basic_experiments = [\"original\", \"gpt_prompt_v1\", \"gpt_prompt_v3\"]\n",
    "\n",
    "                # loading data\n",
    "                if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "\n",
    "                    for basic_experiment in basic_experiments:\n",
    "                        if basic_experiment in large_experiment:\n",
    "                            path = f\"./{basic_experiment}/{format_name}/{representation_name}/\"\n",
    "                            files = os.listdir(path)\n",
    "                            train_data_name = [file for file in files if 'train' in file and 'set' in file][0]\n",
    "                            valid_data_name = [file for file in files if 'valid' in file and 'set' in file][0]\n",
    "                            test_data_name = [file for file in files if 'test' in file and 'set' in file][0]\n",
    "                            temp_training_data = pd.read_csv(path+train_data_name)\n",
    "        #                     print(training_data.shape)\n",
    "                            temp_training_data_list.append(temp_training_data)\n",
    "                            temp_validation_data = pd.read_csv(path+valid_data_name)\n",
    "        #                     print(validation_data.shape)\n",
    "                            temp_validation_data_list.append(temp_validation_data)\n",
    "                            temp_test_data = pd.read_csv(path+test_data_name)\n",
    "        #                     print(test_data.shape)\n",
    "                            temp_test_data_list.append(temp_test_data)\n",
    "\n",
    "        #             training_data = temp_training_data_list[0]\n",
    "        #             validation_data = temp_validation_data_list[0]\n",
    "        #             test_data = temp_test_data_list[0]\n",
    "\n",
    "                else:\n",
    "                    files = os.listdir(path)\n",
    "                    train_data_name = [file for file in files if 'train' in file and 'set' in file][0]\n",
    "                    valid_data_name = [file for file in files if 'valid' in file and 'set' in file][0]\n",
    "                    test_data_name = [file for file in files if 'test' in file and 'set' in file][0]\n",
    "                    training_data = pd.read_csv(path+train_data_name)\n",
    "        #             print(training_data.shape)\n",
    "                    validation_data = pd.read_csv(path+valid_data_name)\n",
    "        #             print(validation_data.shape)\n",
    "                    test_data = pd.read_csv(path+test_data_name)\n",
    "        #             print(test_data.shape)\n",
    "\n",
    "                train_df = 0\n",
    "                if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "                    train_df = temp_training_data_list[0]\n",
    "                else:\n",
    "                    train_df = training_data\n",
    "\n",
    "                for star_value in [1, 2, 3, 4, 5]:\n",
    "\n",
    "                    filtered_reviews = train_df[train_df['stars']==star_value]\n",
    "\n",
    "                    filtered_reviews_num = len(filtered_reviews)\n",
    "                    filtered_users_count = len(filtered_reviews['user_id'].unique())\n",
    "                    filtered_businesses_count = len(filtered_reviews['business_id'].unique())\n",
    "\n",
    "                    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "                        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "                        print(\"\\n\\n\\n\")\n",
    "                        print(\"-----------------------------------------------------------\")\n",
    "                        print(\"\\n\\n\\n\")\n",
    "#                                     return 0\n",
    "                    else:\n",
    "                        print(\"filtered_reviews_num:\", filtered_reviews_num)\n",
    "                        print(\"filtered_users_count:\", filtered_users_count)\n",
    "                        print(\"filtered_businesses_count:\", filtered_businesses_count)\n",
    "\n",
    "                        unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "                        unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "                        # obtaining X_test and y_test\n",
    "                        X_test = 0\n",
    "                        y_test = 0\n",
    "                        if \"ori\" in large_experiment and \"gpt\" in large_experiment and \"concat\" not in large_experiment:\n",
    "\n",
    "                            test_data_list = []\n",
    "                            is_test_data_empty = False\n",
    "                            for data_df in temp_test_data_list:\n",
    "                                data_df = data_df[\n",
    "                                    (data_df['user_id'].isin(unique_user_ids)) & \n",
    "                                    (data_df['business_id'].isin(unique_business_ids))\n",
    "                                ]\n",
    "                                if len(data_df) < 1:\n",
    "                                    is_test_data_empty = True\n",
    "                                else:\n",
    "                                    data_df.reset_index(inplace=True)\n",
    "                                    test_data_list.append(data_df)\n",
    "\n",
    "                            if is_test_data_empty:\n",
    "                                X_test = 0\n",
    "                                y_test = 0\n",
    "                            else:\n",
    "                                concatenated_vectors = []\n",
    "                                for i in range(len(test_data_list[0])):\n",
    "                                    col_vectors = []\n",
    "                                    for j in range(2): # 2 vectors\n",
    "                                        for data in test_data_list:\n",
    "                                            data_cols = list(data.columns)\n",
    "                                            columns_to_train = [col for col in data_cols if \"vector\" in col.lower()]\n",
    "                                            col = columns_to_train[j]\n",
    "                                            col_vectors.append(np.array(eval(data[col][i])))\n",
    "                                    concatenated_vector = np.concatenate(col_vectors)\n",
    "                                    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "                                X_test = np.array(concatenated_vectors)\n",
    "                                y_test = np.array(test_data_list[0]['stars'])\n",
    "\n",
    "                        else:\n",
    "#                                         print(\"1\")\n",
    "                            test_df = test_data[\n",
    "                                (test_data['user_id'].isin(unique_user_ids)) & \n",
    "                                (test_data['business_id'].isin(unique_business_ids))\n",
    "                            ]\n",
    "#                                         print(\"2\")\n",
    "                            if len(test_df) < 1:\n",
    "                                X_test = 0\n",
    "                                y_test = 0\n",
    "                            else:\n",
    "                                test_df.reset_index(inplace=True)        \n",
    "                                cols = list(test_df.columns)\n",
    "                                columns_to_train = [col for col in cols if \"vector\" in col.lower()]\n",
    "\n",
    "                                concatenated_vectors = []\n",
    "                                for i in range(len(test_df)):\n",
    "                                    col_vectors = []\n",
    "                                    for col in columns_to_train:\n",
    "                                        col_vectors.append(np.array(eval(test_df.loc[i, col])))\n",
    "                                    concatenated_vector = np.concatenate(col_vectors)\n",
    "                                    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "                                X_test = np.array(concatenated_vectors)\n",
    "                                y_test = np.array(test_df['stars'])\n",
    "#                                             print(\"3\")\n",
    "\n",
    "                        if not isinstance(X_test, np.ndarray) and X_test == 0:\n",
    "                            print('Test data is empty ! \\n\\n')\n",
    "                            print(\"-----------------------------------------------------------\")\n",
    "\n",
    "                        else:\n",
    "#                                         print(\"3.1\")\n",
    "                            # start testing\n",
    "                            deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "                            machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                                            \"AdaBoost\", \"XGBoost\", \"CatBoost\"]\n",
    "\n",
    "                            for model_name, metrics in models.items():\n",
    "\n",
    "                                for metric, run_id in metrics.items():\n",
    "\n",
    "#                                                     run_name = f\"{large_experiment}/{format_name}/{representation_name}/{model_name}\"\n",
    "#                                                     run_name = f\"{large_experiment}/{format_name}/{representation_name}\"\n",
    "\n",
    "                                    results = {}\n",
    "\n",
    "                                    rmse = 0.0\n",
    "                                    mse = 0.0\n",
    "                                    mae = 0.0\n",
    "                                    accuracy = 0.0\n",
    "                                    auc_score = 0.0\n",
    "                                    f1 = 0.0\n",
    "                                    precision = 0.0\n",
    "                                    recall = 0.0\n",
    "                                    specificity = 0.0\n",
    "\n",
    "                                    results[\"rmse\"] = rmse\n",
    "                                    results[\"mse\"] = mse\n",
    "                                    results[\"mae\"] = mae\n",
    "                                    results[\"accuracy\"] = accuracy\n",
    "                                    results[\"auc_score\"] = auc_score\n",
    "                                    results[\"f1\"] = f1\n",
    "                                    results[\"precision\"] = precision\n",
    "                                    results[\"recall\"] = recall\n",
    "                                    results[\"specificity\"] = specificity\n",
    "#                                                 print(\"3.5\")\n",
    "                                    record_file_name = \"inspect_few_reviews_performance_results.csv\"\n",
    "                                    if not os.path.exists(record_file_name):\n",
    "                                        column_list = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"metric\",\n",
    "                                                       \"star_value\", \"filtered_reviews_num\", \"filtered_users_count\", \n",
    "                                                       \"filtered_businesses_count\", \"num_of_test_data\", *results.keys()]\n",
    "                                        empty_df = pd.DataFrame(columns=column_list)\n",
    "                                        empty_df.to_csv(record_file_name, index=False, float_format='%.6f')\n",
    "                                    else:\n",
    "                                        record_df = pd.read_csv(record_file_name)\n",
    "                                        columns_to_compare = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"metric\", \"star_value\"]\n",
    "                                        record_df = record_df[columns_to_compare]\n",
    "                                        record_df = record_df.values.tolist()\n",
    "                                        record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "\n",
    "                                        temp_df_values = [[large_experiment, format_name, representation_name, model_name, metric, star_value]]\n",
    "\n",
    "                                        temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                                        temp_df = temp_df.values.tolist()[0]\n",
    "                                        temp_df = [str(value) for value in temp_df]\n",
    "#                                                     print(\"4\")\n",
    "                                        if temp_df in record_df:\n",
    "                                            print(\"This combination already exists in record results !\")\n",
    "#                                                     return 0\n",
    "                                        else:\n",
    "\n",
    "                                            if run_id == 0 or len(X_test) < 1:\n",
    "\n",
    "                                                results[\"rmse\"] = rmse\n",
    "                                                results[\"mse\"] = mse\n",
    "                                                results[\"mae\"] = mae\n",
    "                                                results[\"accuracy\"] = accuracy\n",
    "                                                results[\"auc_score\"] = auc_score\n",
    "                                                results[\"f1\"] = f1\n",
    "                                                results[\"precision\"] = precision\n",
    "                                                results[\"recall\"] = recall\n",
    "                                                results[\"specificity\"] = specificity\n",
    "\n",
    "                                            else:\n",
    "#                                                             print(\"5\")\n",
    "                                                # Specify the directory containing the MLmodel file\n",
    "                                                mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "                                                model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "                                                # Load the model from the specified directory\n",
    "#                                                             model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "                                                print(f'Start predicting with {large_experiment}/{format_name}/{representation_name}/{model_name}/{metric} ...')\n",
    "\n",
    "                                                if model_name in deep_learning_model_names:\n",
    "\n",
    "                                                    model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "                                                    # Convert to float tensor\n",
    "                                                    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "                                                    Xi_test_tensor = torch.arange(X_test_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_test_tensor.size(0), 1, 1).int()\n",
    "                                                    y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "                                                    # Convert data to DataLoader\n",
    "                                                    test_dataset = TensorDataset(Xi_test_tensor, X_test_tensor, y_test_tensor)\n",
    "                                                    test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "                                                    for t, (xi, x, _) in enumerate(test_loader):\n",
    "\n",
    "                                                        if model_name == \"DeepFM\":\n",
    "                                                            y_pred = model(xi, x)\n",
    "                                                        elif model_name == \"MLP\":\n",
    "                                                            y_pred = model(x)\n",
    "                                                            y_pred = y_pred.view(-1)\n",
    "                                                        else:    \n",
    "                                                            y_pred = model(x)\n",
    "\n",
    "                                                        y_pred = y_pred.detach().numpy()\n",
    "\n",
    "                                                elif model_name in machine_learning_model_names:\n",
    "                                                    if model_name == \"XGBoost\":\n",
    "                                                        model = mlflow.xgboost.load_model(model_dir)\n",
    "                                                    elif model_name == \"CatBoost\":\n",
    "                                                        model = mlflow.catboost.load_model(model_dir)\n",
    "                                                    else:\n",
    "                                                        model = mlflow.sklearn.load_model(model_dir)\n",
    "\n",
    "                                                    y_pred = model.predict(X_test)\n",
    "\n",
    "                                                else:\n",
    "                                                    print(f\"Please select model in {deep_learning_model_names} or {machine_learning_model_names} !\")\n",
    "                                                    return 0\n",
    "\n",
    "                                                if task_type==\"regression\":\n",
    "                                                    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "                                                    mse = mean_squared_error(y_test, y_pred)\n",
    "                                                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                                                else:\n",
    "                                                    accuracy = accuracy_score(y_test, y_pred) # Accuracy\n",
    "                                                    auc_score = roc_auc_score(y_test, y_pred) # AUC\n",
    "                                                    f1 = f1_score(y_test, y_pred) # F1-score\n",
    "                                                    precision = precision_score(y_test, y_pred) # Precision\n",
    "                                                    recall = recall_score(y_test, y_pred) # Recall\n",
    "                                                    specificity = recall_score(y_test, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "\n",
    "                                                results[\"rmse\"] = rmse\n",
    "                                                results[\"mse\"] = mse\n",
    "                                                results[\"mae\"] = mae\n",
    "                                                results[\"accuracy\"] = accuracy\n",
    "                                                results[\"auc_score\"] = auc_score\n",
    "                                                results[\"f1\"] = f1\n",
    "                                                results[\"precision\"] = precision\n",
    "                                                results[\"recall\"] = recall\n",
    "                                                results[\"specificity\"] = specificity\n",
    "\n",
    "                                            print(f\"{large_experiment}/{format_name}/{representation_name}/{model_name}/{metric}, star_value:{star_value}\", results)\n",
    "\n",
    "                                            performance_results[large_experiment] = {}\n",
    "                                            performance_results[large_experiment][format_name] = {}\n",
    "                                            performance_results[large_experiment][format_name][representation_name] = {}\n",
    "                                            performance_results[large_experiment][format_name][representation_name][model_name] = {}\n",
    "                                            performance_results[large_experiment][format_name][representation_name][model_name][metric] = {}\n",
    "                                            performance_results[large_experiment][format_name][representation_name][model_name][metric][star_value] = {}\n",
    "\n",
    "                                            value_list = [large_experiment, format_name, representation_name, model_name, metric, star_value, \n",
    "                                                          filtered_reviews_num, filtered_users_count, filtered_businesses_count, len(X_test), *results.values()]\n",
    "\n",
    "                                            performance_results_df = []\n",
    "                                            performance_results_df.append(value_list)\n",
    "                                            column_list = [\"experiment_name\", \"format_name\", \"representation_name\", \"model_name\", \"metric\", \"star_value\", \n",
    "                                                           \"filtered_reviews_num\", \"filtered_users_count\", \"filtered_businesses_count\", \"num_of_test_data\", *results.keys()]\n",
    "\n",
    "                                            # Save results to file\n",
    "                                            if run_id != 0:\n",
    "                                                records_df = pd.DataFrame(performance_results_df, columns=column_list)\n",
    "                                                records_df.to_csv(record_file_name, mode='a', header=False, index=False, float_format='%.6f')\n",
    "\n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37317254",
   "metadata": {},
   "source": [
    "# Print nested dictionary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent + str(key) + ':')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "        else:\n",
    "            print(' ' * indent + str(key) + ': ' + str(value))\n",
    "\n",
    "def write_nested_dict_to_file(d, f, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            f.write(\"  \" * indent + str(key) + \":\\n\")\n",
    "            write_nested_dict_to_file(value, f, indent + 1)\n",
    "        else:\n",
    "            f.write(\"  \" * indent + str(key) + \": \" + str(value) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda6dcd",
   "metadata": {},
   "source": [
    "# Different number of reviews performance inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def return_filtered_train_test_data(train_df, test_df, user_reviews_num=3,user_comparison=\"equal\", \n",
    "                                    user_reviews_num_range=None, business_reviews_num=100000,\n",
    "                                    business_comparison=\"max\", business_reviews_num_range=None):\n",
    "    \n",
    "    # filter reviews\n",
    "    filtered_reviews, calculation_results = sample_reviews_and_calculate_price_then_return_data(train_df, 100, \n",
    "                                                        user_reviews_num=user_reviews_num,\n",
    "                                                        user_comparison=user_comparison,\n",
    "                                                        user_reviews_num_range=user_reviews_num_range,\n",
    "                                                        business_reviews_num=business_reviews_num,\n",
    "                                                        business_comparison=business_comparison,\n",
    "                                                        business_reviews_num_range=business_reviews_num_range,\n",
    "                                                        genai=\"GPT-3.5 Turbo\", \n",
    "                                                        sampling_method='random', column='None')\n",
    "    \n",
    "    filtered_reviews_num = calculation_results[\"filtered_reviews_num\"]\n",
    "    filtered_users_count = calculation_results[\"filtered_users_count\"]\n",
    "    filtered_businesses_count = calculation_results[\"filtered_businesses_count\"]\n",
    "    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"User reviews: {user_reviews_num} {user_comparison} or {user_reviews_num_range}\")\n",
    "    print(f\"Business reviews: {business_reviews_num} {business_comparison} or {business_reviews_num_range}\")\n",
    "    \n",
    "    calculation_results = {}\n",
    "    \n",
    "    calculation_results[\"Train_data_num_before\"] = len(train_df)\n",
    "    calculation_results[\"Train_data_num_after\"] = filtered_reviews_num\n",
    "    calculation_results[\"Train_data_user_count_before\"] = len(train_df['user_id'].unique())\n",
    "    calculation_results[\"Train_data_user_count_after\"] = filtered_users_count\n",
    "    calculation_results[\"Train_data_business_count_before\"] = len(train_df['business_id'].unique())\n",
    "    calculation_results[\"Train_data_business_count_after\"] = filtered_businesses_count\n",
    "    \n",
    "    print(\"Training data num:\")\n",
    "    print(\"Before:\", len(train_df), \"After:\", filtered_reviews_num, '\\n')\n",
    "    print(\"Training data user count:\")\n",
    "    print(\"Before:\", len(train_df['user_id'].unique()), \"After:\", filtered_users_count, '\\n')\n",
    "    print(\"Training data business count:\")\n",
    "    print(\"Before:\", len(train_df['business_id'].unique()), \"After:\", filtered_businesses_count, '\\n')\n",
    "    \n",
    "    unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "    unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "    test_data = test_df[\n",
    "        (test_df['user_id'].isin(unique_user_ids)) & \n",
    "        (test_df['business_id'].isin(unique_business_ids))\n",
    "    ]\n",
    "    test_data.reset_index(inplace=True)\n",
    "    \n",
    "    calculation_results[\"Test_data_num_before\"] = len(test_df)\n",
    "    calculation_results[\"Test_data_num_after\"] = len(test_data)\n",
    "    calculation_results[\"Test_data_user_count_before\"] = len(test_df['user_id'].unique())\n",
    "    calculation_results[\"Test_data_user_count_after\"] = len(test_data['user_id'].unique())\n",
    "    calculation_results[\"Test_data_business_count_before\"] = len(test_df['business_id'].unique())\n",
    "    calculation_results[\"Test_data_business_count_after\"] = len(test_data['business_id'].unique())\n",
    "    \n",
    "    print(\"Validation (Test) data num:\")\n",
    "    print(\"Before:\", len(test_df), \"After:\", len(test_data), '\\n')\n",
    "    print(\"Validation (Test) data user count:\")\n",
    "    print(\"Before:\", len(test_df['user_id'].unique()), \"After:\", len(test_data['user_id'].unique()), '\\n')\n",
    "    print(\"Validation (Test) data business count:\")\n",
    "    print(\"Before:\", len(test_df['business_id'].unique()), \"After:\", len(test_data['business_id'].unique()))\n",
    "\n",
    "    return filtered_reviews, test_data, calculation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
