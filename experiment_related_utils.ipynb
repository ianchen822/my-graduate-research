{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b570aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import nbformat\n",
    "\n",
    "# def load_notebook(notebook_path):\n",
    "#     with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#         nb = nbformat.read(f, as_version=4)\n",
    "#     code_cells = [cell.source for cell in nb.cells if cell.cell_type == 'code']\n",
    "#     exec('\\n'.join(code_cells), globals())\n",
    "\n",
    "# # import written function and variable\n",
    "\n",
    "# parent_directory = Path('./')\n",
    "# data_preprocessing_utils_path = parent_directory / 'data_preprocessing_utils.ipynb'\n",
    "# load_notebook(data_preprocessing_utils_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9c33e",
   "metadata": {},
   "source": [
    "# Obtain and log best training and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def obtain_best_train_params(model_name, record_data_df):\n",
    "    \n",
    "    train_best_params = {\n",
    "        'MSE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "        'MAE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "        'RMSE': {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "    }\n",
    "    \n",
    "    if model_name == \"AutoInt\":\n",
    "        record_data_df = record_data_df[(record_data_df[f\"{model_name}_atten_embed_dim\"]==32) &\n",
    "                                    (record_data_df[f\"{model_name}_num_layers\"]==3) &\n",
    "                                    (record_data_df[f\"{model_name}_mlp_dims\"]=='(256, 256, 256)') &\n",
    "                                    (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "    elif model_name == \"MLP\":\n",
    "        record_data_df = record_data_df[(record_data_df[f\"{model_name}_embed_dims\"]=='(256, 256, 256)') &\n",
    "                                    (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "    batch_size = 'Batch_size'\n",
    "    dl_learning_rate = 'DL_learning_rate'\n",
    "    dropouts = f'{model_name}_dropouts'\n",
    "    dropout = f'{model_name}_dropout'\n",
    "\n",
    "    with open(f\"{model_name}_train_best_params.txt\", \"w\") as f:\n",
    "        for metric in ['MSE', 'MAE', 'RMSE']:\n",
    "            print(f'Train {metric}:\\n')\n",
    "            min_metric_indices = record_data_df[metric].nsmallest(5).index\n",
    "            n = 0\n",
    "            for index in min_metric_indices:\n",
    "                n += 1\n",
    "                train_best_params[metric][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                train_best_params[metric][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                if model_name == \"MLP\":\n",
    "                    train_best_params[metric][f'{n}']['dropout'] = float(record_data_df.loc[index, dropout])\n",
    "                else: # AutoInt\n",
    "                    train_best_params[metric][f'{n}']['dropout'] = ast.literal_eval(record_data_df.loc[index, dropouts])\n",
    "                print(f'{n}. ',\n",
    "                    \"batch_size:\", train_best_params[metric][f'{n}']['batch_size'], \n",
    "                    \"dl_learning_rate:\", train_best_params[metric][f'{n}']['dl_learning_rate'], \n",
    "                    \"dropout:\", train_best_params[metric][f'{n}']['dropout'],\n",
    "                    #               \"\\n\",\n",
    "                    \"MSE\", record_data_df.loc[index, 'MSE'], \n",
    "                    \"MAE\", record_data_df.loc[index, 'MAE'], \n",
    "                    \"RMSE\", record_data_df.loc[index, 'RMSE'], \n",
    "                    \"Index:\", index\n",
    "                )\n",
    "                f.write(f'Train {metric}:\\n')\n",
    "                f.write(f'{n}. ' + \n",
    "                      \" batch_size: \" + str(train_best_params[metric][f'{n}']['batch_size']) + \n",
    "                      \" dl_learning_rate: \" + str(train_best_params[metric][f'{n}']['dl_learning_rate']) + \n",
    "                      \" dropout: \" + str(train_best_params[metric][f'{n}']['dropout']) +\n",
    "        #               \"\\n\",\n",
    "                      \" MSE: \" + str(record_data_df.loc[index, 'MSE']) +\n",
    "                      \" MAE: \" + str(record_data_df.loc[index, 'MAE']) +\n",
    "                      \" RMSE: \" + str(record_data_df.loc[index, 'RMSE']) + \n",
    "                      \" Index: \" + str(index)\n",
    "                )\n",
    "            print(\"\\n\")\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "    return train_best_params\n",
    "    \n",
    "def obtain_best_all_params(model_name, record_data, train_best_params=None):\n",
    "    \n",
    "    if model_name in [\"AutoInt\", \"MLP\"]:\n",
    "        \n",
    "        if train_best_params == None:\n",
    "            print(\"AutoInt and MLP need training params first !\")\n",
    "            return 0\n",
    "        \n",
    "        best_all_params = {\n",
    "            \"Train_MSE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },\n",
    "            \"Train_MAE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },\n",
    "            \"Train_RMSE\": {\n",
    "                \"Model_MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "                \"Model_RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "            },    \n",
    "        }\n",
    "        \n",
    "        batch_size = 'Batch_size'\n",
    "        dl_learning_rate = 'DL_learning_rate'\n",
    "        dropouts = f'{model_name}_dropouts'\n",
    "        dropout = f'{model_name}_dropout'\n",
    "        \n",
    "        atten_embed_dim = f\"{model_name}_atten_embed_dim\" # AutoInt\n",
    "        num_layers = f\"{model_name}_num_layers\" # AutoInt\n",
    "        mlp_dims = f\"{model_name}_mlp_dims\" # AutoInt\n",
    "        embed_dims = f\"{model_name}_embed_dims\" # MLP\n",
    "        \n",
    "        for train_metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            \n",
    "            record_data_df = record_data\n",
    "            \n",
    "            metric_batch_size = train_best_params[train_metric]['1']['batch_size']\n",
    "            metric_dl_learning_rate = train_best_params[train_metric]['1']['dl_learning_rate']\n",
    "            metric_dropout = train_best_params[train_metric]['1']['dropout']\n",
    "#             print(\"metric_batch_size:\", metric_batch_size, \"metric_batch_size type:\", type(metric_batch_size))\n",
    "#             print(\"metric_dl_learning_rate:\", metric_dl_learning_rate, \"metric_dl_learning_rate type:\", type(metric_dl_learning_rate))\n",
    "#             print(\"metric_dropout:\", metric_dropout, \"metric_dropout type:\", type(metric_dropout))\n",
    "            record_data_df = record_data_df[(record_data_df[f\"{batch_size}\"]==metric_batch_size) &\n",
    "                                        (record_data_df[f\"{dl_learning_rate}\"]==metric_dl_learning_rate) &\n",
    "                                        (record_data_df[\"Epoch/Epochs_num\"]=='100th/100')]\n",
    "#             print(len(record_data_df))\n",
    "            if model_name == \"AutoInt\":\n",
    "                record_data_df = record_data_df[record_data_df[f\"{dropouts}\"]==str(metric_dropout)]\n",
    "#                 print(len(record_data_df))\n",
    "            elif model_name == \"MLP\":\n",
    "                record_data_df = record_data_df[record_data_df[f\"{dropout}\"]==metric_dropout]\n",
    "#                 print(len(record_data_df))\n",
    "            else:\n",
    "                print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "                return 0\n",
    "            \n",
    "            for model_metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            \n",
    "                min_metric_indices = record_data_df[model_metric].nsmallest(5).index\n",
    "                n = 0\n",
    "                for index in min_metric_indices:\n",
    "                    n += 1\n",
    "                    best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                    best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                    \n",
    "                    if model_name == \"AutoInt\":\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dropout'] = ast.literal_eval(record_data_df.loc[index, dropouts])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['atten_embed_dim'] = int(record_data_df.loc[index, atten_embed_dim])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['num_layers'] = int(record_data_df.loc[index, num_layers])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['mlp_dims'] = ast.literal_eval(record_data_df.loc[index, mlp_dims])\n",
    "                        \n",
    "                    elif model_name == \"MLP\":\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['dropout'] = float(record_data_df.loc[index, dropout])\n",
    "                        best_all_params[f\"Train_{train_metric}\"][f\"Model_{model_metric}\"][f'{n}']['embed_dims'] = ast.literal_eval(record_data_df.loc[index, embed_dims])\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "                        return 0\n",
    "        \n",
    "        print_nested_dict(best_all_params)\n",
    "        with open(f\"{model_name}_all_best_params.txt\", \"w\") as f:\n",
    "            write_nested_dict_to_file(best_all_params, f)\n",
    "        \n",
    "        return best_all_params\n",
    "    \n",
    "    elif model_name in [\"FM\", \"XGBoost\", \"RandomForest\"]:\n",
    "        \n",
    "        best_all_params = {\n",
    "            \"MSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "            \"MAE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}},\n",
    "            \"RMSE\": {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}\n",
    "        }\n",
    "        \n",
    "        # FM\n",
    "        batch_size = 'Batch_size'\n",
    "        dl_learning_rate = 'DL_learning_rate'\n",
    "        factors_num = f\"{model_name}_factors_num\"\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_colsample_bytree = f\"{model_name}_colsample_bytree\"\n",
    "        xgb_lambda = f\"{model_name}_lambda\"\n",
    "        xgb_learning_rate = f\"{model_name}_learning_rate\"\n",
    "        xgb_max_depth = f\"{model_name}_max_depth\"\n",
    "        xgb_min_child_weight = f\"{model_name}_min_child_weight\"\n",
    "        xgb_n_estimators = f\"{model_name}_n_estimators\"\n",
    "        xgb_subsample = f\"{model_name}_subsample\"\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_max_depth = f\"{model_name}_max_depth\"\n",
    "        rf_max_features = f\"{model_name}_max_features\"\n",
    "        rf_min_samples_leaf = f\"{model_name}_min_samples_leaf\"\n",
    "        rf_min_samples_split = f\"{model_name}_min_samples_split\"\n",
    "        rf_n_estimators = f\"{model_name}_n_estimators\"\n",
    "        \n",
    "        record_data_df = record_data\n",
    "        \n",
    "        if model_name == \"FM\":\n",
    "            record_data_df = record_data_df[record_data_df[\"Epoch/Epochs_num\"]=='100th/100']\n",
    "        \n",
    "        for metric in [\"MSE\", \"MAE\", \"RMSE\"]:\n",
    "            min_metric_indices = record_data_df[metric].nsmallest(5).index\n",
    "            n = 0\n",
    "            for index in min_metric_indices:\n",
    "                n += 1\n",
    "                if model_name == \"FM\":\n",
    "                    best_all_params[metric][f'{n}']['factors_num'] = int(record_data_df.loc[index, factors_num])\n",
    "                    best_all_params[metric][f'{n}']['batch_size'] = int(record_data_df.loc[index, batch_size])\n",
    "                    best_all_params[metric][f'{n}']['dl_learning_rate'] = float(record_data_df.loc[index, dl_learning_rate])\n",
    "                \n",
    "                elif model_name == \"XGBoost\":\n",
    "                    best_all_params[metric][f'{n}']['colsample_bytree'] = float(record_data_df.loc[index, xgb_colsample_bytree])\n",
    "                    best_all_params[metric][f'{n}']['lambda'] = float(record_data_df.loc[index, xgb_lambda])\n",
    "                    best_all_params[metric][f'{n}']['learning_rate'] = float(record_data_df.loc[index, xgb_learning_rate])\n",
    "                    best_all_params[metric][f'{n}']['max_depth'] = int(record_data_df.loc[index, xgb_max_depth])\n",
    "                    best_all_params[metric][f'{n}']['min_child_weight'] = int(record_data_df.loc[index, xgb_min_child_weight])\n",
    "                    best_all_params[metric][f'{n}']['n_estimators'] = int(record_data_df.loc[index, xgb_n_estimators])\n",
    "                    best_all_params[metric][f'{n}']['subsample'] = float(record_data_df.loc[index, xgb_subsample])\n",
    "    \n",
    "                elif model_name == \"RandomForest\":\n",
    "                    max_feature_num = record_data_df.loc[index, rf_max_features]\n",
    "                    max_feature_num = int(max_feature_num) if max_feature_num.isdigit() else max_feature_num\n",
    "                    max_depth = record_data_df.loc[index, rf_max_depth]\n",
    "                    max_depth = None if math.isnan(max_depth) else int(max_depth)\n",
    "                    best_all_params[metric][f'{n}']['max_depth'] = max_depth\n",
    "                    best_all_params[metric][f'{n}']['max_features'] = max_feature_num\n",
    "                    best_all_params[metric][f'{n}']['min_samples_leaf'] = int(record_data_df.loc[index, rf_min_samples_leaf])\n",
    "                    best_all_params[metric][f'{n}']['min_samples_split'] = int(record_data_df.loc[index, rf_min_samples_split])\n",
    "                    best_all_params[metric][f'{n}']['n_estimators'] = int(record_data_df.loc[index, rf_n_estimators])\n",
    "        \n",
    "        print_nested_dict(best_all_params)\n",
    "        with open(f\"{model_name}_all_best_params.txt\", \"w\") as f:\n",
    "            write_nested_dict_to_file(best_all_params, f)\n",
    "        \n",
    "        return best_all_params\n",
    "        \n",
    "    else:\n",
    "        print(\"Currently support FM, AutoInt, MLP, XGBoost, Random Forest !\")\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8aaa2",
   "metadata": {},
   "source": [
    "# Testing with mlflow experiment name (run id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def test_with_mlflow_experiment_name(X_train, y_train, X_valid, y_valid, model_name, \n",
    "                                experiment_name, metric, sorting='ASC', top_n=1, save_records=True):\n",
    "    \n",
    "    if \"tv\" not in experiment_name:\n",
    "        print(\"Experiment must come from validation experiment !\")\n",
    "        return 0\n",
    "    \n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    experiment_id = 0\n",
    "    if experiment is not None:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    else:\n",
    "        print(\"Experiment is empty !\")\n",
    "        return 0\n",
    "    \n",
    "    search_params = {\n",
    "        \"order_by\": [f\"metrics.eval_{metric.lower()} {sorting}\"],\n",
    "        \"max_results\": top_n\n",
    "    }\n",
    "    results = mlflow.search_runs(experiment_ids=experiment_id, \n",
    "                                 order_by=search_params[\"order_by\"], \n",
    "                                 max_results=search_params[\"max_results\"])\n",
    "    run_id = 0\n",
    "    if not results.empty:\n",
    "        run_id = results.iloc[0][\"run_id\"]\n",
    "        run_id = str(run_id)\n",
    "        print(f\"Run ID from best {metric} params in {experiment_name}:\", run_id)\n",
    "    else:\n",
    "        print(\"Run ID is empty !\")\n",
    "        return 0\n",
    "\n",
    "    run_info = mlflow.get_run(run_id)\n",
    "    params = run_info.data.params\n",
    "    \n",
    "    print_nested_dict(params)\n",
    "    \n",
    "    phase = \"test\"\n",
    "    \n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\"]\n",
    "    regression_loss_list = [\"MSE, RMSE\", \"MAE\"]\n",
    "    classification_loss_list = [\"CrossEntropy\", \"BinaryCrossEntropy\"]\n",
    "    \n",
    "    if save_records:\n",
    "            \n",
    "        validation_column_list = [\n",
    "            \"Timestamp\", \"Model_name\", *params.keys(), \"RMSE\", \"MSE\", \"MAE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "        ]\n",
    "\n",
    "        # Create folder if not exists\n",
    "        folder_name = f\"{model_name}_result_records\"\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records_with_runID.csv')\n",
    "\n",
    "        if not os.path.exists(validation_file_path):\n",
    "            empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "            empty_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "\n",
    "        else:\n",
    "            record_df = pd.read_csv(validation_file_path)\n",
    "            columns_to_compare = [\"Model_name\", *params.keys()]\n",
    "            record_df = record_df[columns_to_compare]\n",
    "            record_df = record_df.values.tolist()\n",
    "            record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "            temp_df_values = [[model_name, *params.values()]]\n",
    "            temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "            temp_df = temp_df.values.tolist()[0]\n",
    "            temp_df = [str(value) for value in temp_df]\n",
    "\n",
    "            if temp_df in record_df:\n",
    "                print(\"Parameters already exists in test results !\")\n",
    "                return 0\n",
    "\n",
    "    else:\n",
    "        print(\"Test results are not saved to csv !\")\n",
    "    \n",
    "    \n",
    "    if model_name in deep_learning_model_names:\n",
    " \n",
    "        with mlflow.start_run(run_name=f\"Trained {metric.upper()} params\") as run:\n",
    "            \n",
    "            mlflow.log_params(params) # Log training parameters\n",
    "            model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "            model = mlflow.pytorch.load_model(model_dir) # Load the PyTorch model from the specified directory\n",
    "            \n",
    "            # Log model summary.\n",
    "            with open(f\"{model_name}_model_summary.txt\", \"w\") as f:\n",
    "                f.write(str(summary(model)))\n",
    "            mlflow.log_artifact(f\"{model_name}_model_summary.txt\")\n",
    "\n",
    "            # Convert to float tensor\n",
    "            X_train_tensor = torch.from_numpy(X_train).float()\n",
    "            Xi_train_tensor = torch.arange(X_train_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_train_tensor.size(0), 1, 1).int()\n",
    "            y_train_tensor = torch.from_numpy(y_train).float()\n",
    "            X_valid_tensor = torch.from_numpy(X_valid).float()\n",
    "            Xi_valid_tensor = torch.arange(X_valid_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_valid_tensor.size(0), 1, 1).int()\n",
    "            y_valid_tensor = torch.from_numpy(y_valid).float()\n",
    "\n",
    "            # Convert data to DataLoader\n",
    "            train_dataset = TensorDataset(Xi_train_tensor, X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataset = TensorDataset(Xi_valid_tensor, X_valid_tensor, y_valid_tensor)\n",
    "            validation_loader = DataLoader(validation_dataset, batch_size=len(validation_data), shuffle=True)\n",
    "            \n",
    "            y_pred = 0\n",
    "            validation_results_df = []\n",
    "            criterion = 0\n",
    "            \n",
    "            print(f\"Start testing with best {metric.upper()} params ...\")\n",
    "            \n",
    "            for t, (xi, x, y_true) in enumerate(validation_loader):\n",
    "\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                elif model_name == \"MLP\":\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = y_pred.view(-1)\n",
    "                else:\n",
    "                    y_pred = model(x)\n",
    "            \n",
    "            # Calculate metric values\n",
    "            loss = 0.0\n",
    "\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if params['task_type'] == \"Regression\":\n",
    "                criterion = nn.MSELoss()\n",
    "                if params['loss_type'] == \"RMSE\":\n",
    "                    eps = 1e-6\n",
    "                    if model_name == \"FM\":\n",
    "                        loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                    else:\n",
    "                        loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                elif params['loss_type'] == \"MSE\":\n",
    "                    if model_name == \"FM\":\n",
    "                        loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                    else:        \n",
    "                        loss = criterion(y_pred, y_true.view(-1))\n",
    "                else:\n",
    "                    print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                    return 0\n",
    "\n",
    "                y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = y_pred.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                predictions = y_pred\n",
    "                \n",
    "                rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                mse = mean_squared_error(y_true, y_pred)\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "            elif params['task_type'] == \"Classification\":\n",
    "                if params['loss_type'] == \"BCE\":\n",
    "                        criterion = nn.BCELoss()\n",
    "                        y_pred = torch.sigmoid(y_pred)\n",
    "                elif params['loss_type'] == \"CE\":\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                else:\n",
    "                    print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                    return 0\n",
    "\n",
    "                loss = criterion(y_pred, y_true)\n",
    "\n",
    "                y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                y_pred = y_pred.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                predictions = y_pred\n",
    "                \n",
    "                accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                precision = precision_score(y_true, y_pred) # Precision\n",
    "                recall = recall_score(y_true, y_pred) # Recall\n",
    "                specificity = recall_score(y_true, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "#             mlflow.log_metric(f\"{phase}_loss\", f\"{loss:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mse\", f\"{mse:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mae\", f\"{mae:6f}\")\n",
    "\n",
    "            mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_f1\", f\"{f1:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_precision\", f\"{precision:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_recall\", f\"{recall:6f}\")\n",
    "            mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:6f}\")\n",
    "\n",
    "            # Save the trained model to MLflow.\n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            mlflow.pytorch.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *params.values(), \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            validation_results_df.append(value_list)\n",
    "            \n",
    "            # Save training and validation results to file\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "    \n",
    "    elif model_name in machine_learning_model_names:\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"Trained {metric.upper()} params\") as run:\n",
    "            \n",
    "            mlflow.log_params(params) # Log training parameters\n",
    "            model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "            model = 0\n",
    "            \n",
    "            if model_name in [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\"]:\n",
    "                model = mlflow.sklearn.load_model(model_dir)\n",
    "            elif model_name == \"XGBoost\":\n",
    "                model = mlflow.xgboost.load_model(model_dir)\n",
    "            elif model_name == \"CatBoost\":\n",
    "                model = mlflow.catboost.load_model(model_dir)\n",
    "            else:\n",
    "                print(f\"Model not in {machine_learning_model_names}, couldn't load the model !\")\n",
    "                        \n",
    "            validation_results_df = []\n",
    "\n",
    "            print(f\"Start testing with best {metric.upper()} params ...\")\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "            predictions = y_pred\n",
    "            \n",
    "            # Calculate metric values\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if params['task_type'] == \"Regression\":\n",
    "                rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "                mse = mean_squared_error(y_valid, y_pred)\n",
    "                mae = mean_absolute_error(y_valid, y_pred)\n",
    "            elif params['task_type'] == \"Classification\":\n",
    "                accuracy = accuracy_score(y_valid, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_valid, y_pred) # AUC\n",
    "                f1 = f1_score(y_valid, y_pred) # F1-score\n",
    "                precision = precision_score(y_valid, y_pred) # Precision\n",
    "                recall = recall_score(y_valid, y_pred) # Recall\n",
    "                specificity = recall_score(y_valid, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "            mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mse\", f\"{mse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mae\", f\"{mae:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_f1\", f\"{f1:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_precision\", f\"{precision:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_recall\", f\"{recall:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:4f}\")\n",
    "\n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            # Save the trained model to MLflow.\n",
    "            if model_name in [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\"]:\n",
    "                mlflow.sklearn.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"XGBoost\":\n",
    "                mlflow.xgboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.xgboost.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"CatBoost\":\n",
    "                mlflow.catboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "    #                 mlflow.catboost.log_model(model, f\"{model_name}_model\")\n",
    "            else:\n",
    "                print(f\"Model not in {machine_learning_model_names}, couldn't save the model !\")\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *params.values(), \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            validation_results_df.append(value_list)       \n",
    "    #         print(f\"RMSE on the validation set: {rmse}\")\n",
    "\n",
    "            # Save validation results to file\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "    \n",
    "    else:\n",
    "        return f\"Please select a model in {deep_learning_model_names} and {machine_learning_model_names} !\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58c6ce",
   "metadata": {},
   "source": [
    "# Review num distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def get_word_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def calcuate_reviews_num(reviews_df): \n",
    "\n",
    "    # Count reviews per user and calculate total word count and token count per user\n",
    "    user_stats = defaultdict(dict)\n",
    "    \n",
    "    for index, review in reviews_df.iterrows():\n",
    "        user_id = review['user_id']\n",
    "        if user_id not in user_stats:\n",
    "            user_stats[user_id]['reviews_num'] = 0\n",
    "            user_stats[user_id]['words_num'] = 0\n",
    "            user_stats[user_id]['tokens_num'] = 0\n",
    "\n",
    "        user_stats[user_id]['reviews_num'] += 1\n",
    "        user_stats[user_id]['words_num'] += len(review['text'].split())\n",
    "        user_stats[user_id]['tokens_num'] += len(get_word_tokens(review['text']))\n",
    "\n",
    "    # Count reviews per business and calculate total word count and token count per business\n",
    "    business_stats = defaultdict(dict)\n",
    "\n",
    "    for index, review in reviews_df.iterrows():\n",
    "        business_id = review['business_id']\n",
    "        if business_id not in business_stats:\n",
    "            business_stats[business_id]['reviews_num'] = 0\n",
    "            business_stats[business_id]['words_num'] = 0\n",
    "            business_stats[business_id]['tokens_num'] = 0\n",
    "\n",
    "        business_stats[business_id]['reviews_num'] += 1\n",
    "        business_stats[business_id]['words_num'] += len(review['text'].split())\n",
    "        business_stats[business_id]['tokens_num'] += len(get_word_tokens(review['text']))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    user_df = pd.DataFrame.from_dict(user_stats, orient='index').reset_index()\n",
    "    user_df.columns = ['user_id', 'reviews_num', 'words_num', 'tokens_num']\n",
    "\n",
    "    business_df = pd.DataFrame.from_dict(business_stats, orient='index').reset_index()\n",
    "    business_df.columns = ['business_id', 'reviews_num', 'words_num', 'tokens_num']\n",
    "    \n",
    "    return user_df, business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05256d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    lower_bound = mean - 2 * std\n",
    "    upper_bound = mean + 2 * std\n",
    "    return data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "def calculate_distribution(data):\n",
    "    \n",
    "    data = remove_outliers(data)\n",
    "    \n",
    "    distribution = {\n",
    "        'min': np.min(data),\n",
    "        'max': np.max(data),\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'mode': float(np.argmax(np.bincount(data))),\n",
    "        'percentiles': {\n",
    "            '5th': np.percentile(data, 5),\n",
    "            '10th': np.percentile(data, 10),\n",
    "            '15th': np.percentile(data, 15),\n",
    "            '20th': np.percentile(data, 20),\n",
    "            '25th': np.percentile(data, 25),\n",
    "            '30th': np.percentile(data, 30),\n",
    "            '35th': np.percentile(data, 35),\n",
    "            '40th': np.percentile(data, 40),\n",
    "            '45th': np.percentile(data, 45),\n",
    "            '50th': np.percentile(data, 50),\n",
    "            '55th': np.percentile(data, 55),\n",
    "            '60th': np.percentile(data, 60),\n",
    "            '65th': np.percentile(data, 65),\n",
    "            '70th': np.percentile(data, 70),\n",
    "            '75th': np.percentile(data, 75),\n",
    "            '80th': np.percentile(data, 80),\n",
    "            '85th': np.percentile(data, 85),\n",
    "            '90th': np.percentile(data, 90),\n",
    "            '95th': np.percentile(data, 95),\n",
    "            '100th': np.percentile(data, 100),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "#     print(\"Minimum:\", distribution['min'])\n",
    "#     print(\"Maximum:\", distribution['max'])\n",
    "#     print(\"Mean:\", distribution['mean'])\n",
    "#     print(\"Standard Deviation:\", distribution['std'])\n",
    "#     print(\"Mode:\", distribution['mode'])\n",
    "\n",
    "#     percentiles = distribution['percentiles']\n",
    "#     for percentile, value in percentiles.items():\n",
    "#         print(f\"{percentile.capitalize()} Percentile:\", value)\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842d8ad",
   "metadata": {},
   "source": [
    "# Few reviews performance inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31939082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def few_reviews_performance_inspection(train_df, test_df, run_id_experiment_task_model, \n",
    "                                       user_reviews_num=3,user_comparison=\"equal\", \n",
    "                                       user_reviews_num_range=None, business_reviews_num=100000,\n",
    "                                       business_comparison=\"max\", business_reviews_num_range=None):\n",
    "    \n",
    "    # filter reviews\n",
    "    filtered_reviews, calculation_results = sample_reviews_and_calculate_price_then_return_data(train_df, 100, \n",
    "                                                        user_reviews_num=user_reviews_num,\n",
    "                                                        user_comparison=user_comparison,\n",
    "                                                        user_reviews_num_range=user_reviews_num_range,\n",
    "                                                        business_reviews_num=business_reviews_num,\n",
    "                                                        business_comparison=business_comparison,\n",
    "                                                        business_reviews_num_range=business_reviews_num_range,\n",
    "                                                        genai=\"GPT-3.5 Turbo\", \n",
    "                                                        sampling_method='random', column='None')\n",
    "    \n",
    "    filtered_reviews_num = calculation_results[\"filtered_reviews_num\"]\n",
    "    filtered_users_count = calculation_results[\"filtered_users_count\"]\n",
    "    filtered_businesses_count = calculation_results[\"filtered_businesses_count\"]\n",
    "    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "        return 0\n",
    "    print(\"filtered_reviews_num:\", filtered_reviews_num)\n",
    "    print(\"filtered_users_count:\", filtered_users_count)\n",
    "    print(\"filtered_businesses_count:\", filtered_businesses_count)\n",
    "    \n",
    "    unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "    unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "    test_data = test_df[\n",
    "        (test_df['user_id'].isin(unique_user_ids)) & \n",
    "        (test_df['business_id'].isin(unique_business_ids))\n",
    "    ]\n",
    "    test_data.reset_index(inplace=True)\n",
    "\n",
    "    # get X_test and y_test\n",
    "    cols = list(test_data.columns)\n",
    "    columns_to_train = [col for col in cols if \"vector\" in col.lower()]\n",
    "\n",
    "    concatenated_vectors = []\n",
    "    for i in range(len(test_data)):\n",
    "        col_vectors = []\n",
    "        for col in columns_to_train:\n",
    "            col_vectors.append(np.array(eval(test_data.loc[i, col])))\n",
    "        concatenated_vector = np.concatenate(col_vectors)\n",
    "        concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "    X_test = np.array(concatenated_vectors)\n",
    "    y_test = np.array(test_data['stars'])\n",
    "\n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\", \"CatBoost\"]\n",
    "    \n",
    "    num_experiments = len(run_id_experiment_task_model[\"run_id\"])\n",
    "    performance_results = {}\n",
    "    for i in range(num_experiments):\n",
    "        run_id = run_id_experiment_task_model[\"run_id\"][i]\n",
    "        experiment_name = run_id_experiment_task_model[\"experiment_name\"][i]\n",
    "    \n",
    "        task_type = run_id_experiment_task_model[\"task_type\"]\n",
    "        model_name = run_id_experiment_task_model[\"model_name\"]\n",
    "\n",
    "        print(\"run_id:\", run_id)\n",
    "        print(\"experiment_name:\", experiment_name)\n",
    "        print(\"task_type:\", task_type)\n",
    "        print(\"model_name\", model_name)\n",
    "\n",
    "        model = 0\n",
    "        y_pred = 0\n",
    "\n",
    "        # Specify the directory containing the MLmodel file\n",
    "        mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "        model_dir = f\"runs:/{run_id}/{model_name}_model\"\n",
    "\n",
    "        print(f'Start predicting with {model_name} model ...')\n",
    "\n",
    "        if model_name in deep_learning_model_names:\n",
    "\n",
    "            # Load the model from the specified directory\n",
    "            model = mlflow.pytorch.load_model(model_dir)\n",
    "\n",
    "            # Convert to float tensor\n",
    "            X_test_tensor = torch.from_numpy(X_test).float()\n",
    "            Xi_test_tensor = torch.arange(X_test_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_test_tensor.size(0), 1, 1).int()\n",
    "            y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "            # Convert data to DataLoader\n",
    "            test_dataset = TensorDataset(Xi_test_tensor, X_test_tensor, y_test_tensor)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            for t, (xi, x, _) in enumerate(test_loader):\n",
    "\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                elif model_name == \"MLP\":\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = y_pred.view(-1)\n",
    "                else:    \n",
    "                    y_pred = model(x)\n",
    "\n",
    "                y_pred = y_pred.detach().numpy()\n",
    "\n",
    "        elif model_name in machine_learning_model_names:\n",
    "            if model_name == \"XGBoost\":\n",
    "                model = mlflow.xgboost.load_model(model_dir)\n",
    "            elif model_name == \"CatBoost\":\n",
    "                model = mlflow.catboost.load_model(model_dir)\n",
    "            else:\n",
    "                model = mlflow.sklearn.load_model(model_dir)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Please select model in {deep_learning_model_names} or {machine_learning_model_names} !\")\n",
    "            return 0\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        mae = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "\n",
    "        if task_type==\"regression\":\n",
    "            rmse = root_mean_squared_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_test, y_pred) # AUC\n",
    "            f1 = f1_score(y_test, y_pred) # F1-score\n",
    "            precision = precision_score(y_test, y_pred) # Precision\n",
    "            recall = recall_score(y_test, y_pred) # Recall\n",
    "            specificity = recall_score(y_test, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "\n",
    "        results[\"rmse\"] = rmse\n",
    "        results[\"mse\"] = mse\n",
    "        results[\"mae\"] = mae\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"auc_score\"] = auc_score\n",
    "        results[\"f1\"] = f1\n",
    "        results[\"precision\"] = precision\n",
    "        results[\"recall\"] = recall\n",
    "        results[\"specificity\"] = specificity\n",
    "        \n",
    "        print(f\"{experiment_name}:\\n\", results)\n",
    "        \n",
    "        performance_results[experiment_name] = results\n",
    "\n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37317254",
   "metadata": {},
   "source": [
    "# Print nested dictionary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent + str(key) + ':')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "        else:\n",
    "            print(' ' * indent + str(key) + ': ' + str(value))\n",
    "\n",
    "def write_nested_dict_to_file(d, f, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            f.write(\"  \" * indent + str(key) + \":\\n\")\n",
    "            write_nested_dict_to_file(value, f, indent + 1)\n",
    "        else:\n",
    "            f.write(\"  \" * indent + str(key) + \": \" + str(value) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda6dcd",
   "metadata": {},
   "source": [
    "# Different number of reviews performance inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def return_filtered_train_test_data(train_df, test_df, user_reviews_num=3,user_comparison=\"equal\", \n",
    "                                    user_reviews_num_range=None, business_reviews_num=100000,\n",
    "                                    business_comparison=\"max\", business_reviews_num_range=None):\n",
    "    \n",
    "    # filter reviews\n",
    "    filtered_reviews, calculation_results = sample_reviews_and_calculate_price_then_return_data(train_df, 100, \n",
    "                                                        user_reviews_num=user_reviews_num,\n",
    "                                                        user_comparison=user_comparison,\n",
    "                                                        user_reviews_num_range=user_reviews_num_range,\n",
    "                                                        business_reviews_num=business_reviews_num,\n",
    "                                                        business_comparison=business_comparison,\n",
    "                                                        business_reviews_num_range=business_reviews_num_range,\n",
    "                                                        genai=\"GPT-3.5 Turbo\", \n",
    "                                                        sampling_method='random', column='None')\n",
    "    \n",
    "    filtered_reviews_num = calculation_results[\"filtered_reviews_num\"]\n",
    "    filtered_users_count = calculation_results[\"filtered_users_count\"]\n",
    "    filtered_businesses_count = calculation_results[\"filtered_businesses_count\"]\n",
    "    if filtered_reviews_num==0 or filtered_users_count==0 or filtered_businesses_count==0:\n",
    "        print(\"filtered_reviews_num or filtered_users_count or filtered_businesses_count = 0 !\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"User reviews: {user_reviews_num} {user_comparison} or {user_reviews_num_range}\")\n",
    "    print(f\"Business reviews: {business_reviews_num} {business_comparison} or {business_reviews_num_range}\")\n",
    "    \n",
    "    calculation_results = {}\n",
    "    \n",
    "    calculation_results[\"Train_data_num_before\"] = len(train_df)\n",
    "    calculation_results[\"Train_data_num_after\"] = filtered_reviews_num\n",
    "    calculation_results[\"Train_data_user_count_before\"] = len(train_df['user_id'].unique())\n",
    "    calculation_results[\"Train_data_user_count_after\"] = filtered_users_count\n",
    "    calculation_results[\"Train_data_business_count_before\"] = len(train_df['business_id'].unique())\n",
    "    calculation_results[\"Train_data_business_count_after\"] = filtered_businesses_count\n",
    "    \n",
    "    print(\"Training data num:\")\n",
    "    print(\"Before:\", len(train_df), \"After:\", filtered_reviews_num, '\\n')\n",
    "    print(\"Training data user count:\")\n",
    "    print(\"Before:\", len(train_df['user_id'].unique()), \"After:\", filtered_users_count, '\\n')\n",
    "    print(\"Training data business count:\")\n",
    "    print(\"Before:\", len(train_df['business_id'].unique()), \"After:\", filtered_businesses_count, '\\n')\n",
    "    \n",
    "    unique_user_ids = filtered_reviews['user_id'].unique()\n",
    "    unique_business_ids = filtered_reviews['business_id'].unique()\n",
    "\n",
    "    test_data = test_df[\n",
    "        (test_df['user_id'].isin(unique_user_ids)) & \n",
    "        (test_df['business_id'].isin(unique_business_ids))\n",
    "    ]\n",
    "    test_data.reset_index(inplace=True)\n",
    "    \n",
    "    calculation_results[\"Test_data_num_before\"] = len(test_df)\n",
    "    calculation_results[\"Test_data_num_after\"] = len(test_data)\n",
    "    calculation_results[\"Test_data_user_count_before\"] = len(test_df['user_id'].unique())\n",
    "    calculation_results[\"Test_data_user_count_after\"] = len(test_data['user_id'].unique())\n",
    "    calculation_results[\"Test_data_business_count_before\"] = len(test_df['business_id'].unique())\n",
    "    calculation_results[\"Test_data_business_count_after\"] = len(test_data['business_id'].unique())\n",
    "    \n",
    "    print(\"Validation (Test) data num:\")\n",
    "    print(\"Before:\", len(test_df), \"After:\", len(test_data), '\\n')\n",
    "    print(\"Validation (Test) data user count:\")\n",
    "    print(\"Before:\", len(test_df['user_id'].unique()), \"After:\", len(test_data['user_id'].unique()), '\\n')\n",
    "    print(\"Validation (Test) data business count:\")\n",
    "    print(\"Before:\", len(test_df['business_id'].unique()), \"After:\", len(test_data['business_id'].unique()))\n",
    "\n",
    "    return filtered_reviews, test_data, calculation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
