{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87aea73d",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a179e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Layers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        return self.fc(x) + self.bias\n",
    "\n",
    "\n",
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(sum(field_dims), embed_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class FieldAwareFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields) original is long type``\n",
    "        \"\"\"\n",
    "        xs = [embedding(x) for embedding in self.embeddings]\n",
    "        ix = []\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "\n",
    "\n",
    "class FactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class InnerProductNetwork(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        return torch.sum(x[:, row] * x[:, col], dim=2)\n",
    "\n",
    "\n",
    "class OuterProductNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_fields, embed_dim, kernel_type='mat'):\n",
    "        super().__init__()\n",
    "        num_ix = num_fields * (num_fields - 1) // 2\n",
    "        if kernel_type == 'mat':\n",
    "            kernel_shape = embed_dim, num_ix, embed_dim\n",
    "        elif kernel_type == 'vec':\n",
    "            kernel_shape = num_ix, embed_dim\n",
    "        elif kernel_type == 'num':\n",
    "            kernel_shape = num_ix, 1\n",
    "        else:\n",
    "            raise ValueError('unknown kernel type: ' + kernel_type)\n",
    "        self.kernel_type = kernel_type\n",
    "        self.kernel = torch.nn.Parameter(torch.zeros(kernel_shape))\n",
    "        torch.nn.init.xavier_uniform_(self.kernel.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        p, q = x[:, row], x[:, col]\n",
    "        if self.kernel_type == 'mat':\n",
    "            kp = torch.sum(p.unsqueeze(1) * self.kernel, dim=-1).permute(0, 2, 1)\n",
    "            return torch.sum(kp * q, -1)\n",
    "        else:\n",
    "            return torch.sum(p * q * self.kernel.unsqueeze(0), -1)\n",
    "\n",
    "\n",
    "class CrossNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.w = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, 1, bias=False) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.b = torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.zeros((input_dim,))) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            xw = self.w[i](x)\n",
    "            x = x0 * xw + self.b[i] + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionalFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, attn_size, dropouts):\n",
    "        super().__init__()\n",
    "        self.attention = torch.nn.Linear(embed_dim, attn_size)\n",
    "        self.projection = torch.nn.Linear(attn_size, 1)\n",
    "        self.fc = torch.nn.Linear(embed_dim, 1)\n",
    "        self.dropouts = dropouts\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        num_fields = x.shape[1]\n",
    "        row, col = [], []\n",
    "        for i in range(num_fields - 1):\n",
    "            for j in range(i + 1, num_fields):\n",
    "                row.append(i), col.append(j)\n",
    "        p, q = x[:, row], x[:, col]\n",
    "        inner_product = p * q\n",
    "        attn_scores = F.relu(self.attention(inner_product))\n",
    "        attn_scores = F.softmax(self.projection(attn_scores), dim=1)\n",
    "        attn_scores = F.dropout(attn_scores, p=self.dropouts[0], training=self.training)\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)\n",
    "        attn_output = F.dropout(attn_output, p=self.dropouts[1], training=self.training)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "class CompressedInteractionNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, cross_layer_sizes, split_half=True):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(cross_layer_sizes)\n",
    "        self.split_half = split_half\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        prev_dim, fc_input_dim = input_dim, 0\n",
    "        for i in range(self.num_layers):\n",
    "            cross_layer_size = cross_layer_sizes[i]\n",
    "            self.conv_layers.append(torch.nn.Conv1d(input_dim * prev_dim, cross_layer_size, 1,\n",
    "                                                    stride=1, dilation=1, bias=True))\n",
    "            if self.split_half and i != self.num_layers - 1:\n",
    "                cross_layer_size //= 2\n",
    "            prev_dim = cross_layer_size\n",
    "            fc_input_dim += prev_dim\n",
    "        self.fc = torch.nn.Linear(fc_input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        xs = []\n",
    "        x0, h = x.unsqueeze(2), x\n",
    "        for i in range(self.num_layers):\n",
    "            x = x0 * h.unsqueeze(1)\n",
    "            batch_size, f0_dim, fin_dim, embed_dim = x.shape\n",
    "            x = x.view(batch_size, f0_dim * fin_dim, embed_dim)\n",
    "            x = F.relu(self.conv_layers[i](x))\n",
    "            if self.split_half and i != self.num_layers - 1:\n",
    "                x, h = torch.split(x, x.shape[1] // 2, dim=1)\n",
    "            else:\n",
    "                h = x\n",
    "            xs.append(x)\n",
    "        return self.fc(torch.sum(torch.cat(xs, dim=1), 2))\n",
    "\n",
    "\n",
    "class AnovaKernel(torch.nn.Module):\n",
    "    def __init__(self, order, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        batch_size, num_fields, embed_dim = x.shape\n",
    "        a_prev = torch.ones((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "        for t in range(self.order):\n",
    "            a = torch.zeros((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "            a[:, t+1:, :] += x[:, t:, :] * a_prev[:, t:-1, :]\n",
    "            a = torch.cumsum(a, dim=1)\n",
    "            a_prev = a\n",
    "        if self.reduce_sum:\n",
    "            return torch.sum(a[:, -1, :], dim=-1, keepdim=True)\n",
    "        else:\n",
    "            return a[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fb4df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# FM Model\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, input_dim, factors_num):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.interaction = nn.Parameter(torch.rand(input_dim, factors_num), requires_grad=True)\n",
    "        nn.init.uniform_(self.interaction, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear_term = self.linear(x)\n",
    "        interaction_1 = torch.matmul(x, self.interaction).pow(2)\n",
    "        interaction_2 = torch.matmul(x. pow(2), self.interaction.pow(2))\n",
    "        interaction_term = 0.5 * torch.sum(interaction_2 - interaction_1, 1, keepdim=True)\n",
    "        output = linear_term + interaction_term\n",
    "        return output\n",
    "\n",
    "# DeepFM Model\n",
    "class DeepFM(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_sizes, embedding_size, hidden_dims, num_classes=1, dropout=[0.5, 0.5]):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.field_size = len(feature_sizes)\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_classes = num_classes\n",
    "        # self.dtype = torch.long\n",
    "        # self.bias = torch.nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        \n",
    "        # FM part\n",
    "        self.fm_first_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, 1) for feature_size in self.feature_sizes])\n",
    "        self.fm_second_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, self.embedding_size) for feature_size in self.feature_sizes])\n",
    "        \n",
    "        # Deep part\n",
    "        all_dims = [self.field_size * self.embedding_size] + self.hidden_dims\n",
    "        for i in range(1, len(hidden_dims) + 1):\n",
    "            setattr(self, 'linear_'+str(i), nn.Linear(all_dims[i-1], all_dims[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(all_dims[i]))\n",
    "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
    "            setattr(self, 'dropout_'+str(i), nn.Dropout(dropout[i-1]))\n",
    "        \n",
    "    def forward(self, Xi, Xv):\n",
    "        \n",
    "        # FM part\n",
    "        fm_first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_first_order_embeddings)]\n",
    "        fm_first_order = torch.cat(fm_first_order_emb_arr, 1)\n",
    "        fm_second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_second_order_embeddings)]\n",
    "        fm_sum_second_order_emb = sum(fm_second_order_emb_arr)\n",
    "        fm_sum_second_order_emb_square = fm_sum_second_order_emb * fm_sum_second_order_emb  # (x+y)^2\n",
    "        fm_second_order_emb_square = [\n",
    "            item*item for item in fm_second_order_emb_arr]\n",
    "        fm_second_order_emb_square_sum = sum(\n",
    "            fm_second_order_emb_square)  # x^2+y^2\n",
    "        fm_second_order = (fm_sum_second_order_emb_square - fm_second_order_emb_square_sum) * 0.5\n",
    "        \n",
    "        # Deep part\n",
    "        deep_emb = torch.cat(fm_second_order_emb_arr, 1)\n",
    "        deep_out = deep_emb\n",
    "        for i in range(1, len(self.hidden_dims) + 1):\n",
    "            deep_out = getattr(self, 'linear_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'batchNorm_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'activation_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'dropout_' + str(i))(deep_out)\n",
    "        \n",
    "        # Sum part\n",
    "        total_sum = torch.sum(fm_first_order, 1) + torch.sum(fm_second_order, 1) + torch.sum(deep_out, 1)\n",
    "        \n",
    "        return total_sum\n",
    "\n",
    "# AFM Model\n",
    "class AttentionalFactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Attentional Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        J Xiao, et al. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, attn_size, dropouts):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.afm = AttentionalFactorizationMachine(embed_dim, attn_size, dropouts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "        x = self.linear(x) + self.afm(embed_x)\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added\n",
    "\n",
    "# DCN Model\n",
    "class DeepCrossNetworkModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Deep & Cross Network.\n",
    "\n",
    "    Reference:\n",
    "        R Wang, et al. Deep & Cross Network for Ad Click Predictions, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, num_layers, mlp_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.num_fields = len(field_dims) # manual added\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.cn = CrossNetwork(self.embed_output_dim, num_layers)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout, output_layer=False)\n",
    "        self.linear = torch.nn.Linear(mlp_dims[-1] + self.embed_output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x).view(-1, self.embed_output_dim)\n",
    "        embed_x = embed_x.view(-1, self.embed_output_dim) # manual added\n",
    "        x_l1 = self.cn(embed_x)\n",
    "        h_l2 = self.mlp(embed_x)\n",
    "        x_stack = torch.cat([x_l1, h_l2], dim=1)\n",
    "        p = self.linear(x_stack)\n",
    "#         return torch.sigmoid(p.squeeze(1))\n",
    "        return p.squeeze(1) # manual added\n",
    "\n",
    "# xDeepFM Model\n",
    "class ExtremeDeepFactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of xDeepFM.\n",
    "\n",
    "    Reference:\n",
    "        J Lian, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, mlp_dims, dropout, cross_layer_sizes, split_half=True):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims) # manual added\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.cin = CompressedInteractionNetwork(len(field_dims), cross_layer_sizes, split_half)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x)\n",
    "        x = self.linear(x) + self.cin(embed_x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added\n",
    "\n",
    "# AutoInt Model\n",
    "class AutomaticFeatureInteractionModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AutoInt.\n",
    "\n",
    "    Reference:\n",
    "        W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.atten_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.atten_output_dim = len(field_dims) * atten_embed_dim\n",
    "        self.has_residual = has_residual\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropouts[1])\n",
    "        self.self_attns = torch.nn.ModuleList([\n",
    "            torch.nn.MultiheadAttention(atten_embed_dim, num_heads, dropout=dropouts[0]) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.attn_fc = torch.nn.Linear(self.atten_output_dim, 1)\n",
    "        if self.has_residual:\n",
    "            self.V_res_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"x:\", x.size())\n",
    "#         embed_x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim)\n",
    "        embed_x = x.view(embed_shape)\n",
    "#         print(\"embed_x:\", embed_x.size())\n",
    "        atten_x = self.atten_embedding(embed_x)\n",
    "#         print(\"atten_x:\", atten_x.size())\n",
    "        cross_term = atten_x.transpose(0, 1)\n",
    "#         cross_term = atten_x\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        for self_attn in self.self_attns:\n",
    "            cross_term, _ = self_attn(cross_term, cross_term, cross_term)\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        cross_term = cross_term.transpose(0, 1)\n",
    "#         print(\"cross_term:\", cross_term.size())\n",
    "        if self.has_residual:\n",
    "            V_res = self.V_res_embedding(embed_x)\n",
    "#             print(\"V_res\", V_res.size())\n",
    "            cross_term += V_res\n",
    "        cross_term = F.relu(cross_term).contiguous().view(-1, self.atten_output_dim)\n",
    "        x = self.linear(x) + self.attn_fc(cross_term) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "        return x.squeeze(1)\n",
    "\n",
    "# AFN Model\n",
    "class LNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of LNN layer\n",
    "    Input shape\n",
    "        - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "    Output shape\n",
    "        - 2D tensor with shape:``(batch_size,LNN_dim*embedding_size)``.\n",
    "    Arguments\n",
    "        - **in_features** : Embedding of feature.\n",
    "        - **num_fields**: int.The field size of feature.\n",
    "        - **LNN_dim**: int.The number of Logarithmic neuron.\n",
    "        - **bias**: bool.Whether or not use bias in LNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_fields, embed_dim, LNN_dim, bias=False):\n",
    "        super(LNN, self).__init__()\n",
    "        self.num_fields = num_fields\n",
    "        self.embed_dim = embed_dim\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.lnn_output_dim = LNN_dim * embed_dim\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(LNN_dim, num_fields))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(LNN_dim, embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields, embedding_size)``\n",
    "        \"\"\"\n",
    "        embed_x_abs = torch.abs(x) # Computes the element-wise absolute value of the given input tensor.\n",
    "        embed_x_afn = torch.add(embed_x_abs, 1e-7)\n",
    "        # Logarithmic Transformation\n",
    "        embed_x_log = torch.log1p(embed_x_afn) # torch.log1p and torch.expm1\n",
    "        lnn_out = torch.matmul(self.weight, embed_x_log)\n",
    "        if self.bias is not None:\n",
    "            lnn_out += self.bias\n",
    "        lnn_exp = torch.expm1(lnn_out)\n",
    "        output = F.relu(lnn_exp).contiguous().view(-1, self.lnn_output_dim)\n",
    "        return output\n",
    "\n",
    "class AdaptiveFactorizationNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AFN.\n",
    "\n",
    "    Reference:\n",
    "        Cheng W, et al. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, 2019.\n",
    "    \"\"\"\n",
    "    def __init__(self, field_dims, embed_dim, LNN_dim, mlp_dims, dropouts):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.linear = FeaturesLinear(field_dims)    # Linear\n",
    "        self.embed_dim = embed_dim # manual added\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)   # Embedding\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.LNN_output_dim = self.LNN_dim * embed_dim\n",
    "        self.LNN = LNN(self.num_fields, embed_dim, LNN_dim)\n",
    "        self.mlp = MultiLayerPerceptron(self.LNN_output_dim, mlp_dims, dropouts[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) # manual added\n",
    "        embed_shape = (batch_size, self.num_fields, self.embed_dim) # manual added\n",
    "        embed_x = x.view(embed_shape) # manual added\n",
    "#         embed_x = self.embedding(x)\n",
    "        lnn_out = self.LNN(embed_x)\n",
    "        x = self.linear(x) + self.mlp(lnn_out)\n",
    "#         return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1) # manual added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2305888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Validation (or Testing) Function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def model_training_validtion_or_testing(X_train, y_train, X_valid, y_valid, \n",
    "                             model_name, hyperparameters, task_type, loss_type, optimizer_type,\n",
    "                             dl_learning_rate, epochs_num, batch_size, save_records=True, testing=False):\n",
    "    \n",
    "    deep_learning_model_names = [\"FM\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\"]\n",
    "    \n",
    "    model = 0\n",
    "\n",
    "    if model_name in deep_learning_model_names:\n",
    "\n",
    "        if model_name == \"FM\":\n",
    "            model = FactorizationMachine(**hyperparameters)\n",
    "        elif model_name == \"DeepFM\":\n",
    "            model = DeepFM(**hyperparameters)\n",
    "        elif model_name == \"AFM\":\n",
    "            model = AttentionalFactorizationMachineModel(**hyperparameters)\n",
    "        elif model_name == \"DCN\":\n",
    "            model = DeepCrossNetworkModel(**hyperparameters)\n",
    "        elif model_name == \"xDeepFM\":\n",
    "            model = ExtremeDeepFactorizationMachineModel(**hyperparameters)\n",
    "        elif model_name == \"AutoInt\":\n",
    "            model = AutomaticFeatureInteractionModel(**hyperparameters)\n",
    "        elif model_name == \"AFN\":\n",
    "            model = AdaptiveFactorizationNetwork(**hyperparameters)\n",
    "        else:\n",
    "            print(f\"Please choose a model in {deep_learning_model_names} !\")\n",
    "\n",
    "        # Convert to float tensor\n",
    "        X_train_tensor = torch.from_numpy(X_train).float()\n",
    "        Xi_train_tensor = torch.arange(X_train_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_train_tensor.size(0), 1, 1).int()\n",
    "        y_train_tensor = torch.from_numpy(y_train).float()\n",
    "        X_valid_tensor = torch.from_numpy(X_valid).float()\n",
    "        Xi_valid_tensor = torch.arange(X_valid_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_valid_tensor.size(0), 1, 1).int()\n",
    "        y_valid_tensor = torch.from_numpy(y_valid).float()\n",
    "\n",
    "        # Convert data to DataLoader\n",
    "        train_dataset = TensorDataset(Xi_train_tensor, X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_dataset = TensorDataset(Xi_valid_tensor, X_valid_tensor, y_valid_tensor)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=len(validation_data), shuffle=True)\n",
    "\n",
    "        criterion = nn.MSELoss() # Use mean squared error loss as default criterion type\n",
    "#         metric_type = \"RMSE\" # Use RMSE as default metric type\n",
    "        \n",
    "        regression_loss_list = [\"MSE, RMSE\"]\n",
    "        classification_loss_list = [\"CrossEntropy\", \"BinaryCrossEntropy\"]\n",
    "#         classification_metric_list = [\"Accuracy\", \"AUC\", \"ROC\", \"F1-score\"]\n",
    "\n",
    "        optimizer_type_list = [\"Adam\", \"Adagrad\", \"RMSprop\", \"Adadelta\", \"Adamax\", \"Nadam\"]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)  # Use Adam optimizer as default type\n",
    "        \n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adagrad\":\n",
    "            optimizer = optim.Adagrad(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"RMSprop\":\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adadelta\":\n",
    "            optimizer = optim.Adadelta(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adamax\":\n",
    "            optimizer = optim.Adamax(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Nadam\":\n",
    "            optimizer = optim.Nadam(model.parameters(), lr=dl_learning_rate)\n",
    "        else:\n",
    "            print(f\"Please select an optimizer in {optimizer_name_list}\")\n",
    "            return 0\n",
    "        \n",
    "        # Train the model\n",
    "        training_results_df = []\n",
    "        \n",
    "        print(f'Start training {model_name} model ...')\n",
    "        for epoch in range(epochs_num):\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            total_batches = 0\n",
    "            \n",
    "            total_rmse = 0.0\n",
    "            total_mse = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_auc_score = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_recall = 0.0\n",
    "            total_specificity = 0.0\n",
    "\n",
    "            for t, (xi, x, y_true) in enumerate(train_loader):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = 0\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                else:    \n",
    "                    y_pred = model(x)\n",
    "                \n",
    "                loss = 0.0\n",
    "                \n",
    "                rmse = 0.0\n",
    "                mse = 0.0\n",
    "                accuracy = 0.0\n",
    "                auc_score = 0.0\n",
    "                f1 = 0.0\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                specificity = 0.0\n",
    "                \n",
    "                if task_type == \"Regression\":\n",
    "                    criterion = nn.MSELoss()\n",
    "                    if loss_type == \"RMSE\":\n",
    "                        eps = 1e-6\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                        else:\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                    elif loss_type == \"MSE\":\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                        else:        \n",
    "                            loss = criterion(y_pred, y_true.view(-1))\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                        return 0\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy()\n",
    "                    y_pred = y_pred.detach().numpy()\n",
    "                    \n",
    "                    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    \n",
    "                elif task_type == \"Classification\":\n",
    "                    if loss_type == \"BCE\":\n",
    "                        criterion = nn.BCELoss()\n",
    "                        y_pred = torch.sigmoid(y_pred)\n",
    "                    elif loss_type == \"CE\":\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                        return 0\n",
    "                    \n",
    "                    loss = criterion(y_pred, y_true)\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy()\n",
    "                    y_pred = y_pred.detach().numpy()\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                    auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                    f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                    precision = precision_score(y_true, y_pred) # Precision\n",
    "                    recall = recall_score(y_true, y_pred) # Recall\n",
    "                    specificity = recall_score(y_true, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Please make sure the task is regression or classification !\")\n",
    "                    return 0\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_batches += 1\n",
    "                \n",
    "                total_rmse += rmse\n",
    "                total_mse += mse\n",
    "                total_accuracy += accuracy\n",
    "                total_auc_score += auc_score\n",
    "                total_f1 += f1\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_specificity += specificity\n",
    "            \n",
    "            avg_loss = total_loss / total_batches\n",
    "            \n",
    "            avg_rmse = total_rmse / total_batches\n",
    "            avg_mse = total_mse / total_batches\n",
    "            \n",
    "            avg_accuracy = total_accuracy / total_batches\n",
    "            avg_auc_score = total_auc_score / total_batches\n",
    "            avg_f1 = total_f1 / total_batches\n",
    "            avg_precision = total_precision / total_batches\n",
    "            avg_recall = total_recall / total_batches\n",
    "            avg_specificity = total_specificity / total_batches\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, \n",
    "                \"for_testing\" if testing else \"for_validation\",\n",
    "                *hyperparameters.values(), task_type, loss_type,\n",
    "                optimizer_type, dl_learning_rate, epochs_num, batch_size, f\"{epoch + 1}th/{epochs_num}\", \n",
    "                avg_loss, avg_rmse, avg_mse, avg_accuracy, avg_auc_score, \n",
    "                avg_f1, avg_precision, avg_recall, avg_specificity\n",
    "            ]\n",
    "#             print(f'Epoch {epoch + 1}/{epochs_num}, Average RMSE: {average_rmse:.4f}')\n",
    "            \n",
    "            training_results_df.append(value_list)\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        \n",
    "        validation_results_df = []\n",
    "        \n",
    "        print(\"Start validating ...\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t, (xi, x, y_true) in enumerate(validation_loader):\n",
    "                \n",
    "                predictions = 0\n",
    "                if model_name == \"DeepFM\":\n",
    "                    predictions = model(xi, x)\n",
    "                else:\n",
    "                    predictions = model(x)\n",
    "                \n",
    "                all_predictions.append(predictions.numpy())\n",
    "\n",
    "        # Concatenate all predictions into a single numpy array\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "        # Reshape all_predictions if needed\n",
    "        y_pred = all_predictions.squeeze()\n",
    "\n",
    "        # Calculate metric values\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "        \n",
    "        y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            rmse = root_mean_squared_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "        elif task_type == \"Classification\":\n",
    "            accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "            f1 = f1_score(y_true, y_pred) # F1-score\n",
    "            precision = precision_score(y_true, y_pred) # Precision\n",
    "            recall = recall_score(y_true, y_pred) # Recall\n",
    "            specificity = recall_score(y_true, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "        else:\n",
    "            print(\"Please make sure the task is regression or classification !\")\n",
    "            return 0\n",
    "        \n",
    "        value_list = [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), \n",
    "            task_type, loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size, \n",
    "            rmse, mse, accuracy, auc_score, f1, precision, recall, specificity\n",
    "        ]\n",
    "        \n",
    "        validation_results_df.append(value_list)\n",
    "#         print(f\"RMSE on the validation set: {rmse}\")\n",
    "        \n",
    "        # Save training and validation results to file\n",
    "        training_column_list = [\n",
    "            \"Timestamp\", \"Model_name\", \"Training_for\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "            \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "            \"Epoch/Epochs_num\", \"Avg_Loss\", \"Avg_RMSE\", \"Avg_MSE\", \"Avg_Accuracy\", \n",
    "            \"Avg_AUC_score\", \"Avg_F1\", \"Avg_Precision\", \"Avg_Recall\", \"Avg_Specificity\"\n",
    "        ]\n",
    "        validation_column_list = [\n",
    "            \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "            \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "            \"RMSE\", \"MSE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "        ]\n",
    "        \n",
    "        training_records_df = pd.DataFrame(training_results_df, columns=training_column_list)\n",
    "        validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "        \n",
    "#         output = ', '.join([f\"{col} : {val}\" for col, val in validation_records_df.iloc[0].items()])\n",
    "#         print(output)\n",
    "        \n",
    "        if save_records:\n",
    "            \n",
    "            # Create folder if not exists\n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            training_file_path = os.path.join(folder_name, f'{model_name}_training_result_records.csv')\n",
    "            \n",
    "            if not os.path.exists(training_file_path):\n",
    "                training_records_df.to_csv(training_file_path, index=False, float_format='%.4f')\n",
    "            else:\n",
    "                training_records_df.to_csv(training_file_path, mode='a', header=False, index=False, float_format='%.4f')\n",
    "            \n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            validation_file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "            if testing:\n",
    "                validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                validation_file_path = validation_file_path\n",
    "            \n",
    "            if not os.path.exists(validation_file_path):\n",
    "                validation_records_df.to_csv(validation_file_path, index=False, float_format='%.4f')\n",
    "            else:\n",
    "                validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.4f')\n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "\n",
    "    elif model_name in machine_learning_model_names:\n",
    "\n",
    "        if model_name == \"Linear\":\n",
    "            model = LinearRegression() if task_type == \"Regression\" else LogisticRegression()\n",
    "        elif model_name == \"KNN\":\n",
    "            model = KNeighborsRegressor(**hyperparameters) if task_type == \"Regression\" else KNeighborsClassifier(**hyperparameters)\n",
    "        elif model_name == \"SVM\":\n",
    "            model = SVR(**hyperparameters) if task_type == \"Regression\" else SVC(**hyperparameters)\n",
    "        elif model_name == \"DecisionTree\":\n",
    "            model = DecisionTreeRegressor(**hyperparameters) if task_type == \"Regression\" else DecisionTreeClassifier(**hyperparameters)\n",
    "        elif model_name == \"RandomForest\":\n",
    "            model = RandomForestRegressor(**hyperparameters) if task_type == \"Regression\" else RandomForestClassifier(**hyperparameters)\n",
    "        elif model_name == \"AdaBoost\":\n",
    "            model = AdaBoostRegressor(**hyperparameters) if task_type == \"Regression\" else AdaBoostClassifier(**hyperparameters)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**hyperparameters) if task_type == \"Regression\" else XGBClassifier(**hyperparameters)\n",
    "        else:\n",
    "            print(f\"Please choose a model in {machine_learning_model_names} !\")\n",
    "\n",
    "        # Train the model\n",
    "        print(f'Start training {model_name} model ...')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        \n",
    "        validation_results_df = []\n",
    "        \n",
    "        print(\"Start validating ...\")\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        # Calculate metric values\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "            mse = mean_squared_error(y_valid, y_pred)\n",
    "        elif task_type == \"Classification\":\n",
    "            accuracy = accuracy_score(y_valid, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_valid, y_pred) # AUC\n",
    "            f1 = f1_score(y_valid, y_pred) # F1-score\n",
    "            precision = precision_score(y_valid, y_pred) # Precision\n",
    "            recall = recall_score(y_valid, y_pred) # Recall\n",
    "            specificity = recall_score(y_valid, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "        else:\n",
    "            print(\"Please make sure the task is regression or classification !\")\n",
    "            return 0\n",
    "        \n",
    "        value_list = [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), task_type, \n",
    "            rmse, mse, accuracy, auc_score, f1, precision, recall, specificity\n",
    "        ]\n",
    "        \n",
    "        validation_results_df.append(value_list)       \n",
    "#         print(f\"RMSE on the validation set: {rmse}\")\n",
    "        \n",
    "        # Save validation results to file\n",
    "        training_records_df = f\"{model_name} has no training result records.\"\n",
    "        validation_column_list = [\n",
    "            \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\",\n",
    "            \"RMSE\", \"MSE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "        ]\n",
    "        validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "        \n",
    "#         output = ', '.join([f\"{col} : {val}\" for col, val in validation_records_df.iloc[0].items()])\n",
    "#         print(output)\n",
    "        \n",
    "        if save_records:\n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            if testing:\n",
    "                file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                file_path = file_path\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                validation_records_df.to_csv(file_path, index=False, float_format='%.4f')\n",
    "            else:\n",
    "                validation_records_df.to_csv(file_path, mode='a', header=False, index=False, float_format='%.4f')\n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Please select a model in {deep_learning_model_names} and {machine_learning_model_names} !\")\n",
    "        return 0\n",
    "    \n",
    "    return training_records_df, validation_records_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88be8b",
   "metadata": {},
   "source": [
    "# Experiments: Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fa6e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data_path = '../Data_preprocessing/'\n",
    "training_data_path = 'research_training_set_with_concatenated_reviews_and_feature_vectors.csv'\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "# training_data.head()\n",
    "\n",
    "# data_path = '../Data_preprocessing/'\n",
    "validation_data_path = 'research_validation_set_with_concatenated_reviews_and_feature_vectors.csv'\n",
    "validation_data = pd.read_csv(validation_data_path)\n",
    "# validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "162170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Data preprocessing\n",
    "\n",
    "# columns_to_train = ['user_feature_vector', 'business_feature_vector']\n",
    "columns_to_train = ['user_feature_vector', 'business_feature_vector', \"categories_feature_vector\"]\n",
    "\n",
    "concatenated_vectors = []\n",
    "for i in range(len(training_data)):\n",
    "    col_vectors = []\n",
    "    for col in columns_to_train:\n",
    "        col_vectors.append(np.array(eval(training_data[col][i])))\n",
    "    concatenated_vector = np.concatenate(col_vectors)\n",
    "    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "X_train = np.array(concatenated_vectors)\n",
    "y_train = np.array(training_data['stars'])\n",
    "\n",
    "concatenated_vectors = []\n",
    "for i in range(len(validation_data)):\n",
    "    col_vectors = []\n",
    "    for col in columns_to_train:\n",
    "        col_vectors.append(np.array(eval(validation_data[col][i])))\n",
    "    concatenated_vector = np.concatenate(col_vectors)\n",
    "    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "X_valid = np.array(concatenated_vectors)\n",
    "y_valid = np.array(validation_data['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "656e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM\n",
    "# input_dim, factors_num\n",
    "# field_dims, embed_dim=16\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "embedding_size = 768\n",
    "field_num = len(columns_to_train)\n",
    "\n",
    "FM_params = {\n",
    "    'hyperparameters': {\n",
    "        'input_dim': [input_dim], \n",
    "        'factors_num': [5, 10, 15], \n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# DeepFM\n",
    "# feature_sizes, embedding_size, hidden_dims, num_classes=1, dropout=[0.5, 0.5]\n",
    "\n",
    "# Paper:\n",
    "# test activation function: relu, tanh (relu is better)\n",
    "# embedding size seems to be 5\n",
    "# dropout: test 1, 0.9, 0.8, 0.7, 0.6, 0.5 (when 0.6 ~ 0.9 is better)\n",
    "# hidden layer num: test 1, 3, 5, 7, better at begining, but more getting worse\n",
    "# shape: given layer num and total neuron num, test 4 shape, such as 3, 600 =>\n",
    "# constant (200-200-200), increasing (100-200-300), decreasing (300-200-100), and diamond (150-300-150)\n",
    "# constant is better, which is consistent with previous studies\n",
    "\n",
    "DeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'feature_sizes': [[embedding_size for i in range(field_num)]], \n",
    "        'embedding_size': [4, 8], \n",
    "        'hidden_dims': [[64, 32], [128, 64], [256, 128]], \n",
    "        'num_classes': [1], \n",
    "        'dropout': [(0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFM\n",
    "# field_dims, embed_dim, attn_size, dropouts \n",
    "# embed_dim=16, attn_size=16, dropouts=(0.2, 0.2)\n",
    "\n",
    "# Paper\n",
    "# optimizer: mini-batch Adagrad. The batch size for Frappe and MovieLens is set to 128 and 4096 \n",
    "# The embedding size is set to 256 for all methods\n",
    "# without special mention, the attention factor is also 256 (test on 1, 4, 8, 16, 32, 64, 128, 256)\n",
    "# Validation error: performance is stable across different size of attention factors\n",
    "# also test dropout for fm, afm, fifm, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8\n",
    "# Specifically, for AFM, the optimal dropout ratio on Frappe and MovieLens is 0.2 and 0.5\n",
    "# also test l2 regulation(attention network) from 0, 0.5, 1, 2, 4, 8, 16\n",
    "# prove using l2 is better, and when l2 is larger, rmse is decreasing gently \n",
    "\n",
    "AFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'attn_size': [8, 16], \n",
    "        'dropouts': [(0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# DCN\n",
    "# field_dims, embed_dim, num_layers, mlp_dims, dropout \n",
    "# embed_dim=16, num_layers=3, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "# Paper:\n",
    "# cross layer(num_layers): 1~6, hidden layer: 2 ~ 5, with size 32 ~ 1024, \n",
    "# Based on test loss, 2 deep layers of size 1024 and 6 cross layers for the DCN model\n",
    "# 5 deep layers of size 1024 for the DNN\n",
    "# Based on validation loss, more cross layer num DCN loss decrease\n",
    " \n",
    "# initial learning rate was tuned from 0.0001 to 0.001 with increments of 0.0001\n",
    "# early stopping: at training step 150,000\n",
    "# optimization: mini-batch stochastic optimization with Adam optimizer. \n",
    "# batch size: 512. Batch normalization was applied to the deep network\n",
    "# gradient clip: 100\n",
    "# Real-valued features are normalized by applying a log transform. \n",
    "# For categorical features, we embed the features in dense vectors of dimension 6(category cardinality)1/4.\n",
    "# Concatenating all embeddings results in a vector of dimension 1026\n",
    "\n",
    "# Not CTR task, grid search:\n",
    "# deep layers num ranged from 1 to 10 with layer size from 50 to 300. \n",
    "# The number of cross layers ranged from 4 to 10. \n",
    "# The number of residual units ranged from 1 to 5 with their input dim and cross dim from 50 to 300.\n",
    "# For DCN, the input vector was fed to the cross network directly\n",
    "\n",
    "DCN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'num_layers': [3],\n",
    "        'mlp_dims': [(16, 16)],\n",
    "        'dropout': [0.5]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# xDeepFM\n",
    "# embed_dim, mlp_dims, dropout, cross_layer_sizes, split_half=True \n",
    "# embed_dim=16, cross_layer_sizes=(16, 16), split_half=False, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "# Paper\n",
    "# Learning rate : 0.001. optimization method: Adam, mini-batch size: 4096. \n",
    "# L2 regularization with  = 0.0001 for DNN, DCN, Wide&Deep, DeepFM and xDeepFM, dropout 0.5 for PNN\n",
    "# embed_dim = 10, \n",
    "# Neuron num per layer: (1) 400 for DNN layers; \n",
    "# (2) 200 for CIN layers on Criteo dataset, and 100 for CIN layers on Dianping and Bing News datasets\n",
    "# test cin num_layer 1, 2, 3, 4 (3 best, degrad after 3)\n",
    "# test cin num neurons 20, 40, 100, 200, Bing news is increasing(200 best), Dianping is 20 ~ 100 better, 200 degrades \n",
    "# test cin sigmoid, tanh, relu, identity 4 activation, identity is best, then relu\n",
    "# best cross depth and dnn depth is (3, 2)\n",
    "\n",
    "xDeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'cross_layer_sizes': [(16, 16)],\n",
    "        \"split_half\": [False],\n",
    "        'mlp_dims': [(16, 16)],\n",
    "        'dropout': [0.5]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AutoInt\n",
    "# field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True\n",
    "# embed_dim=16, atten_embed_dim=64, num_heads=2, num_layers=3, mlp_dims=(400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "# Paper:\n",
    "# embed_dim is set to 16, batch size set to 1024\n",
    "# num_head is 2, num_layer(interaction layer) is 3, num of hidden units (interaction layer) is 32\n",
    "# test dropout from 0.1 ~ 0.9\n",
    "# optimizer is Adam\n",
    "# test num_layer(interaction layer) from 0 ~ 4, 1 increase dramaticaly, 1 ~ 4 become stable\n",
    "# test atten_embed_dim 8, 16, 24, 32, movie len is getting better, for KDD12, 24 is best, then decrease\n",
    "# hidden units shape is set to (1, 200) or (4, 100)\n",
    "# residaul is crucial\n",
    "\n",
    "AutoInt_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'atten_embed_dim': [(64, 32)],\n",
    "        \"num_heads\": [2],\n",
    "        \"num_layers\": [3],\n",
    "        'mlp_dims': [(16, 16), (400, 400)],\n",
    "        'dropouts': [(0.5, 0.5, 0.5)],\n",
    "        \"has_residual\": [True]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFN\n",
    "# field_dims, embed_dim, LNN_dim, mlp_dims, dropouts\n",
    "# embed_dim=16, LNN_dim=1500, mlp_dims=(400, 400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "# Paper:\n",
    "# LNN_dim: 1500, 1200, 800, 600, embed_dim: 10\n",
    "# batch size: 4096\n",
    "# optimizer: Adam # earning rate :0.001\n",
    "# mlp_dims: (400, 400, 400)\n",
    "# All the other hyperparameters are tuned on the validation set\n",
    "\n",
    "AFN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'LNN_dim': [1500, 1000], \n",
    "        'mlp_dims': [(400, 400, 400)], \n",
    "        'dropouts': [(0.5, 0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 200, 500]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Linear parameters\n",
    "Linear_params = {\n",
    "    'hyperparameters': {\n",
    "        \n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # K-Nearest Neighbors parameters\n",
    "# n_neighbors: int, default=5  # Number of neighbors\n",
    "# weights: {'uniform', 'distance'}, default='uniform'  # Weight function used in prediction\n",
    "# metric: {'euclidean', 'manhattan', 'chebyshev', 'minkowski', \n",
    "#          'wminkowski', 'seuclidean', 'mahalanobis'}, default='minkowski'  # Distance metric\n",
    "\n",
    "KNN_params = {\n",
    "    'hyperparameters': {\n",
    "        'n_neighbors': [5],\n",
    "        'weights': ['uniform'],\n",
    "        'metric': ['euclidean']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Support Vector Machine parameters\n",
    "# C: float, default=1.0  # Penalty parameter C of the error term\n",
    "# kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'  # Kernel function\n",
    "# gamma: {'scale', 'auto'} or float, default='scale'  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "\n",
    "SVM_params = {\n",
    "    'hyperparameters': {\n",
    "        'C': [1.0],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Decision Tree parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "\n",
    "DecisionTree_params = {\n",
    "    'hyperparameters': {\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Random Forest parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "# n_estimators: int, default=100  # Number of trees in the forest\n",
    "\n",
    "RandomForest_params = {\n",
    "    'hyperparameters': {\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [50, 100]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # AdaBoost parameters\n",
    "# n_estimators: int, default=50  # Maximum number of estimators at which boosting is terminated\n",
    "# learning_rate: float, default=1.0  # Learning rate shrinks the contribution of each classifier\n",
    "\n",
    "AdaBoost_params = {\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.02, 0.001]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # XGBoost parameters\n",
    "# learning_rate: float, default=0.1  # Boosting learning rate (xgb's \"eta\")\n",
    "# n_estimators: int, default=100  # Number of boosted trees to fit\n",
    "# subsample: float, default=1  # Subsample ratio of the training instances (xgb's \"subsample\")\n",
    "# colsample_bytree: float, default=1  # Subsample ratio of columns when constructing each tree (xgb's \"colsample_bytree\")\n",
    "\n",
    "XGBoost_params = {\n",
    "    'hyperparameters': {\n",
    "        'learning_rate': [0.01, 0.02, 0.001],\n",
    "        'n_estimators': [50, 100],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training hyperparameter combinations setting\n",
    "\n",
    "func_params_list = {\n",
    "    \n",
    "    \"FM\": FM_params,\n",
    "    \"DeepFM\": DeepFM_params,\n",
    "    \"AFM\": AFM_params,\n",
    "    \"DCN\": DCN_params,\n",
    "    \"xDeepFM\": xDeepFM_params,\n",
    "    \"AutoInt\": AutoInt_params,\n",
    "    \"AFN\": AFN_params,\n",
    "    \n",
    "    \"Linear\": Linear_params,\n",
    "    \"KNN\": KNN_params,\n",
    "    \"SVM\": SVM_params,\n",
    "    \"DecisionTree\": DecisionTree_params,\n",
    "    \"RandomForest\": RandomForest_params,\n",
    "    \"AdaBoost\": AdaBoost_params,\n",
    "    \"XGBoost\": XGBoost_params \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5416c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "Linear\n",
      "-----------------------------------------------------------------------------------\n",
      "Start training Linear model ...\n",
      "Start validating ...\n"
     ]
    }
   ],
   "source": [
    "# Start Training and Validation\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "model_names = [\"FM\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\",\n",
    "               \"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\", \"XGBoost\"]\n",
    "\n",
    "# test_model_names = [\"Linear\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print(f\"{model_name}\")\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    \n",
    "    model_params = func_params_list[model_name][\"hyperparameters\"]\n",
    "    model_params_combinations = list(product(*model_params.values()))\n",
    "    \n",
    "    func_params = func_params_list[model_name]['func_params']\n",
    "    func_params_combinations = list(product(*func_params.values()))\n",
    "\n",
    "#     model_func_combinations = []\n",
    "    \n",
    "    for combination in model_params_combinations:\n",
    "        model_params_combination = {}\n",
    "        model_params_with_names = zip(model_params.keys(), combination)\n",
    "        for name, value in model_params_with_names:\n",
    "            model_params_combination[name] = value\n",
    "\n",
    "        for combination in func_params_combinations:\n",
    "            func_params_combination = {}\n",
    "            func_params_with_names = zip(func_params.keys(), combination)\n",
    "            \n",
    "            for name, value in func_params_with_names:\n",
    "                func_params_combination[name] = value\n",
    "\n",
    "            merge_params = {\"hyperparameters\": model_params_combination, **func_params_combination}\n",
    "            \n",
    "            # Training and validation then save results to csv file\n",
    "            model_training_validtion_or_testing(X_train, y_train, X_valid, y_valid, \n",
    "                             model_name, **merge_params, save_records=True, testing=False)\n",
    "        \n",
    "#             model_func_combinations.append(merge_params)\n",
    "    \n",
    "#     print(f\"{model_name} training combinations: {len(model_func_combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6e3c0",
   "metadata": {},
   "source": [
    "# Experiments: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data_path = '../Data_preprocessing/'\n",
    "training_data_path = 'research_training_set_with_concatenated_reviews_and_feature_vectors.csv'\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "# training_data.head()\n",
    "\n",
    "# data_path = '../Data_preprocessing/'\n",
    "test_data_path = 'research_test_set_with_concatenated_reviews_and_feature_vectors.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "# test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Data preprocessing\n",
    "\n",
    "# columns_to_train = ['user_feature_vector', 'business_feature_vector']\n",
    "columns_to_train = ['user_feature_vector', 'business_feature_vector', \"categories_feature_vector\"]\n",
    "\n",
    "concatenated_vectors = []\n",
    "for i in range(len(training_data)):\n",
    "    col_vectors = []\n",
    "    for col in columns_to_train:\n",
    "        col_vectors.append(np.array(eval(training_data[col][i])))\n",
    "    concatenated_vector = np.concatenate(col_vectors)\n",
    "    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "X_train = np.array(concatenated_vectors)\n",
    "y_train = np.array(training_data['stars'])\n",
    "\n",
    "concatenated_vectors = []\n",
    "for i in range(len(test_data)):\n",
    "    col_vectors = []\n",
    "    for col in columns_to_train:\n",
    "        col_vectors.append(np.array(eval(test_data[col][i])))\n",
    "    concatenated_vector = np.concatenate(col_vectors)\n",
    "    concatenated_vectors.append(concatenated_vector)\n",
    "\n",
    "X_test = np.array(concatenated_vectors)\n",
    "y_test = np.array(test_data['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM\n",
    "# input_dim, factors_num\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "embedding_size = 768\n",
    "field_num = len(columns_to_train)\n",
    "\n",
    "best_FM_params = {\n",
    "    'hyperparameters': {\n",
    "        'input_dim': [input_dim], \n",
    "        'factors_num': [5, 10, 15], \n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# DeepFM\n",
    "# feature_sizes, embedding_size, hidden_dims, num_classes=1, dropout=[0.5, 0.5]\n",
    "\n",
    "best_DeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'feature_sizes': [[embedding_size for i in range(field_num)]], \n",
    "        'embedding_size': [4, 8], \n",
    "        'hidden_dims': [[64, 32], [128, 64], [256, 128]], \n",
    "        'num_classes': [1], \n",
    "        'dropout': [(0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFM\n",
    "# field_dims, embed_dim, attn_size, dropouts \n",
    "# embed_dim=16, attn_size=16, dropouts=(0.2, 0.2)\n",
    "\n",
    "best_AFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'attn_size': [8, 16], \n",
    "        'dropouts': [(0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# DCN\n",
    "# field_dims, embed_dim, num_layers, mlp_dims, dropout \n",
    "# embed_dim=16, num_layers=3, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "best_DCN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'num_layers': [3],\n",
    "        'mlp_dims': [(16, 16)],\n",
    "        'dropout': [0.5]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# xDeepFM\n",
    "# embed_dim, mlp_dims, dropout, cross_layer_sizes, split_half=True \n",
    "# embed_dim=16, cross_layer_sizes=(16, 16), split_half=False, mlp_dims=(16, 16), dropout=0.2\n",
    "\n",
    "best_xDeepFM_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'cross_layer_sizes': [(16, 16)],\n",
    "        \"split_half\": [False],\n",
    "        'mlp_dims': [(16, 16)],\n",
    "        'dropout': [0.5]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AutoInt\n",
    "# field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True\n",
    "# embed_dim=16, atten_embed_dim=64, num_heads=2, num_layers=3, mlp_dims=(400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "best_AutoInt_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'atten_embed_dim': [(64, 32)],\n",
    "        \"num_heads\": [2],\n",
    "        \"num_layers\": [3],\n",
    "        'mlp_dims': [(16, 16), (400, 400)],\n",
    "        'dropouts': [(0.5, 0.5, 0.5)],\n",
    "        \"has_residual\": [True]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# AFN\n",
    "# field_dims, embed_dim, LNN_dim, mlp_dims, dropouts\n",
    "# embed_dim=16, LNN_dim=1500, mlp_dims=(400, 400, 400), dropouts=(0, 0, 0)\n",
    "\n",
    "best_AFN_params = {\n",
    "    'hyperparameters': {\n",
    "        'field_dims': [[embedding_size for i in range(field_num)]], \n",
    "        'embed_dim': [768], \n",
    "        'LNN_dim': [1500, 1000], \n",
    "        'mlp_dims': [(400, 400, 400)], \n",
    "        'dropouts': [(0.5, 0.5, 0.5)]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': ['RMSE'], 'optimizer_type': ['Adam'], \n",
    "        'dl_learning_rate': [0.01, 0.02, 0.001], 'epochs_num': [10, 20, 30], 'batch_size': [100, 200, 500]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Linear parameters\n",
    "best_Linear_params = {\n",
    "    'hyperparameters': {\n",
    "        \n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # K-Nearest Neighbors parameters\n",
    "# n_neighbors: int, default=5  # Number of neighbors\n",
    "# weights: {'uniform', 'distance'}, default='uniform'  # Weight function used in prediction\n",
    "# metric: {'euclidean', 'manhattan', 'chebyshev', 'minkowski', \n",
    "#          'wminkowski', 'seuclidean', 'mahalanobis'}, default='minkowski'  # Distance metric\n",
    "\n",
    "best_KNN_params = {\n",
    "    'hyperparameters': {\n",
    "        'n_neighbors': [5],\n",
    "        'weights': ['uniform'],\n",
    "        'metric': ['euclidean']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Support Vector Machine parameters\n",
    "# C: float, default=1.0  # Penalty parameter C of the error term\n",
    "# kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'  # Kernel function\n",
    "# gamma: {'scale', 'auto'} or float, default='scale'  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "\n",
    "best_SVM_params = {\n",
    "    'hyperparameters': {\n",
    "        'C': [1.0],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale']\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Decision Tree parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "\n",
    "best_DecisionTree_params = {\n",
    "    'hyperparameters': {\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Random Forest parameters\n",
    "# max_depth: int or None, default=None  # Maximum depth of the tree\n",
    "# min_samples_split: int or float, default=2  # Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf: int or float, default=1  # Minimum number of samples required to be at a leaf node\n",
    "# n_estimators: int, default=100  # Number of trees in the forest\n",
    "\n",
    "best_RandomForest_params = {\n",
    "    'hyperparameters': {\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [50, 100]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # AdaBoost parameters\n",
    "# n_estimators: int, default=50  # Maximum number of estimators at which boosting is terminated\n",
    "# learning_rate: float, default=1.0  # Learning rate shrinks the contribution of each classifier\n",
    "\n",
    "best_AdaBoost_params = {\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.02, 0.001]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# # XGBoost parameters\n",
    "# learning_rate: float, default=0.1  # Boosting learning rate (xgb's \"eta\")\n",
    "# n_estimators: int, default=100  # Number of boosted trees to fit\n",
    "# subsample: float, default=1  # Subsample ratio of the training instances (xgb's \"subsample\")\n",
    "# colsample_bytree: float, default=1  # Subsample ratio of columns when constructing each tree (xgb's \"colsample_bytree\")\n",
    "\n",
    "best_XGBoost_params = {\n",
    "    'hyperparameters': {\n",
    "        'learning_rate': [0.01, 0.02, 0.001],\n",
    "        'n_estimators': [50, 100],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1]\n",
    "    },\n",
    "    'func_params':{\n",
    "        'task_type': ['Regression'], 'loss_type': [None], 'optimizer_type': [None], \n",
    "        'dl_learning_rate': [None], 'epochs_num': [None], 'batch_size': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training hyperparameter combinations setting\n",
    "\n",
    "best_func_params_list = {\n",
    "    \n",
    "    \"FM\": best_FM_params,\n",
    "    \"DeepFM\": best_DeepFM_params,\n",
    "    \"AFM\": best_AFM_params,\n",
    "    \"DCN\": best_DCN_params,\n",
    "    \"xDeepFM\": best_xDeepFM_params,\n",
    "    \"AutoInt\": best_AutoInt_params,\n",
    "    \"AFN\": best_AFN_params,\n",
    "    \n",
    "    \"Linear\": best_Linear_params,\n",
    "    \"KNN\": best_KNN_params,\n",
    "    \"SVM\": best_SVM_params,\n",
    "    \"DecisionTree\": best_DecisionTree_params,\n",
    "    \"RandomForest\": best_RandomForest_params,\n",
    "    \"AdaBoost\": best_AdaBoost_params,\n",
    "    \"XGBoost\": best_XGBoost_params \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training and Testing\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "model_names = [\"FM\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\",\n",
    "               \"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\", \"XGBoost\"]\n",
    "\n",
    "# test_model_names = [\"Linear\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print(f\"{model_name}\")\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    \n",
    "    model_params = best_func_params_list[model_name][\"hyperparameters\"]\n",
    "    model_params_combinations = list(product(*model_params.values()))\n",
    "    \n",
    "    func_params = best_func_params_list[model_name]['func_params']\n",
    "    func_params_combinations = list(product(*func_params.values()))\n",
    "\n",
    "#     model_func_combinations = []\n",
    "    \n",
    "    for combination in model_params_combinations:\n",
    "        model_params_combination = {}\n",
    "        model_params_with_names = zip(model_params.keys(), combination)\n",
    "        for name, value in model_params_with_names:\n",
    "            model_params_combination[name] = value\n",
    "\n",
    "        for combination in func_params_combinations:\n",
    "            func_params_combination = {}\n",
    "            func_params_with_names = zip(func_params.keys(), combination)\n",
    "            \n",
    "            for name, value in func_params_with_names:\n",
    "                func_params_combination[name] = value\n",
    "\n",
    "            merge_params = {\"hyperparameters\": model_params_combination, **func_params_combination}\n",
    "            \n",
    "            # Training and validation then save results to csv file\n",
    "            model_training_validtion_or_testing(X_train, y_train, X_test, y_test, \n",
    "                             model_name, **merge_params, save_records=True, testing=True)\n",
    "        \n",
    "#             model_func_combinations.append(merge_params)\n",
    "    \n",
    "#     print(f\"{model_name} training combinations: {len(model_func_combinations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
