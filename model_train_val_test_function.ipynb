{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8759bd9",
   "metadata": {},
   "source": [
    "# Model training, validation, testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Validation (or Testing) Function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def model_training_validtion_or_testing(X_train, y_train, X_valid, y_valid, \n",
    "                             model_name, hyperparameters, task_type, loss_type, optimizer_type,\n",
    "                             dl_learning_rate, epochs_num, batch_size, save_records=True, testing=False):\n",
    "    \n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\"]\n",
    "    \n",
    "    model = 0\n",
    "\n",
    "    if model_name in deep_learning_model_names:\n",
    "        \n",
    "        if save_records:\n",
    "            \n",
    "            training_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", \"Training_for\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "                \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "                \"Epoch/Epochs_num\", \"Avg_Loss\", \"Avg_RMSE\", \"Avg_MSE\", \"Avg_Accuracy\", \n",
    "                \"Avg_AUC_score\", \"Avg_F1\", \"Avg_Precision\", \"Avg_Recall\", \"Avg_Specificity\"\n",
    "            ]\n",
    "            validation_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "                \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "                \"RMSE\", \"MSE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "            ]\n",
    "            \n",
    "            # Create folder if not exists\n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            training_file_path = os.path.join(folder_name, f'{model_name}_training_result_records.csv')\n",
    "            \n",
    "            # Create record file if not exists\n",
    "            if not os.path.exists(training_file_path):\n",
    "                empty_df = pd.DataFrame(columns=training_column_list)\n",
    "                empty_df.to_csv(training_file_path, index=False, float_format='%.6f')\n",
    "#             else:\n",
    "#                 record_df = pd.read_csv(training_file_path)\n",
    "#                 columns_to_compare = [\"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "#                                       \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\"]\n",
    "#                 record_df = record_df[columns_to_compare]\n",
    "#                 record_df = record_df.values.tolist()\n",
    "#                 temp_df_values = [[model_name, *hyperparameters.values(), task_type, \n",
    "#                                    loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size]]\n",
    "#                 temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "#                 temp_df = temp_df.values.tolist()[0]\n",
    "                \n",
    "#                 if temp_df in record_df:\n",
    "# #                     print(\"Parameters already exists !\")\n",
    "#                     return 0\n",
    "            \n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            validation_file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "            if testing:\n",
    "                validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                validation_file_path = validation_file_path\n",
    "            \n",
    "            if not os.path.exists(validation_file_path):\n",
    "                empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "                empty_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "            else:\n",
    "                record_df = pd.read_csv(validation_file_path)\n",
    "                columns_to_compare = [\"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())],\n",
    "                          \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\"]\n",
    "                record_df = record_df[columns_to_compare]\n",
    "                record_df = record_df.values.tolist()\n",
    "                temp_df_values = [[model_name, *hyperparameters.values(), task_type, \n",
    "                                   loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size]]\n",
    "                temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                temp_df = temp_df.values.tolist()[0]\n",
    "\n",
    "                if temp_df in record_df:\n",
    "#                     print(\"Parameters already exists !\")\n",
    "                    return 0\n",
    "        \n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "        \n",
    "        if model_name == \"FM\":\n",
    "            model = FactorizationMachine(**hyperparameters)\n",
    "        elif model_name == \"MLP\":\n",
    "            model = MultiLayerPerceptron(**hyperparameters)\n",
    "        elif model_name == \"DeepFM\":\n",
    "            model = DeepFM(**hyperparameters)\n",
    "        elif model_name == \"AFM\":\n",
    "            model = AttentionalFactorizationMachineModel(**hyperparameters)\n",
    "        elif model_name == \"DCN\":\n",
    "            model = DeepCrossNetworkModel(**hyperparameters)\n",
    "        elif model_name == \"xDeepFM\":\n",
    "            model = ExtremeDeepFactorizationMachineModel(**hyperparameters)\n",
    "        elif model_name == \"AutoInt\":\n",
    "            model = AutomaticFeatureInteractionModel(**hyperparameters)\n",
    "        elif model_name == \"AFN\":\n",
    "            model = AdaptiveFactorizationNetwork(**hyperparameters)\n",
    "        else:\n",
    "            print(f\"Please choose a model in {deep_learning_model_names} !\")\n",
    "\n",
    "        # Convert to float tensor\n",
    "        X_train_tensor = torch.from_numpy(X_train).float()\n",
    "        Xi_train_tensor = torch.arange(X_train_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_train_tensor.size(0), 1, 1).int()\n",
    "        y_train_tensor = torch.from_numpy(y_train).float()\n",
    "        X_valid_tensor = torch.from_numpy(X_valid).float()\n",
    "        Xi_valid_tensor = torch.arange(X_valid_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_valid_tensor.size(0), 1, 1).int()\n",
    "        y_valid_tensor = torch.from_numpy(y_valid).float()\n",
    "\n",
    "        # Convert data to DataLoader\n",
    "        train_dataset = TensorDataset(Xi_train_tensor, X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_dataset = TensorDataset(Xi_valid_tensor, X_valid_tensor, y_valid_tensor)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=len(validation_data), shuffle=True)\n",
    "\n",
    "        criterion = nn.MSELoss() # Use mean squared error loss as default criterion type\n",
    "#         metric_type = \"RMSE\" # Use RMSE as default metric type\n",
    "        \n",
    "        regression_loss_list = [\"MSE, RMSE\"]\n",
    "        classification_loss_list = [\"CrossEntropy\", \"BinaryCrossEntropy\"]\n",
    "#         classification_metric_list = [\"Accuracy\", \"AUC\", \"ROC\", \"F1-score\"]\n",
    "\n",
    "        optimizer_type_list = [\"Adam\", \"Adagrad\", \"RMSprop\", \"Adadelta\", \"Adamax\", \"Nadam\"]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)  # Use Adam optimizer as default type\n",
    "        \n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adagrad\":\n",
    "            optimizer = optim.Adagrad(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"RMSprop\":\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adadelta\":\n",
    "            optimizer = optim.Adadelta(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Adamax\":\n",
    "            optimizer = optim.Adamax(model.parameters(), lr=dl_learning_rate)\n",
    "        elif optimizer_type == \"Nadam\":\n",
    "            optimizer = optim.Nadam(model.parameters(), lr=dl_learning_rate)\n",
    "        else:\n",
    "            print(f\"Please select an optimizer in {optimizer_name_list}\")\n",
    "            return 0\n",
    "        \n",
    "        # Train the model\n",
    "        training_results_df = []\n",
    "        \n",
    "        print(f'Start training {model_name} model ...')\n",
    "        for epoch in range(epochs_num):\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            total_batches = 0\n",
    "            \n",
    "            total_rmse = 0.0\n",
    "            total_mse = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_auc_score = 0.0\n",
    "            total_f1 = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_recall = 0.0\n",
    "            total_specificity = 0.0\n",
    "\n",
    "            for t, (xi, x, y_true) in enumerate(train_loader):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = 0\n",
    "                if model_name == \"DeepFM\":\n",
    "                    y_pred = model(xi, x)\n",
    "                else:    \n",
    "                    y_pred = model(x)\n",
    "                \n",
    "                loss = 0.0\n",
    "                \n",
    "                rmse = 0.0\n",
    "                mse = 0.0\n",
    "                accuracy = 0.0\n",
    "                auc_score = 0.0\n",
    "                f1 = 0.0\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                specificity = 0.0\n",
    "                \n",
    "                if task_type == \"Regression\":\n",
    "                    criterion = nn.MSELoss()\n",
    "                    if loss_type == \"RMSE\":\n",
    "                        eps = 1e-6\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                        else:\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                    elif loss_type == \"MSE\":\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                        else:        \n",
    "                            loss = criterion(y_pred, y_true.view(-1))\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                        return 0\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy()\n",
    "                    y_pred = y_pred.detach().numpy()\n",
    "                    \n",
    "                    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    \n",
    "                elif task_type == \"Classification\":\n",
    "                    if loss_type == \"BCE\":\n",
    "                        criterion = nn.BCELoss()\n",
    "                        y_pred = torch.sigmoid(y_pred)\n",
    "                    elif loss_type == \"CE\":\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                        return 0\n",
    "                    \n",
    "                    loss = criterion(y_pred, y_true)\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy()\n",
    "                    y_pred = y_pred.detach().numpy()\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                    auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                    f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                    precision = precision_score(y_true, y_pred) # Precision\n",
    "                    recall = recall_score(y_true, y_pred) # Recall\n",
    "                    specificity = recall_score(y_true, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Please make sure the task is regression or classification !\")\n",
    "                    return 0\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_batches += 1\n",
    "                \n",
    "                total_rmse += rmse\n",
    "                total_mse += mse\n",
    "                total_accuracy += accuracy\n",
    "                total_auc_score += auc_score\n",
    "                total_f1 += f1\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_specificity += specificity\n",
    "            \n",
    "            avg_loss = total_loss / total_batches\n",
    "            \n",
    "            avg_rmse = total_rmse / total_batches\n",
    "            avg_mse = total_mse / total_batches\n",
    "            \n",
    "            avg_accuracy = total_accuracy / total_batches\n",
    "            avg_auc_score = total_auc_score / total_batches\n",
    "            avg_f1 = total_f1 / total_batches\n",
    "            avg_precision = total_precision / total_batches\n",
    "            avg_recall = total_recall / total_batches\n",
    "            avg_specificity = total_specificity / total_batches\n",
    "            \n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, \n",
    "                \"for_testing\" if testing else \"for_validation\",\n",
    "                *hyperparameters.values(), task_type, loss_type,\n",
    "                optimizer_type, dl_learning_rate, epochs_num, batch_size, f\"{epoch + 1}th/{epochs_num}\", \n",
    "                avg_loss, avg_rmse, avg_mse, avg_accuracy, avg_auc_score, \n",
    "                avg_f1, avg_precision, avg_recall, avg_specificity\n",
    "            ]\n",
    "#             print(f'Epoch {epoch + 1}/{epochs_num}, Average RMSE: {average_rmse:.4f}')\n",
    "            \n",
    "            training_results_df.append(value_list)\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        \n",
    "        validation_results_df = []\n",
    "        \n",
    "        print(\"Start validating ...\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t, (xi, x, y_true) in enumerate(validation_loader):\n",
    "                \n",
    "                predictions = 0\n",
    "                if model_name == \"DeepFM\":\n",
    "                    predictions = model(xi, x)\n",
    "                else:\n",
    "                    predictions = model(x)\n",
    "                \n",
    "                all_predictions.append(predictions.numpy())\n",
    "\n",
    "        # Concatenate all predictions into a single numpy array\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "        # Reshape all_predictions if needed\n",
    "        y_pred = all_predictions.squeeze()\n",
    "\n",
    "        # Calculate metric values\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "        \n",
    "        y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            rmse = root_mean_squared_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "        elif task_type == \"Classification\":\n",
    "            accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "            f1 = f1_score(y_true, y_pred) # F1-score\n",
    "            precision = precision_score(y_true, y_pred) # Precision\n",
    "            recall = recall_score(y_true, y_pred) # Recall\n",
    "            specificity = recall_score(y_true, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "        else:\n",
    "            print(\"Please make sure the task is regression or classification !\")\n",
    "            return 0\n",
    "        \n",
    "        value_list = [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), \n",
    "            task_type, loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size, \n",
    "            rmse, mse, accuracy, auc_score, f1, precision, recall, specificity\n",
    "        ]\n",
    "        \n",
    "        validation_results_df.append(value_list)\n",
    "#         print(f\"RMSE on the validation set: {rmse}\")\n",
    "        \n",
    "        # Save training and validation results to file\n",
    "        training_records_df = pd.DataFrame(training_results_df, columns=training_column_list)\n",
    "        validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "        \n",
    "#         output = ', '.join([f\"{col} : {val}\" for col, val in validation_records_df.iloc[0].items()])\n",
    "#         print(output)\n",
    "        \n",
    "#         if save_records:\n",
    "            \n",
    "#             # Create folder if not exists\n",
    "#             folder_name = f\"{model_name}_result_records\"\n",
    "#             if not os.path.exists(folder_name):\n",
    "#                 os.makedirs(folder_name)\n",
    "            \n",
    "#             training_file_path = os.path.join(folder_name, f'{model_name}_training_result_records.csv')\n",
    "            \n",
    "#             if not os.path.exists(training_file_path):\n",
    "#                 training_records_df.to_csv(training_file_path, index=False, float_format='%.6f')\n",
    "#             else:\n",
    "        \n",
    "        training_records_df.to_csv(training_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "            \n",
    "#             # If testing, then change validation results csv file path to testing results csv file path\n",
    "#             validation_file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "#             if testing:\n",
    "#                 validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "#             else: \n",
    "#                 validation_file_path = validation_file_path\n",
    "            \n",
    "#             if not os.path.exists(validation_file_path):\n",
    "#                 validation_records_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "#             else:\n",
    "                \n",
    "        validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "        \n",
    "#         else:\n",
    "#             print(\"Training and validation results are not saved to csv !\")\n",
    "\n",
    "    elif model_name in machine_learning_model_names:\n",
    "        \n",
    "        if save_records:\n",
    "            \n",
    "            training_records_df = f\"{model_name} has no training result records.\"\n",
    "            validation_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\",\n",
    "                \"RMSE\", \"MSE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "            ]\n",
    "            \n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            if testing:\n",
    "                file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                file_path = file_path\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "                empty_df.to_csv(file_path, index=False, float_format='%.6f')\n",
    "            else:\n",
    "                record_df = pd.read_csv(file_path)\n",
    "                columns_to_compare = [\"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\"]\n",
    "                record_df = record_df[columns_to_compare]\n",
    "                record_df = record_df.values.tolist()\n",
    "                temp_df_values = [[model_name, *hyperparameters.values(), task_type]]\n",
    "                temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                temp_df = temp_df.values.tolist()[0]\n",
    "                \n",
    "                if temp_df in record_df:\n",
    "#                     print(\"Parameters already exists !\")\n",
    "                    return 0\n",
    "                \n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "        \n",
    "        if model_name == \"Linear\":\n",
    "            model = LinearRegression() if task_type == \"Regression\" else LogisticRegression()\n",
    "        elif model_name == \"KNN\":\n",
    "            model = KNeighborsRegressor(**hyperparameters) if task_type == \"Regression\" else KNeighborsClassifier(**hyperparameters)\n",
    "        elif model_name == \"SVM\":\n",
    "            model = SVR(**hyperparameters) if task_type == \"Regression\" else SVC(**hyperparameters)\n",
    "        elif model_name == \"DecisionTree\":\n",
    "            model = DecisionTreeRegressor(**hyperparameters) if task_type == \"Regression\" else DecisionTreeClassifier(**hyperparameters)\n",
    "        elif model_name == \"RandomForest\":\n",
    "            model = RandomForestRegressor(**hyperparameters) if task_type == \"Regression\" else RandomForestClassifier(**hyperparameters)\n",
    "        elif model_name == \"AdaBoost\":\n",
    "            model = AdaBoostRegressor(**hyperparameters) if task_type == \"Regression\" else AdaBoostClassifier(**hyperparameters)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**hyperparameters) if task_type == \"Regression\" else XGBClassifier(**hyperparameters)\n",
    "        else:\n",
    "            print(f\"Please choose a model in {machine_learning_model_names} !\")\n",
    "\n",
    "        # Train the model\n",
    "        print(f'Start training {model_name} model ...')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        \n",
    "        validation_results_df = []\n",
    "        \n",
    "        print(\"Start validating ...\")\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        # Calculate metric values\n",
    "        rmse = 0.0\n",
    "        mse = 0.0\n",
    "        accuracy = 0.0\n",
    "        auc_score = 0.0\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        specificity = 0.0\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "            mse = mean_squared_error(y_valid, y_pred)\n",
    "        elif task_type == \"Classification\":\n",
    "            accuracy = accuracy_score(y_valid, y_pred) # Accuracy\n",
    "            auc_score = roc_auc_score(y_valid, y_pred) # AUC\n",
    "            f1 = f1_score(y_valid, y_pred) # F1-score\n",
    "            precision = precision_score(y_valid, y_pred) # Precision\n",
    "            recall = recall_score(y_valid, y_pred) # Recall\n",
    "            specificity = recall_score(y_valid, y_pred, pos_label=0) # Specificity (True Negative Rate)\n",
    "        else:\n",
    "            print(\"Please make sure the task is regression or classification !\")\n",
    "            return 0\n",
    "        \n",
    "        value_list = [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), task_type, \n",
    "            rmse, mse, accuracy, auc_score, f1, precision, recall, specificity\n",
    "        ]\n",
    "        \n",
    "        validation_results_df.append(value_list)       \n",
    "#         print(f\"RMSE on the validation set: {rmse}\")\n",
    "        \n",
    "        # Save validation results to file\n",
    "        validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "        \n",
    "#         output = ', '.join([f\"{col} : {val}\" for col, val in validation_records_df.iloc[0].items()])\n",
    "#         print(output)\n",
    "        \n",
    "#         if save_records:\n",
    "#             folder_name = f\"{model_name}_result_records\"\n",
    "#             if not os.path.exists(folder_name):\n",
    "#                 os.makedirs(folder_name)\n",
    "            \n",
    "#             file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "            \n",
    "#             # If testing, then change validation results csv file path to testing results csv file path\n",
    "#             if testing:\n",
    "#                 file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "#             else: \n",
    "#                 file_path = file_path\n",
    "            \n",
    "#             if not os.path.exists(file_path):\n",
    "#                 validation_records_df.to_csv(file_path, index=False, float_format='%.6f')\n",
    "#             else:\n",
    "                \n",
    "        validation_records_df.to_csv(file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "        \n",
    "#         else:\n",
    "#             print(\"Training and validation results are not saved to csv !\")\n",
    "\n",
    "    else:\n",
    "        return f\"Please select a model in {deep_learning_model_names} and {machine_learning_model_names} !\"\n",
    "         \n",
    "    return training_records_df, validation_records_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367d725",
   "metadata": {},
   "source": [
    "# (MLflow) Model training, validation, testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6904ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import nbformat\n",
    "\n",
    "# def load_notebook(notebook_path):\n",
    "#     with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#         nb = nbformat.read(f, as_version=4)\n",
    "#     code_cells = [cell.source for cell in nb.cells if cell.cell_type == 'code']\n",
    "#     exec('\\n'.join(code_cells), globals())\n",
    "\n",
    "## import written function and variable\n",
    "\n",
    "# parent_directory = Path('../../../')\n",
    "## parent_directory = Path('../../')\n",
    "# data_preprocessing_utils_path = parent_directory / 'data_preprocessing_utils.ipynb'\n",
    "\n",
    "# load_notebook(data_preprocessing_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Validation (or Testing) Function with MLflow\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def mlflow_model_training_validtion_or_testing(X_train, y_train, X_valid, y_valid, \n",
    "                             model_name, hyperparameters, task_type, loss_type, optimizer_type,\n",
    "                             dl_learning_rate, epochs_num, batch_size, \n",
    "                             train_metric=None, model_metric=None, save_records=True, testing=False):\n",
    "    \n",
    "    deep_learning_model_names = [\"FM\", \"MLP\", \"DeepFM\", \"AFM\", \"DCN\", \"xDeepFM\", \"AutoInt\", \"AFN\"]\n",
    "    machine_learning_model_names = [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \n",
    "                                    \"AdaBoost\", \"XGBoost\"]\n",
    "    \n",
    "    phase = \"test\" if testing else \"eval\"\n",
    "    model = 0\n",
    "    predictions = 0 # for logging model\n",
    "\n",
    "    if model_name in deep_learning_model_names:\n",
    "\n",
    "        if save_records:\n",
    "\n",
    "            training_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", \"Training_for\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "                \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "                \"Epoch/Epochs_num\", \"Avg_Loss\", \"Avg_RMSE\", \"Avg_MSE\", \"Avg_MAE\", \"Avg_Accuracy\", \n",
    "                \"Avg_AUC_score\", \"Avg_F1\", \"Avg_Precision\", \"Avg_Recall\", \"Avg_Specificity\"\n",
    "            ]\n",
    "            validation_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \n",
    "                \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\", \n",
    "                \"Epoch/Epochs_num\", \"Loss\", \"RMSE\", \"MSE\", \"MAE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "            ]\n",
    "\n",
    "            # Create folder if not exists\n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "\n",
    "            training_file_path = os.path.join(folder_name, f'{model_name}_training_result_records.csv')\n",
    "\n",
    "            # Create record file if not exists\n",
    "            if not os.path.exists(training_file_path):\n",
    "                empty_df = pd.DataFrame(columns=training_column_list)\n",
    "                empty_df.to_csv(training_file_path, index=False, float_format='%.6f')\n",
    "\n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            validation_file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "\n",
    "            if testing:\n",
    "                validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                validation_file_path = validation_file_path\n",
    "\n",
    "            if not os.path.exists(validation_file_path):\n",
    "                empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "                empty_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "            else:\n",
    "                record_df = pd.read_csv(validation_file_path)\n",
    "                columns_to_compare = [\"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())],\n",
    "                          \"Task_type\", \"Loss_type\", \"Optimizer_type\", \"DL_learning_rate\", \"Epochs_num\", \"Batch_size\"]\n",
    "                record_df = record_df[columns_to_compare]\n",
    "                record_df = record_df.values.tolist()\n",
    "                record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "                temp_df_values = [[model_name, *hyperparameters.values(), task_type, \n",
    "                                   loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size]]\n",
    "                temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                temp_df = temp_df.values.tolist()[0]\n",
    "                temp_df = [str(value) for value in temp_df]\n",
    "\n",
    "                if temp_df in record_df:\n",
    "                    print(\"Parameters already exists in validation or test results !\")\n",
    "                    return 0\n",
    "        \n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "        \n",
    "        run_name=0\n",
    "        if testing:\n",
    "            if train_metric != None and model_metric == None:\n",
    "                run_name = f\"Retrained {train_metric.uppper()} params\"\n",
    "            elif train_metric != None and model_metric != None:\n",
    "                run_name = f\"Retrained {train_metric.uppper()} and {model_metric.upper()} params\"\n",
    "            elif train_metric == None and model_metric == None:\n",
    "                run_name = None\n",
    "            else:\n",
    "                print(\"If you want to set run name, please make sure train_metric is not None !\")\n",
    "                return 0\n",
    "        else:\n",
    "            run_name = None\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "        \n",
    "            # Log training parameters\n",
    "            params = {\n",
    "                **hyperparameters,\n",
    "                \"task_type\": task_type,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"optimizer_type\": optimizer_type,\n",
    "                \"dl_learning_rate\": dl_learning_rate,\n",
    "                \"epochs_num\": epochs_num,\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "            mlflow.log_params(params)\n",
    "    \n",
    "        \n",
    "            if model_name == \"FM\":\n",
    "                model = FactorizationMachine(**hyperparameters)\n",
    "            elif model_name == \"MLP\":\n",
    "                model = MultiLayerPerceptron(**hyperparameters)\n",
    "            elif model_name == \"DeepFM\":\n",
    "                model = DeepFM(**hyperparameters)\n",
    "            elif model_name == \"AFM\":\n",
    "                model = AttentionalFactorizationMachineModel(**hyperparameters)\n",
    "            elif model_name == \"DCN\":\n",
    "                model = DeepCrossNetworkModel(**hyperparameters)\n",
    "            elif model_name == \"xDeepFM\":\n",
    "                model = ExtremeDeepFactorizationMachineModel(**hyperparameters)\n",
    "            elif model_name == \"AutoInt\":\n",
    "                model = AutomaticFeatureInteractionModel(**hyperparameters)\n",
    "            elif model_name == \"AFN\":\n",
    "                model = AdaptiveFactorizationNetwork(**hyperparameters)\n",
    "            else:\n",
    "                print(f\"Please choose a model in {deep_learning_model_names} !\")\n",
    "            \n",
    "            # Log model summary.\n",
    "            with open(f\"{model_name}_model_summary.txt\", \"w\") as f:\n",
    "                f.write(str(summary(model)))\n",
    "            mlflow.log_artifact(f\"{model_name}_model_summary.txt\")\n",
    "\n",
    "            # Convert to float tensor\n",
    "            X_train_tensor = torch.from_numpy(X_train).float()\n",
    "            Xi_train_tensor = torch.arange(X_train_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_train_tensor.size(0), 1, 1).int()\n",
    "            y_train_tensor = torch.from_numpy(y_train).float()\n",
    "            X_valid_tensor = torch.from_numpy(X_valid).float()\n",
    "            Xi_valid_tensor = torch.arange(X_valid_tensor.size(1)).unsqueeze(0).unsqueeze(-1).repeat(X_valid_tensor.size(0), 1, 1).int()\n",
    "            y_valid_tensor = torch.from_numpy(y_valid).float()\n",
    "\n",
    "            # Convert data to DataLoader\n",
    "            train_dataset = TensorDataset(Xi_train_tensor, X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataset = TensorDataset(Xi_valid_tensor, X_valid_tensor, y_valid_tensor)\n",
    "            validation_loader = DataLoader(validation_dataset, batch_size=len(validation_data), shuffle=True)\n",
    "\n",
    "            criterion = nn.MSELoss() # Use mean squared error loss as default criterion type\n",
    "    #         metric_type = \"RMSE\" # Use RMSE as default metric type\n",
    "\n",
    "            regression_loss_list = [\"MSE, RMSE\", \"MAE\"]\n",
    "            classification_loss_list = [\"CrossEntropy\", \"BinaryCrossEntropy\"]\n",
    "    #         classification_metric_list = [\"Accuracy\", \"AUC\", \"ROC\", \"F1-score\"]\n",
    "\n",
    "            optimizer_type_list = [\"Adam\", \"SGD\", \"Adagrad\", \"RMSprop\", \"Adadelta\", \"Adamax\", \"Nadam\"]\n",
    "            optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)  # Use Adam optimizer as default type\n",
    "\n",
    "            if optimizer_type == \"Adam\":\n",
    "                optimizer = optim.Adam(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"SGD\":\n",
    "                optimizer = optim.SGD(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"Adagrad\":\n",
    "                optimizer = optim.Adagrad(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"RMSprop\":\n",
    "                optimizer = optim.RMSprop(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"Adadelta\":\n",
    "                optimizer = optim.Adadelta(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"Adamax\":\n",
    "                optimizer = optim.Adamax(model.parameters(), lr=dl_learning_rate)\n",
    "            elif optimizer_type == \"Nadam\":\n",
    "                optimizer = optim.Nadam(model.parameters(), lr=dl_learning_rate)\n",
    "            else:\n",
    "                print(f\"Please select an optimizer in {optimizer_name_list}\")\n",
    "                return 0\n",
    "            \n",
    "            # Train and validate the model\n",
    "            training_results_df = []\n",
    "            validation_results_df = []\n",
    "\n",
    "            print(f'Start training {model_name} model ...')\n",
    "            for epoch in range(epochs_num):\n",
    "\n",
    "                total_loss = 0.0\n",
    "                total_batches = 0\n",
    "\n",
    "                total_rmse = 0.0\n",
    "                total_mse = 0.0\n",
    "                total_mae = 0.0\n",
    "                total_accuracy = 0.0\n",
    "                total_auc_score = 0.0\n",
    "                total_f1 = 0.0\n",
    "                total_precision = 0.0\n",
    "                total_recall = 0.0\n",
    "                total_specificity = 0.0\n",
    "\n",
    "                for t, (xi, x, y_true) in enumerate(train_loader):\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    y_pred = 0\n",
    "                    if model_name == \"DeepFM\":\n",
    "                        y_pred = model(xi, x)\n",
    "                    elif model_name == \"MLP\":\n",
    "                        y_pred = model(x)\n",
    "                        y_pred = y_pred.view(-1)\n",
    "                    else:    \n",
    "                        y_pred = model(x)\n",
    "\n",
    "                    loss = 0.0\n",
    "\n",
    "                    rmse = 0.0\n",
    "                    mse = 0.0\n",
    "                    mae = 0.0\n",
    "                    accuracy = 0.0\n",
    "                    auc_score = 0.0\n",
    "                    f1 = 0.0\n",
    "                    precision = 0.0\n",
    "                    recall = 0.0\n",
    "                    specificity = 0.0\n",
    "\n",
    "                    if task_type == \"Regression\":\n",
    "                        criterion = nn.MSELoss()\n",
    "                        if loss_type == \"RMSE\":\n",
    "                            eps = 1e-6\n",
    "                            if model_name == \"FM\":\n",
    "                                loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                            else:\n",
    "                                loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                        elif loss_type == \"MSE\":\n",
    "                            if model_name == \"FM\":\n",
    "                                loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                            else:        \n",
    "                                loss = criterion(y_pred, y_true.view(-1))\n",
    "                        else:\n",
    "                            print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                            return 0\n",
    "\n",
    "                        y_true = y_true.detach().numpy()\n",
    "                        y_pred = y_pred.detach().numpy()\n",
    "                        y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                        predictions = y_pred\n",
    "\n",
    "                        rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                        mse = mean_squared_error(y_true, y_pred)\n",
    "                        mae = mean_absolute_error(y_true, y_pred)\n",
    "                        \n",
    "                        # log metrics at each step\n",
    "#                         mlflow.log_metric(\"loss\", f\"{loss:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"rmse\", f\"{rmse:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"mse\", f\"{mse:4f}\", step=t)\n",
    "\n",
    "                    elif task_type == \"Classification\":\n",
    "                        if loss_type == \"BCE\":\n",
    "                            criterion = nn.BCELoss()\n",
    "                            y_pred = torch.sigmoid(y_pred)\n",
    "                        elif loss_type == \"CE\":\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                        else:\n",
    "                            print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                            return 0\n",
    "\n",
    "                        loss = criterion(y_pred, y_true)\n",
    "\n",
    "                        y_true = y_true.detach().numpy()\n",
    "                        y_pred = y_pred.detach().numpy()\n",
    "#                         y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                        predictions = y_pred # for logging model\n",
    "                        \n",
    "                        accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                        auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                        f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                        precision = precision_score(y_true, y_pred) # Precision\n",
    "                        recall = recall_score(y_true, y_pred) # Recall\n",
    "                        specificity = recall_score(y_true, y_pred, pos_label=1) # Specificity (True Negative Rate)                        \n",
    "                        \n",
    "                        # log metrics at each step\n",
    "#                         mlflow.log_metric(\"loss\", f\"{loss:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"accuracy\", f\"{accuracy:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"auc_score\", f\"{auc_score:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"f1\", f\"{f1:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"precision\", f\"{precision:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"recall\", f\"{recall:4f}\", step=t)\n",
    "#                         mlflow.log_metric(\"specificity\", f\"{specificity:4f}\", step=t)\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"Please make sure the task is regression or classification !\")\n",
    "                        return 0\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    total_batches += 1\n",
    "\n",
    "                    total_rmse += rmse\n",
    "                    total_mse += mse\n",
    "                    total_mae += mae\n",
    "                    total_accuracy += accuracy\n",
    "                    total_auc_score += auc_score\n",
    "                    total_f1 += f1\n",
    "                    total_precision += precision\n",
    "                    total_recall += recall\n",
    "                    total_specificity += specificity\n",
    "\n",
    "                avg_loss = total_loss / total_batches\n",
    "\n",
    "                avg_rmse = total_rmse / total_batches\n",
    "                avg_mse = total_mse / total_batches\n",
    "                avg_mae = total_mae / total_batches\n",
    "\n",
    "                avg_accuracy = total_accuracy / total_batches\n",
    "                avg_auc_score = total_auc_score / total_batches\n",
    "                avg_f1 = total_f1 / total_batches\n",
    "                avg_precision = total_precision / total_batches\n",
    "                avg_recall = total_recall / total_batches\n",
    "                avg_specificity = total_specificity / total_batches\n",
    "                \n",
    "                # log metrics at each epoch\n",
    "                mlflow.log_metric(\"train_loss\", f\"{avg_loss:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_rmse\", f\"{avg_rmse:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_mse\", f\"{avg_mse:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_mae\", f\"{avg_mae:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_accuracy\", f\"{avg_accuracy:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_auc_score\", f\"{avg_auc_score:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_f1\", f\"{avg_f1:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_precision\", f\"{avg_precision:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_recall\", f\"{avg_recall:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(\"train_specificity\", f\"{avg_specificity:4f}\", step=(epoch+1))\n",
    "\n",
    "                value_list = [\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, \n",
    "                    \"for_testing\" if testing else \"for_validation\",\n",
    "                    *hyperparameters.values(), task_type, loss_type,\n",
    "                    optimizer_type, dl_learning_rate, epochs_num, batch_size, f\"{epoch + 1}th/{epochs_num}\", \n",
    "                    avg_loss, avg_rmse, avg_mse, avg_mae, avg_accuracy, avg_auc_score, \n",
    "                    avg_f1, avg_precision, avg_recall, avg_specificity\n",
    "                ]\n",
    "    #             print(f'Epoch {epoch + 1}/{epochs_num}, Average RMSE: {average_rmse:.4f}')\n",
    "\n",
    "                training_results_df.append(value_list)\n",
    "\n",
    "                # Make predictions on validation data --------------------------------------\n",
    "                if testing:\n",
    "                    print(\"Start testing ...\")\n",
    "                else:\n",
    "                    print(\"Start validating ...\")\n",
    "                \n",
    "#                 all_predictions = []\n",
    "\n",
    "                y_pred = 0\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for t, (xi, x, y_true) in enumerate(validation_loader):\n",
    "\n",
    "#                         predictions = 0\n",
    "                        if model_name == \"DeepFM\":\n",
    "                            y_pred = model(xi, x)\n",
    "                        elif model_name == \"MLP\":\n",
    "                            y_pred = model(x)\n",
    "                            y_pred = y_pred.view(-1)\n",
    "                        else:\n",
    "                            y_pred = model(x)\n",
    "\n",
    "#                         all_predictions.append(y_pred.numpy())\n",
    "\n",
    "#                 # Concatenate all predictions into a single numpy array\n",
    "#                 all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "#                 # Reshape all_predictions if needed\n",
    "#                 y_pred = all_predictions.squeeze()\n",
    "\n",
    "                # Calculate metric values\n",
    "                loss = 0.0\n",
    "                \n",
    "                rmse = 0.0\n",
    "                mse = 0.0\n",
    "                mae = 0.0\n",
    "                accuracy = 0.0\n",
    "                auc_score = 0.0\n",
    "                f1 = 0.0\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                specificity = 0.0\n",
    "\n",
    "                if task_type == \"Regression\":\n",
    "                    criterion = nn.MSELoss()\n",
    "                    if loss_type == \"RMSE\":\n",
    "                        eps = 1e-6\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1, 1)) + eps)\n",
    "                        else:\n",
    "                            loss = torch.sqrt(criterion(y_pred, y_true.view(-1)) + eps)\n",
    "                    elif loss_type == \"MSE\":\n",
    "                        if model_name == \"FM\":\n",
    "                            loss = criterion(y_pred, y_true.view(-1, 1))\n",
    "                        else:        \n",
    "                            loss = criterion(y_pred, y_true.view(-1))\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {regression_loss_list}\")\n",
    "                        return 0\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                    y_pred = y_pred.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                    y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                    \n",
    "                    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "                elif task_type == \"Classification\":\n",
    "                    if loss_type == \"BCE\":\n",
    "                            criterion = nn.BCELoss()\n",
    "                            y_pred = torch.sigmoid(y_pred)\n",
    "                    elif loss_type == \"CE\":\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                    else:\n",
    "                        print(f\"Please make sure loss type is in {classification_loss_list}\")\n",
    "                        return 0\n",
    "\n",
    "                    loss = criterion(y_pred, y_true)\n",
    "                    \n",
    "                    y_true = y_true.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "                    y_pred = y_pred.detach().numpy() # Convert PyTorch tensor to NumPy array\n",
    "#                     y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_true, y_pred) # Accuracy\n",
    "                    auc_score = roc_auc_score(y_true, y_pred) # AUC\n",
    "                    f1 = f1_score(y_true, y_pred) # F1-score\n",
    "                    precision = precision_score(y_true, y_pred) # Precision\n",
    "                    recall = recall_score(y_true, y_pred) # Recall\n",
    "                    specificity = recall_score(y_true, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Please make sure the task is regression or classification !\")\n",
    "                    return 0\n",
    "\n",
    "                # log metrics at each step\n",
    "#                 mlflow.log_metric(\"eval_loss\", f\"{loss:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_rmse\", f\"{rmse:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_mse\", f\"{mse:4f}\", step=t)\n",
    "                    \n",
    "                # log metrics at each epoch\n",
    "                mlflow.log_metric(f\"{phase}_loss\", f\"{loss:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_mse\", f\"{mse:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_mae\", f\"{mae:4f}\", step=(epoch+1))\n",
    "                \n",
    "                # log metrics at each step\n",
    "#                 mlflow.log_metric(\"eval_loss\", f\"{loss:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_accuracy\", f\"{accuracy:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_auc_score\", f\"{auc_score:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_f1\", f\"{f1:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_precision\", f\"{precision:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_recall\", f\"{recall:4f}\", step=t)\n",
    "#                 mlflow.log_metric(\"eval_specificity\", f\"{specificity:4f}\", step=t)\n",
    "\n",
    "                # log metrics at each epoch\n",
    "#                 mlflow.log_metric(\"eval_loss\", f\"{loss:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_f1\", f\"{f1:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_precision\", f\"{precision:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_recall\", f\"{recall:4f}\", step=(epoch+1))\n",
    "                mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:4f}\", step=(epoch+1))\n",
    "                \n",
    "                value_list = [\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), \n",
    "                    task_type, loss_type, optimizer_type, dl_learning_rate, epochs_num, batch_size, \n",
    "                    f\"{epoch + 1}th/{epochs_num}\", loss, rmse, mse, mae, accuracy, auc_score, f1, precision, \n",
    "                    recall, specificity\n",
    "                ]\n",
    "\n",
    "                validation_results_df.append(value_list)\n",
    "        #         print(f\"RMSE on the validation set: {rmse}\")\n",
    "\n",
    "            # Save training and validation results to file\n",
    "            training_records_df = pd.DataFrame(training_results_df, columns=training_column_list)\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "\n",
    "            training_records_df.to_csv(training_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "                \n",
    "            # Save the trained model to MLflow.\n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            mlflow.pytorch.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "#             mlflow.pytorch.log_model(model, f\"{model_name}_model\")\n",
    "                \n",
    "    elif model_name in machine_learning_model_names:\n",
    "\n",
    "        if save_records:\n",
    "\n",
    "#             training_records_df = f\"{model_name} has no training result records.\"\n",
    "            training_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", \"Training_for\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\",\n",
    "                \"RMSE\", \"MSE\", \"MAE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "            ]\n",
    "            validation_column_list = [\n",
    "                \"Timestamp\", \"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\",\n",
    "                \"RMSE\", \"MSE\", \"MAE\", \"Accuracy\", \"AUC_score\", \"F1\", \"Precision\", \"Recall\", \"Specificity\"\n",
    "            ]\n",
    "\n",
    "            folder_name = f\"{model_name}_result_records\"\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            training_file_path = os.path.join(folder_name, f'{model_name}_training_result_records.csv')\n",
    "\n",
    "            # Create record file if not exists\n",
    "            if not os.path.exists(training_file_path):\n",
    "                empty_df = pd.DataFrame(columns=training_column_list)\n",
    "                empty_df.to_csv(training_file_path, index=False, float_format='%.6f')\n",
    "            \n",
    "            validation_file_path = os.path.join(folder_name, f'{model_name}_validation_result_records.csv')\n",
    "\n",
    "            # If testing, then change validation results csv file path to testing results csv file path\n",
    "            if testing:\n",
    "                validation_file_path = os.path.join(folder_name, f'{model_name}_testing_result_records.csv')\n",
    "            else: \n",
    "                validation_file_path = validation_file_path\n",
    "\n",
    "            if not os.path.exists(validation_file_path):\n",
    "                empty_df = pd.DataFrame(columns=validation_column_list)\n",
    "                empty_df.to_csv(validation_file_path, index=False, float_format='%.6f')\n",
    "            else:\n",
    "                record_df = pd.read_csv(validation_file_path)\n",
    "                columns_to_compare = [\"Model_name\", *[f\"{model_name}_{key}\" for key in list(hyperparameters.keys())], \"Task_type\"]\n",
    "                record_df = record_df[columns_to_compare]\n",
    "                record_df = record_df.values.tolist()\n",
    "                record_df = [[str(value) for value in onelist] for onelist in record_df]\n",
    "                temp_df_values = [[model_name, *hyperparameters.values(), task_type]]\n",
    "                temp_df = pd.DataFrame(temp_df_values, columns=columns_to_compare)\n",
    "                temp_df = temp_df.values.tolist()[0]\n",
    "                temp_df = [str(value) for value in temp_df]\n",
    "\n",
    "                if temp_df in record_df:\n",
    "                    print(\"Parameters already exists in validation or test results !\")\n",
    "                    return 0\n",
    "\n",
    "        else:\n",
    "            print(\"Training and validation results are not saved to csv !\")\n",
    "\n",
    "        if model_name == \"Linear\":\n",
    "            model = LinearRegression() if task_type == \"Regression\" else LogisticRegression()\n",
    "        elif model_name == \"KNN\":\n",
    "            model = KNeighborsRegressor(**hyperparameters) if task_type == \"Regression\" else KNeighborsClassifier(**hyperparameters)\n",
    "        elif model_name == \"SVM\":\n",
    "            model = SVR(**hyperparameters) if task_type == \"Regression\" else SVC(**hyperparameters)\n",
    "        elif model_name == \"DecisionTree\":\n",
    "            model = DecisionTreeRegressor(**hyperparameters) if task_type == \"Regression\" else DecisionTreeClassifier(**hyperparameters)\n",
    "        elif model_name == \"RandomForest\":\n",
    "            model = RandomForestRegressor(**hyperparameters) if task_type == \"Regression\" else RandomForestClassifier(**hyperparameters)\n",
    "        elif model_name == \"AdaBoost\":\n",
    "            model = AdaBoostRegressor(**hyperparameters) if task_type == \"Regression\" else AdaBoostClassifier(**hyperparameters)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**hyperparameters) if task_type == \"Regression\" else XGBClassifier(**hyperparameters)\n",
    "        else:\n",
    "            print(f\"Please choose a model in {machine_learning_model_names} !\")\n",
    "        \n",
    "        run_name=0\n",
    "        if testing:\n",
    "            if train_metric != None and model_metric == None:\n",
    "                run_name = f\"Retrained {train_metric.uppper()} params\"\n",
    "            elif train_metric != None and model_metric != None:\n",
    "                run_name = f\"Retrained {train_metric.uppper()} and {model_metric.upper()} params\"\n",
    "            elif train_metric == None and model_metric == None:\n",
    "                run_name = None\n",
    "            else:\n",
    "                print(\"If you want to set run name, please make sure train_metric is not None !\")\n",
    "                return 0\n",
    "        else:\n",
    "            run_name = None\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "            # Log training parameters\n",
    "            params = {\n",
    "                **hyperparameters,\n",
    "                \"task_type\": task_type,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"optimizer_type\": optimizer_type,\n",
    "                \"dl_learning_rate\": dl_learning_rate,\n",
    "                \"epochs_num\": epochs_num,\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Train the model\n",
    "            print(f'Start training {model_name} model ...')\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on training data\n",
    "            training_results_df = []\n",
    "            y_pred = model.predict(X_train)\n",
    "            y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "            predictions = y_pred # for logging model\n",
    "            \n",
    "            # Calculate metric values\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if task_type == \"Regression\":\n",
    "                rmse = root_mean_squared_error(y_train, y_pred)\n",
    "                mse = mean_squared_error(y_train, y_pred)\n",
    "                mae = mean_absolute_error(y_train, y_pred)\n",
    "            elif task_type == \"Classification\":\n",
    "                accuracy = accuracy_score(y_train, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_train, y_pred) # AUC\n",
    "                f1 = f1_score(y_train, y_pred) # F1-score\n",
    "                precision = precision_score(y_train, y_pred) # Precision\n",
    "                recall = recall_score(y_train, y_pred) # Recall\n",
    "                specificity = recall_score(y_train, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "            mlflow.log_metric(\"train_rmse\", f\"{rmse:4f}\")\n",
    "            mlflow.log_metric(\"train_mse\", f\"{mse:4f}\")\n",
    "            mlflow.log_metric(\"train_mae\", f\"{mae:4f}\")\n",
    "            mlflow.log_metric(\"train_accuracy\", f\"{accuracy:4f}\")\n",
    "            mlflow.log_metric(\"train_auc_score\", f\"{auc_score:4f}\")\n",
    "            mlflow.log_metric(\"train_f1\", f\"{f1:4f}\")\n",
    "            mlflow.log_metric(\"train_precision\", f\"{precision:4f}\")\n",
    "            mlflow.log_metric(\"train_recall\", f\"{recall:4f}\")\n",
    "            mlflow.log_metric(\"train_specificity\", f\"{specificity:4f}\")\n",
    "\n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name,\n",
    "                \"for_testing\" if testing else \"for_validation\",\n",
    "                *hyperparameters.values(), task_type, \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            training_results_df.append(value_list)       \n",
    "    #         print(f\"RMSE on the validation set: {rmse}\")\n",
    "\n",
    "            # Save training results to file\n",
    "            training_records_df = pd.DataFrame(training_results_df, columns=training_column_list)\n",
    "            training_records_df.to_csv(training_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "\n",
    "            # Make predictions on validation data -------------------------------------\n",
    "            validation_results_df = []\n",
    "\n",
    "            if testing:\n",
    "                print(\"Start testing ...\")\n",
    "            else:\n",
    "                print(\"Start validating ...\")\n",
    "                    \n",
    "            y_pred = model.predict(X_valid)\n",
    "            y_pred = np.nan_to_num(y_pred, nan=0) # prevent error\n",
    "            # Calculate metric values\n",
    "            rmse = 0.0\n",
    "            mse = 0.0\n",
    "            mae = 0.0\n",
    "            accuracy = 0.0\n",
    "            auc_score = 0.0\n",
    "            f1 = 0.0\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            specificity = 0.0\n",
    "\n",
    "            if task_type == \"Regression\":\n",
    "                rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "                mse = mean_squared_error(y_valid, y_pred)\n",
    "                mae = mean_absolute_error(y_valid, y_pred)\n",
    "            elif task_type == \"Classification\":\n",
    "                accuracy = accuracy_score(y_valid, y_pred) # Accuracy\n",
    "                auc_score = roc_auc_score(y_valid, y_pred) # AUC\n",
    "                f1 = f1_score(y_valid, y_pred) # F1-score\n",
    "                precision = precision_score(y_valid, y_pred) # Precision\n",
    "                recall = recall_score(y_valid, y_pred) # Recall\n",
    "                specificity = recall_score(y_valid, y_pred, pos_label=1) # Specificity (True Negative Rate)\n",
    "            else:\n",
    "                print(\"Please make sure the task is regression or classification !\")\n",
    "                return 0\n",
    "\n",
    "            mlflow.log_metric(f\"{phase}_rmse\", f\"{rmse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mse\", f\"{mse:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_mae\", f\"{mae:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_accuracy\", f\"{accuracy:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_auc_score\", f\"{auc_score:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_f1\", f\"{f1:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_precision\", f\"{precision:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_recall\", f\"{recall:4f}\")\n",
    "            mlflow.log_metric(f\"{phase}_specificity\", f\"{specificity:4f}\")\n",
    "\n",
    "            value_list = [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), model_name, *hyperparameters.values(), task_type, \n",
    "                rmse, mse, mae, accuracy, auc_score, f1, precision, recall, specificity\n",
    "            ]\n",
    "\n",
    "            validation_results_df.append(value_list)       \n",
    "    #         print(f\"RMSE on the validation set: {rmse}\")\n",
    "\n",
    "            # Save validation results to file\n",
    "            validation_records_df = pd.DataFrame(validation_results_df, columns=validation_column_list)\n",
    "            validation_records_df.to_csv(validation_file_path, mode='a', header=False, index=False, float_format='%.6f')\n",
    "            \n",
    "            input_example = X_train[0]\n",
    "            signatures = infer_signature(input_example, predictions)\n",
    "            # Save the trained model to MLflow.\n",
    "            if model_name in [\"Linear\", \"KNN\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\"]:\n",
    "                mlflow.sklearn.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "#                 mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"XGBoost\":\n",
    "                mlflow.xgboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "#                 mlflow.xgboost.log_model(model, f\"{model_name}_model\")\n",
    "            elif model_name == \"CatBoost\":\n",
    "                mlflow.catboost.log_model(model, f\"{model_name}_model\", signature=signatures)\n",
    "#                 mlflow.catboost.log_model(model, f\"{model_name}_model\")\n",
    "            else:\n",
    "                print(f\"Model not in {machine_learning_model_names}, couldn't save the model !\")\n",
    "            \n",
    "    else:\n",
    "        return f\"Please select a model in {deep_learning_model_names} and {machine_learning_model_names} !\"\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    # mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    return training_records_df, validation_records_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
