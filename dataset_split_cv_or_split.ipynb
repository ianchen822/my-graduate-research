{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c901e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nbformat\n",
    "\n",
    "def load_notebook(notebook_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    code_cells = [cell.source for cell in nb.cells if cell.cell_type == 'code']\n",
    "    exec('\\n'.join(code_cells), globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8957e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp datasets file\n",
    "yelp_datasets_path = '../../Data_preprocessing/yelp_datasets/'\n",
    "\n",
    "# yelp dataset (from yelp offical website)\n",
    "yelp_offical_dataset_path = yelp_datasets_path + 'yelp_dataset_official/'\n",
    "\n",
    "# business, user and review dataset\n",
    "yelp_academic_dataset_business_path = yelp_offical_dataset_path + 'yelp_academic_dataset_business.json'\n",
    "yelp_academic_dataset_user_path = yelp_offical_dataset_path + 'yelp_academic_dataset_user.json'\n",
    "yelp_academic_dataset_review_path = yelp_offical_dataset_path + 'yelp_academic_dataset_review.json'\n",
    "\n",
    "# yelp photo dataset (from yelp official website)\n",
    "yelp_offical_photo_dataset_path = yelp_datasets_path + 'yelp_dataset_official_photos/'\n",
    "\n",
    "# photo dataset\n",
    "photo_dataset_path = yelp_offical_photo_dataset_path + 'photos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b01bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge user data and business data to orginal dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define a function to read JSON files and read them in chunks\n",
    "def read_json_with_chunk(file_path, chunk_size):\n",
    "    chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "    return pd.concat(chunks)\n",
    "\n",
    "# Read user, business and original data\n",
    "user_data = read_json_with_chunk(yelp_academic_dataset_user_path, 10000)\n",
    "business_data = read_json_with_chunk(yelp_academic_dataset_business_path, 10000)\n",
    "\n",
    "# Add prefix to column names in user_data and business_data\n",
    "user_data = user_data.add_prefix('user_').rename(columns={'user_user_id': 'user_id'})\n",
    "business_data = business_data.add_prefix('business_').rename(columns={'business_business_id': 'business_id'})\n",
    "\n",
    "file_path = \"research_original_set_13percent_5reviews_num_threshold_GPT-3.5 Turbo_random_stars.csv\"\n",
    "original_data = pd.read_csv(file_path)\n",
    "\n",
    "# Merge user data into original data\n",
    "original_data = pd.merge(original_data, user_data, how='left', left_on='user_id', right_on='user_id')\n",
    "\n",
    "# Merge business data into original data\n",
    "original_data = pd.merge(original_data, business_data, how='left', left_on='business_id', right_on='business_id')\n",
    "\n",
    "# Save to csv\n",
    "original_data.to_csv(f\"research_original_set_with_user_and_business_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09427de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original dataset\n",
    "\n",
    "file_path = \"research_original_set_with_user_and_business_data.csv\"\n",
    "original_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bd665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import written function and variable\n",
    "\n",
    "parent_directory = Path('../../')\n",
    "data_preprocessing_utils_path = parent_directory / 'data_preprocessing_utils.ipynb'\n",
    "\n",
    "load_notebook(data_preprocessing_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704ef0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has successfully split into training set and test set and saved to csv files !\n",
      "Training data has successfully split into 10 folds and saved to csv files !\n"
     ]
    }
   ],
   "source": [
    "# kfold_cross_validation(original_data, train_ratio=0.9, test_ratio=0.1, fold_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dec9bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (13996, 44), ratio is 1.0\n",
      "Training dataset size: (11196, 44), ratio is 0.7999428408116604\n",
      "Validation dataset size: (1400, 44), ratio is 0.10002857959416976\n",
      "Test dataset size: (1400, 44), ratio is 0.10002857959416976\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = dataset_split(original_data, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, stratify=(False, \"stars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6e0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "\n",
    "training_data.to_csv(\"research_training_set.csv\", index=False)\n",
    "validation_data.to_csv(\"research_validation_set.csv\", index=False)\n",
    "test_data.to_csv(\"research_test_set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
